{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Whisper Large-v3 with LoRA/PEFT\n",
    "\n",
    "**Target Hardware**: RunPod A6000 (48GB VRAM)\n",
    "\n",
    "This notebook fine-tunes Whisper large-v3 on VHP oral history audio using LoRA (Low-Rank Adaptation).\n",
    "\n",
    "## Key Configuration\n",
    "- **Model**: openai/whisper-large-v3 (via HuggingFace transformers)\n",
    "- **Method**: LoRA with r=32, alpha=64\n",
    "- **Learning Rate**: 1e-5 (CRITICAL: not 1e-3)\n",
    "- **Precision**: fp16 (NOT int8 for V3)\n",
    "- **Batch Size**: 4 with gradient accumulation 4 (effective: 16)\n",
    "\n",
    "## Data Requirements\n",
    "- Parquet files: `veterans_history_project_resources_pre2010_train.parquet` and `_val.parquet`\n",
    "- Azure blob storage connection for audio files\n",
    "\n",
    "See [learnings/whisper-lora-finetuning.md](../learnings/whisper-lora-finetuning.md) for gotchas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Dependencies\n",
    "\n",
    "Fine-tuning requires additional packages not in the base project. Add them via uv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fine-tuning dependencies to pyproject.toml (run once)\n",
    "# uv add peft accelerate\n",
    "#\n",
    "# For A6000 with CUDA 11.8:\n",
    "# uv add torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load Azure credentials from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../credentials/creds.env')\n",
    "\n",
    "# Add scripts directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"scripts\"))\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset, Audio\n",
    "import evaluate as hf_evaluate  # HuggingFace evaluate library (for WER metric)\n",
    "\n",
    "# Import project modules\n",
    "import data_loader\n",
    "import azure_utils\n",
    "from evaluate import clean_raw_transcript_str  # Local scripts/evaluate.py\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths - VHP parquet files (same naming as data splits)\n",
    "    \"train_parquet\": \"../data/raw/loc/veterans_history_project_resources_pre2010_train.parquet\",\n",
    "    \"val_parquet\": \"../data/raw/loc/veterans_history_project_resources_pre2010_val.parquet\",\n",
    "    \n",
    "    # Azure blob settings (same as inference configs)\n",
    "    \"blob_prefix\": \"loc_vhp\",\n",
    "    \n",
    "    # Sampling (set to None to use all data, or small number for testing)\n",
    "    \"train_sample_size\": 100,  # Set to None for full training\n",
    "    \"val_sample_size\": 20,\n",
    "    \"random_seed\": 42,\n",
    "    \n",
    "    # Output directory - follows convention: {dataset}-{model}-{task}-{infra}\n",
    "    \"output_dir\": \"../outputs/vhp-pre2010-whisper-large-v3-lora-ft-a6000\",\n",
    "    \n",
    "    # Model - using HuggingFace transformers (not faster-whisper, which is inference-only)\n",
    "    # Note: For inference we use faster-whisper, but for fine-tuning we need the original HF model\n",
    "    \"model_name\": \"openai/whisper-large-v3\",\n",
    "    \n",
    "    # LoRA configuration\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    \"learning_rate\": 1e-5,           # CRITICAL: Use 1e-5 for V3 (not 1e-3)\n",
    "    \"batch_size\": 4,                  # Per device\n",
    "    \"gradient_accumulation\": 4,       # Effective batch = 16\n",
    "    \"warmup_steps\": 500,\n",
    "    \"max_steps\": 5000,               # Adjust based on data size\n",
    "    \"eval_steps\": 500,\n",
    "    \"save_steps\": 500,\n",
    "    \n",
    "    # Precision\n",
    "    \"fp16\": True,                    # Use fp16 for V3\n",
    "    \"bf16\": False,\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "print(f\"Output directory: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data\n",
    "\n",
    "Using existing `data_loader.py` and `azure_utils.py` from scripts/.\n",
    "\n",
    "Ground truth is extracted from `fulltext_file_str` column using `clean_raw_transcript_str()` from evaluate.py (see [notebooks/evals_learn.ipynb](./evals_learn.ipynb) for details on how this works)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetune_dataset(parquet_path: str, sample_size: int = None, random_seed: int = 42):\n",
    "    \"\"\"\n",
    "    Load dataset for fine-tuning using existing data_loader infrastructure.\n",
    "    \n",
    "    Uses:\n",
    "    - data_loader.load_vhp_dataset() for parquet loading and sampling\n",
    "    - evaluate.clean_raw_transcript_str() for ground truth cleaning\n",
    "    - azure_utils for audio download\n",
    "    \"\"\"\n",
    "    # Load using existing data_loader (handles filtering, sampling)\n",
    "    df = data_loader.load_vhp_dataset(\n",
    "        parquet_path=parquet_path,\n",
    "        sample_size=sample_size,\n",
    "        filter_has_transcript=True,\n",
    "        filter_has_media=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded {len(df)} samples\")\n",
    "    return df\n",
    "\n",
    "# Load train and validation sets\n",
    "print(\"Loading training data...\")\n",
    "df_train = load_finetune_dataset(\n",
    "    CONFIG[\"train_parquet\"], \n",
    "    sample_size=CONFIG[\"train_sample_size\"],\n",
    "    random_seed=CONFIG[\"random_seed\"]\n",
    ")\n",
    "\n",
    "print(\"\\nLoading validation data...\")\n",
    "df_val = load_finetune_dataset(\n",
    "    CONFIG[\"val_parquet\"],\n",
    "    sample_size=CONFIG[\"val_sample_size\"],\n",
    "    random_seed=CONFIG[\"random_seed\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(df_train)} samples\")\n",
    "print(f\"Val: {len(df_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hf_dataset(df: pd.DataFrame, blob_prefix: str):\n",
    "    \"\"\"\n",
    "    Convert DataFrame to HuggingFace Dataset with audio and cleaned transcripts.\n",
    "    \n",
    "    Downloads audio from Azure blob and cleans transcripts using existing evaluate.py function.\n",
    "    \"\"\"\n",
    "    from tempfile import NamedTemporaryFile\n",
    "    from pydub import AudioSegment\n",
    "    import soundfile as sf\n",
    "    import librosa\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Get blob path candidates using existing data_loader function\n",
    "        blob_path_candidates = data_loader.get_blob_path_for_row(row, idx, blob_prefix)\n",
    "        \n",
    "        if not blob_path_candidates:\n",
    "            print(f\"  Skipping {idx}: no blob path\")\n",
    "            continue\n",
    "        \n",
    "        # Clean transcript using existing evaluate.py function\n",
    "        raw_transcript = row.get('fulltext_file_str', '')\n",
    "        cleaned_transcript = clean_raw_transcript_str(raw_transcript)\n",
    "        \n",
    "        if not cleaned_transcript.strip():\n",
    "            print(f\"  Skipping {idx}: empty transcript\")\n",
    "            continue\n",
    "        \n",
    "        # Download audio from Azure blob\n",
    "        audio_data = None\n",
    "        for blob_path in blob_path_candidates:\n",
    "            try:\n",
    "                if azure_utils.blob_exists(blob_path):\n",
    "                    audio_bytes = azure_utils.download_blob_to_memory(blob_path)\n",
    "                    \n",
    "                    # Convert to WAV 16kHz mono using pydub (same as infer_whisper.py)\n",
    "                    with NamedTemporaryFile(suffix=Path(blob_path).suffix, delete=False) as tmp:\n",
    "                        tmp.write(audio_bytes)\n",
    "                        tmp_path = tmp.name\n",
    "                    \n",
    "                    audio_seg = AudioSegment.from_file(tmp_path)\n",
    "                    audio_seg = audio_seg.set_frame_rate(16000).set_channels(1)\n",
    "                    \n",
    "                    # Export to wav\n",
    "                    wav_path = tmp_path.replace(Path(blob_path).suffix, '.wav')\n",
    "                    audio_seg.export(wav_path, format='wav')\n",
    "                    \n",
    "                    # Load as numpy array\n",
    "                    audio_data, sr = librosa.load(wav_path, sr=16000)\n",
    "                    \n",
    "                    # Cleanup temp files\n",
    "                    os.unlink(tmp_path)\n",
    "                    if os.path.exists(wav_path):\n",
    "                        os.unlink(wav_path)\n",
    "                    \n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"  Error downloading {blob_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if audio_data is None:\n",
    "            print(f\"  Skipping {idx}: could not download audio\")\n",
    "            continue\n",
    "        \n",
    "        records.append({\n",
    "            \"audio\": {\"array\": audio_data, \"sampling_rate\": 16000},\n",
    "            \"sentence\": cleaned_transcript\n",
    "        })\n",
    "        \n",
    "        if len(records) % 10 == 0:\n",
    "            print(f\"  Processed {len(records)} samples...\")\n",
    "    \n",
    "    print(f\"  Total valid samples: {len(records)}\")\n",
    "    \n",
    "    # Create HuggingFace dataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"audio\": [r[\"audio\"] for r in records],\n",
    "        \"sentence\": [r[\"sentence\"] for r in records]\n",
    "    })\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare HuggingFace datasets\n",
    "print(\"Preparing training dataset (downloading audio from Azure)...\")\n",
    "train_dataset = prepare_hf_dataset(df_train, CONFIG[\"blob_prefix\"])\n",
    "\n",
    "print(\"\\nPreparing validation dataset...\")\n",
    "val_dataset = prepare_hf_dataset(df_val, CONFIG[\"blob_prefix\"])\n",
    "\n",
    "print(f\"\\nFinal dataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a random sample\n",
    "import random\n",
    "random.seed(CONFIG[\"random_seed\"])\n",
    "\n",
    "sample_idx = random.randint(0, len(train_dataset) - 1)\n",
    "sample = train_dataset[sample_idx]\n",
    "\n",
    "print(f\"Sample {sample_idx}:\")\n",
    "print(f\"  Audio duration: {len(sample['audio']['array']) / sample['audio']['sampling_rate']:.1f}s\")\n",
    "print(f\"  Transcript preview: {sample['sentence'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model\n",
    "\n",
    "**Note on model choice**: For fine-tuning we use `openai/whisper-large-v3` from HuggingFace transformers. This is different from inference where we use `faster-whisper` (CTranslate2 optimized). The fine-tuned weights can later be converted to faster-whisper format for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor\n",
    "processor = WhisperProcessor.from_pretrained(CONFIG[\"model_name\"])\n",
    "\n",
    "# Load model in fp16\n",
    "print(f\"Loading model: {CONFIG['model_name']}\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=False,  # IMPORTANT: Don't use 8-bit for V3 (causes hallucinations)\n",
    ")\n",
    "\n",
    "# Clear forced decoder IDs (important for fine-tuning)\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "print(f\"Model loaded. Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply LoRA\n",
    "\n",
    "### Why LoRA for Whisper Fine-tuning?\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is chosen over full fine-tuning for several reasons:\n",
    "\n",
    "1. **Memory Efficiency**: Full fine-tuning of Whisper large-v3 (1.5B params) requires 30-40GB VRAM. LoRA reduces this to ~20GB by only training adapter weights.\n",
    "\n",
    "2. **Catastrophic Forgetting Prevention**: LoRA preserves the base model's general ASR capabilities while adapting to domain-specific audio. Full fine-tuning risks losing pre-trained knowledge.\n",
    "\n",
    "3. **Faster Training**: Only ~1% of parameters are trainable (15.7M vs 1.5B), significantly reducing training time.\n",
    "\n",
    "4. **Easy Model Merging**: LoRA weights can be merged with base model for deployment, or kept separate for A/B testing.\n",
    "\n",
    "**References:**\n",
    "- [HuggingFace PEFT Whisper Training](https://github.com/huggingface/peft/blob/main/examples/int8_training/peft_bnb_whisper_large_v2_training.ipynb)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False  # Disable cache for training\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    \"\"\"Preprocess audio and text for training.\"\"\"\n",
    "    audio = batch[\"audio\"]\n",
    "    \n",
    "    # Extract features from audio\n",
    "    batch[\"input_features\"] = processor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features[0]\n",
    "    \n",
    "    # Tokenize transcription\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing training data...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "val_dataset = val_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "print(f\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Collator and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"Data collator for Whisper seq2seq training.\"\"\"\n",
    "    processor: Any\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]):\n",
    "        # Process input features\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Process label features\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Replace padding with -100 (ignored by loss)\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "        \n",
    "        # Remove BOS token if present\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# WER metric (using HuggingFace evaluate library)\n",
    "wer_metric = hf_evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute WER for evaluation.\"\"\"\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    \n",
    "    # Replace -100 with pad token\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    # Decode\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "print(\"Data collator and metrics ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Configuration\n",
    "\n",
    "### Why Seq2SeqTrainer?\n",
    "\n",
    "Whisper is a sequence-to-sequence (encoder-decoder) model that takes audio input and generates text output. `Seq2SeqTrainer` from HuggingFace is specifically designed for this architecture and provides:\n",
    "\n",
    "1. **Proper generation during evaluation**: Uses `model.generate()` instead of forward pass\n",
    "2. **Label handling**: Correctly handles the decoder input/output shift\n",
    "3. **Beam search support**: For better generation quality during eval\n",
    "\n",
    "**Reference**: [HuggingFace Fine-tune Whisper Guide](https://huggingface.co/blog/fine-tune-whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    max_steps=CONFIG[\"max_steps\"],\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    bf16=CONFIG[\"bf16\"],\n",
    "    weight_decay=0.01,\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']} x {CONFIG['gradient_accumulation']} = {CONFIG['batch_size'] * CONFIG['gradient_accumulation']}\")\n",
    "print(f\"  Max steps: {CONFIG['max_steps']}\")\n",
    "print(f\"  Precision: {'fp16' if CONFIG['fp16'] else 'fp32'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "# Clear GPU cache before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fine-tuning Summary\n",
    "\n",
    "**Is LoRA \"real\" fine-tuning?**\n",
    "\n",
    "LoRA is a form of **parameter-efficient fine-tuning (PEFT)**, not full fine-tuning. The distinction:\n",
    "\n",
    "- **Full fine-tuning**: Updates all 1.5B parameters. Higher capacity but requires more VRAM and risks overfitting.\n",
    "- **LoRA**: Updates only ~15M adapter parameters (~1%). More efficient, preserves base knowledge, still achieves strong domain adaptation.\n",
    "\n",
    "For domain adaptation (like VHP historical audio), LoRA is often preferred because:\n",
    "1. The base model already has strong ASR capabilities\n",
    "2. We want to adapt to acoustic characteristics, not relearn language\n",
    "3. Limited training data makes full fine-tuning prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA weights\n",
    "lora_path = os.path.join(CONFIG[\"output_dir\"], \"lora-weights\")\n",
    "model.save_pretrained(lora_path)\n",
    "processor.save_pretrained(lora_path)\n",
    "\n",
    "print(f\"LoRA weights saved to: {lora_path}\")\n",
    "\n",
    "# Optionally merge and save full model\n",
    "print(\"\\nMerging LoRA weights with base model...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_path = os.path.join(CONFIG[\"output_dir\"], \"merged-model\")\n",
    "merged_model.save_pretrained(merged_path)\n",
    "processor.save_pretrained(merged_path)\n",
    "\n",
    "print(f\"Merged model saved to: {merged_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"=\"*60)\n",
    "print(\"FINE-TUNING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBase Model: {CONFIG['model_name']}\")\n",
    "print(f\"LoRA config: r={CONFIG['lora_r']}, alpha={CONFIG['lora_alpha']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Train parquet: {CONFIG['train_parquet']}\")\n",
    "print(f\"  Val parquet: {CONFIG['val_parquet']}\")\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  LoRA weights: {lora_path}\")\n",
    "print(f\"  Merged model: {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Inference\n",
    "\n",
    "Quick test of the fine-tuned model on samples from the test set.\n",
    "\n",
    "**Note**: The merged model is in HuggingFace format. Our production `infer_whisper.py` uses faster-whisper (CTranslate2 format) for speed. For this quick test, we use HuggingFace transformers pipeline directly. See \"Next Steps\" for converting to faster-whisper format for full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test using HuggingFace transformers pipeline\n",
    "# (For production inference, convert to faster-whisper format - see Next Steps)\n",
    "\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "# Load the merged fine-tuned model\n",
    "pipe = hf_pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=merged_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "# Load test parquet (small sample)\n",
    "test_parquet = CONFIG[\"train_parquet\"].replace(\"_train\", \"_test\")\n",
    "df_test = data_loader.load_vhp_dataset(test_parquet, sample_size=10)\n",
    "\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "print(f\"Model: {merged_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test samples\n",
    "from tempfile import NamedTemporaryFile\n",
    "from pydub import AudioSegment\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for idx, row in df_test.iterrows():\n",
    "    blob_paths = data_loader.get_blob_path_for_row(row, idx, CONFIG[\"blob_prefix\"])\n",
    "    \n",
    "    for blob_path in blob_paths:\n",
    "        if azure_utils.blob_exists(blob_path):\n",
    "            print(f\"[{idx}] Processing: {blob_path}\")\n",
    "            \n",
    "            try:\n",
    "                # Download audio\n",
    "                audio_bytes = azure_utils.download_blob_to_memory(blob_path)\n",
    "                \n",
    "                # Save to temp file\n",
    "                with NamedTemporaryFile(suffix=Path(blob_path).suffix, delete=False) as tmp:\n",
    "                    tmp.write(audio_bytes)\n",
    "                    tmp_path = tmp.name\n",
    "                \n",
    "                # Convert to wav 16kHz mono (like infer_whisper.py)\n",
    "                audio_seg = AudioSegment.from_file(tmp_path)\n",
    "                audio_seg = audio_seg.set_frame_rate(16000).set_channels(1)\n",
    "                \n",
    "                # Limit to first 5 minutes for quick test\n",
    "                if len(audio_seg) > 300000:  # 300 seconds in ms\n",
    "                    audio_seg = audio_seg[:300000]\n",
    "                \n",
    "                wav_path = tmp_path.replace(Path(blob_path).suffix, '.wav')\n",
    "                audio_seg.export(wav_path, format='wav')\n",
    "                \n",
    "                # Run inference\n",
    "                result = pipe(wav_path, return_timestamps=True)\n",
    "                hypothesis = result[\"text\"]\n",
    "                \n",
    "                # Get ground truth\n",
    "                gt = clean_raw_transcript_str(row.get('fulltext_file_str', ''))\n",
    "                \n",
    "                test_results.append({\n",
    "                    \"file_id\": idx,\n",
    "                    \"hypothesis\": hypothesis,\n",
    "                    \"ground_truth\": gt,\n",
    "                    \"blob_path\": blob_path\n",
    "                })\n",
    "                \n",
    "                # Cleanup\n",
    "                os.unlink(tmp_path)\n",
    "                if os.path.exists(wav_path):\n",
    "                    os.unlink(wav_path)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "            \n",
    "            break  # Only process first available blob path\n",
    "\n",
    "print(f\"\\nCompleted: {len(test_results)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View test results\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST INFERENCE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for r in test_results[:3]:  # Show first 3\n",
    "    print(f\"\\nFile ID: {r['file_id']}\")\n",
    "    print(f\"Blob: {r['blob_path']}\")\n",
    "    print(f\"\\nHypothesis (first 300 chars):\")\n",
    "    print(r['hypothesis'][:300] + \"...\" if len(r['hypothesis']) > 300 else r['hypothesis'])\n",
    "    print(f\"\\nGround truth (first 300 chars):\")\n",
    "    print(r['ground_truth'][:300] + \"...\" if len(r['ground_truth']) > 300 else r['ground_truth'])\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Save test results\n",
    "test_output_dir = Path(CONFIG[\"output_dir\"]) / \"test-inference\"\n",
    "test_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_test_results = pd.DataFrame(test_results)\n",
    "df_test_results.to_parquet(test_output_dir / \"test_results.parquet\", index=False)\n",
    "print(f\"\\nTest results saved to: {test_output_dir / 'test_results.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "To run full evaluation on the test set using the production inference pipeline:\n",
    "\n",
    "1. **Convert to faster-whisper format** (optional, for speed):\n",
    "   ```bash\n",
    "   ct2-transformers-converter --model {merged_path} --output_dir models/whisper-large-v3-vhp-lora\n",
    "   ```\n",
    "\n",
    "2. **Create inference config** (e.g., `configs/runs/vhp-pre2010-whisper-large-v3-lora-sample100.yaml`):\n",
    "   ```yaml\n",
    "   experiment_id: vhp-pre2010-whisper-large-v3-lora-sample100\n",
    "   model:\n",
    "     name: \"whisper-large-v3-lora\"\n",
    "     dir: \"./models/whisper-large-v3-vhp-lora\"  # or use HF path\n",
    "     batch_size: 12\n",
    "     device: \"cuda\"\n",
    "     compute_type: \"float16\"\n",
    "   input:\n",
    "     source: \"azure_blob\"\n",
    "     parquet_path: \"data/raw/loc/veterans_history_project_resources_pre2010_test.parquet\"\n",
    "     blob_prefix: \"loc_vhp\"\n",
    "     sample_size: 100\n",
    "   output:\n",
    "     dir: \"outputs/vhp-pre2010-whisper-large-v3-lora-sample100\"\n",
    "   ```\n",
    "\n",
    "3. **Run inference**:\n",
    "   ```bash\n",
    "   uv run python scripts/infer_whisper.py --config configs/runs/vhp-pre2010-whisper-large-v3-lora-sample100.yaml\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
