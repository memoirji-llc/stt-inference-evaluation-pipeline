{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Whisper Large-v3 with LoRA/PEFT\n",
    "\n",
    "**Target Hardware**: RunPod A6000 (48GB VRAM)\n",
    "\n",
    "This notebook fine-tunes Whisper large-v3 on VHP oral history audio using LoRA (Low-Rank Adaptation).\n",
    "\n",
    "## Key Configuration\n",
    "- **Model**: openai/whisper-large-v3 (via HuggingFace transformers)\n",
    "- **Method**: LoRA with r=32, alpha=64\n",
    "- **Learning Rate**: 1e-5 (CRITICAL: not 1e-3)\n",
    "- **Precision**: fp16 (NOT int8 for V3)\n",
    "- **Batch Size**: 4 with gradient accumulation 4 (effective: 16)\n",
    "\n",
    "## Data Requirements\n",
    "- Parquet files: `veterans_history_project_resources_pre2010_train.parquet` and `_val.parquet`\n",
    "- Azure blob storage connection for audio files\n",
    "\n",
    "### How Many Training Samples Do You Need?\n",
    "\n",
    "**Audio constraint**: â‰¤30 seconds per sample (Whisper's max context). VHP interviews are 30-60+ minutes, so we use **NeMo Forced Aligner** to create shorter segments.\n",
    "\n",
    "**Projected sample count from NFA segmentation** (500-file run):\n",
    "- Success rate: ~15% (files >30min skipped for CUDA OOM prevention)\n",
    "- Segments per file: 30-40\n",
    "- **Total: 2,250-3,000 segments** (sufficient for LoRA)\n",
    "\n",
    "**Is this enough?** Yes. LoRA trains only ~1% of parameters for domain adaptation. Industry benchmarks show 1,000-5,000 samples work well for acoustic adaptation without retraining language understanding.\n",
    "\n",
    "**References:**\n",
    "- [NeMo Forced Aligner](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speech_data_processor.html)\n",
    "- [Whisper LoRA Fine-tuning (HuggingFace)](https://huggingface.co/blog/fine-tune-whisper)\n",
    "\n",
    "See [learnings/whisper-lora-finetuning.md](../learnings/whisper-lora-finetuning.md) for gotchas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Dependencies\n",
    "\n",
    "Fine-tuning requires additional packages not in the base project. Add them via uv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fine-tuning dependencies to pyproject.toml (run once)\n",
    "# uv add peft accelerate\n",
    "#\n",
    "# For A6000 with CUDA 11.8:\n",
    "# uv add torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load Azure credentials from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../credentials/creds.env')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset, Audio\n",
    "\n",
    "# IMPORTANT: Import HuggingFace evaluate BEFORE adding scripts to path\n",
    "import evaluate as hf_evaluate\n",
    "\n",
    "# NOW add scripts directory to path for local imports\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"scripts\"))\n",
    "\n",
    "# Import project modules (after sys.path modification)\n",
    "import data_loader\n",
    "import azure_utils\n",
    "\n",
    "# Import specific function from local scripts/evaluate.py\n",
    "# Use importlib to avoid confusion\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"local_evaluate\", Path.cwd().parent / \"scripts\" / \"evaluate.py\")\n",
    "local_evaluate = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(local_evaluate)\n",
    "clean_raw_transcript_str = local_evaluate.clean_raw_transcript_str\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths - VHP parquet files (same naming as data splits)\n",
    "    \"train_parquet\": \"../data/raw/loc/veterans_history_project_resources_pre2010_train.parquet\",\n",
    "    \"val_parquet\": \"../data/raw/loc/veterans_history_project_resources_pre2010_val.parquet\",\n",
    "    \n",
    "    # Azure blob settings (same as inference configs)\n",
    "    \"blob_prefix\": \"loc_vhp\",\n",
    "    \n",
    "    # Sampling (set to None to use all data, or small number for testing)\n",
    "    \"train_sample_size\": 100,  # Set to None for full training\n",
    "    \"val_sample_size\": 20,\n",
    "    \"random_seed\": 42,\n",
    "    \n",
    "    # Output directory - follows convention: {dataset}-{model}-{task}-{infra}\n",
    "    \"output_dir\": \"../outputs/vhp-pre2010-whisper-large-v3-lora-ft-a6000\",\n",
    "    \n",
    "    # Model - using HuggingFace transformers (not faster-whisper, which is inference-only)\n",
    "    # Note: For inference we use faster-whisper, but for fine-tuning we need the original HF model\n",
    "    \"model_name\": \"openai/whisper-large-v3\",\n",
    "    \n",
    "    # LoRA configuration\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    \"learning_rate\": 1e-5,           # CRITICAL: Use 1e-5 for V3 (not 1e-3)\n",
    "    \"batch_size\": 4,                  # Per device\n",
    "    \"gradient_accumulation\": 4,       # Effective batch = 16\n",
    "    \"warmup_steps\": 500,\n",
    "    \"max_steps\": 5000,               # Adjust based on data size\n",
    "    \"eval_steps\": 500,\n",
    "    \"save_steps\": 500,\n",
    "    \n",
    "    # Precision\n",
    "    \"fp16\": True,                    # Use fp16 for V3\n",
    "    \"bf16\": False,\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "print(f\"Output directory: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small test\n",
    "CONFIG = {\n",
    "    # Data paths - VHP parquet files (same naming as data splits)\n",
    "    \"train_parquet\": \"../data/raw/loc/veterans_history_project_resources_pre2010_train_nfa_segmented_demo.parquet\",\n",
    "    \"val_parquet\": \"../data/raw/loc/veterans_history_project_resources_pre2010_train_nfa_segmented_demo.parquet\",\n",
    "    \n",
    "    # Azure blob settings (same as inference configs)\n",
    "    \"blob_prefix\": \"loc_vhp\",\n",
    "    \n",
    "    # Sampling (set to None to use all data, or small number for testing)\n",
    "    \"train_sample_size\": None,\n",
    "    \"val_sample_size\": None,\n",
    "    \"random_seed\": 42,\n",
    "    \n",
    "    # Output directory - follows convention: {dataset}-{model}-{task}-{infra}\n",
    "    \"output_dir\": \"../outputs/vhp-pre2010-whisper-large-v3-lora-ft-a6000\",\n",
    "    \n",
    "    # Model - using HuggingFace transformers (not faster-whisper, which is inference-only)\n",
    "    # Note: For inference we use faster-whisper, but for fine-tuning we need the original HF model\n",
    "    \"model_name\": \"openai/whisper-large-v3\",\n",
    "    \n",
    "    # LoRA configuration\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    \"learning_rate\": 1e-5,           # CRITICAL: Use 1e-5 for V3 (not 1e-3)\n",
    "    \"batch_size\": 4,                  # Per device\n",
    "    \"gradient_accumulation\": 4,       # Effective batch = 16\n",
    "    \"warmup_steps\": 500,\n",
    "    \"max_steps\": 100,  \n",
    "    \"eval_steps\": 50,\n",
    "    \"save_steps\": 50,\n",
    "    \n",
    "    # Precision\n",
    "    \"fp16\": True,                    # Use fp16 for V3\n",
    "    \"bf16\": False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data\n",
    "\n",
    "Using existing `data_loader.py` and `azure_utils.py` from scripts/.\n",
    "\n",
    "Ground truth is extracted from `fulltext_file_str` column using `clean_raw_transcript_str()` from evaluate.py (see [notebooks/evals_learn.ipynb](./evals_learn.ipynb) for details on how this works)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetune_dataset(parquet_path: str, sample_size: int = None, random_seed: int = 42):\n",
    "    \"\"\"\n",
    "    Load dataset for fine-tuning.\n",
    "    \n",
    "    Handles both:\n",
    "    - Segmented parquets (with segmented_audio_url column) - loads directly\n",
    "    - Original parquets (without segmented_audio_url) - uses data_loader filtering\n",
    "    \"\"\"\n",
    "    # Load parquet to check if it's segmented\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    \n",
    "    is_segmented = 'segmented_audio_url' in df.columns\n",
    "    \n",
    "    if is_segmented:\n",
    "        # For segmented parquets, don't use data_loader filtering\n",
    "        # (it filters by video_url/audio_url which segmented parquets don't have)\n",
    "        print(f\"Detected segmented parquet with {len(df)} segments\")\n",
    "        \n",
    "        # Filter to rows with valid segmented_audio_url and transcript\n",
    "        df = df[df['segmented_audio_url'].notna() & (df['segmented_audio_url'] != '')]\n",
    "        print(f\"Filtered to {len(df)} segments with segmented_audio_url\")\n",
    "        \n",
    "        df = df[df['segmented_audio_transcript'].notna() & (df['segmented_audio_transcript'] != '')]\n",
    "        print(f\"Filtered to {len(df)} segments with segmented_audio_transcript\")\n",
    "        \n",
    "        # Apply sampling if requested\n",
    "        if sample_size is not None and sample_size < len(df):\n",
    "            df = df.sample(n=sample_size, random_state=random_seed)\n",
    "            print(f\"Sampled {sample_size} segments\")\n",
    "        else:\n",
    "            print(f\"Using all {len(df)} segments (no sampling)\")\n",
    "    else:\n",
    "        # For original parquets, use existing data_loader\n",
    "        df = data_loader.load_vhp_dataset(\n",
    "            parquet_path=parquet_path,\n",
    "            sample_size=sample_size,\n",
    "            filter_has_transcript=True,\n",
    "            filter_has_media=True\n",
    "        )\n",
    "    \n",
    "    print(f\"Loaded {len(df)} samples\")\n",
    "    return df\n",
    "\n",
    "# Load train and validation sets\n",
    "print(\"Loading training data...\")\n",
    "df_train = load_finetune_dataset(\n",
    "    CONFIG[\"train_parquet\"], \n",
    "    sample_size=CONFIG[\"train_sample_size\"],\n",
    "    random_seed=CONFIG[\"random_seed\"]\n",
    ")\n",
    "\n",
    "print(\"\\nLoading validation data...\")\n",
    "df_val = load_finetune_dataset(\n",
    "    CONFIG[\"val_parquet\"],\n",
    "    sample_size=CONFIG[\"val_sample_size\"],\n",
    "    random_seed=CONFIG[\"random_seed\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(df_train)} samples\")\n",
    "print(f\"Val: {len(df_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hf_dataset(df: pd.DataFrame, blob_prefix: str, max_duration_sec: int = 30):\n",
    "    \"\"\"\n",
    "    Convert DataFrame to HuggingFace Dataset with audio and cleaned transcripts.\n",
    "    \n",
    "    Handles both:\n",
    "    - Segmented parquets (with segmented_audio_url column) - uses pre-segmented audio\n",
    "    - Original parquets (without segmented_audio_url) - downloads and filters by duration\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with VHP data\n",
    "        blob_prefix: Azure blob prefix\n",
    "        max_duration_sec: Maximum audio duration in seconds (only for non-segmented data)\n",
    "    \"\"\"\n",
    "    from tempfile import NamedTemporaryFile\n",
    "    from pydub import AudioSegment\n",
    "    import librosa\n",
    "    import time\n",
    "    \n",
    "    # Check if this is a segmented parquet\n",
    "    is_segmented = 'segmented_audio_url' in df.columns\n",
    "    print(f\"[DEBUG] prepare_hf_dataset: is_segmented={is_segmented}, df size={len(df)}\")\n",
    "    \n",
    "    records = []\n",
    "    skipped_too_long = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n[DEBUG] Processing row {idx}...\")\n",
    "        \n",
    "        # Get transcript\n",
    "        if is_segmented:\n",
    "            # Segmented parquet uses segmented_audio_transcript column\n",
    "            transcript = row.get('segmented_audio_transcript', '')\n",
    "            print(f\"  - Using segmented_audio_transcript: {len(transcript)} chars\")\n",
    "        else:\n",
    "            # Original parquet uses fulltext_file_str with cleaning\n",
    "            raw_transcript = row.get('fulltext_file_str', '')\n",
    "            transcript = clean_raw_transcript_str(raw_transcript)\n",
    "            print(f\"  - Cleaned transcript: {len(transcript)} chars\")\n",
    "        \n",
    "        if not transcript.strip():\n",
    "            print(f\"  - SKIP: empty transcript\")\n",
    "            continue\n",
    "        \n",
    "        # Get blob path\n",
    "        if is_segmented:\n",
    "            # Use pre-segmented audio path\n",
    "            blob_path = row.get('segmented_audio_url', '')\n",
    "            if not blob_path:\n",
    "                print(f\"  - SKIP: no segmented_audio_url\")\n",
    "                continue\n",
    "            blob_path_candidates = [blob_path]\n",
    "            print(f\"  - Blob path: {blob_path}\")\n",
    "        else:\n",
    "            # Use original full-length audio path\n",
    "            blob_path_candidates = data_loader.get_blob_path_for_row(row, idx, blob_prefix)\n",
    "            if not blob_path_candidates:\n",
    "                print(f\"  - SKIP: no blob path\")\n",
    "                continue\n",
    "            print(f\"  - Blob path candidates: {blob_path_candidates}\")\n",
    "        \n",
    "        # Download audio from Azure blob\n",
    "        audio_data = None\n",
    "        for blob_path in blob_path_candidates:\n",
    "            try:\n",
    "                print(f\"  - Checking if blob exists: {blob_path}\")\n",
    "                if not azure_utils.blob_exists(blob_path):\n",
    "                    print(f\"  - Blob does not exist, trying next...\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"  - Downloading blob...\")\n",
    "                download_start = time.time()\n",
    "                audio_bytes = azure_utils.download_blob_to_memory(blob_path)\n",
    "                print(f\"  - Downloaded {len(audio_bytes)} bytes in {time.time() - download_start:.2f}s\")\n",
    "                \n",
    "                # Convert to WAV 16kHz mono using pydub\n",
    "                with NamedTemporaryFile(suffix=Path(blob_path).suffix, delete=False) as tmp:\n",
    "                    tmp.write(audio_bytes)\n",
    "                    tmp_path = tmp.name\n",
    "                \n",
    "                print(f\"  - Converting audio to 16kHz mono WAV...\")\n",
    "                convert_start = time.time()\n",
    "                audio_seg = AudioSegment.from_file(tmp_path)\n",
    "                audio_seg = audio_seg.set_frame_rate(16000).set_channels(1)\n",
    "                \n",
    "                # Export to wav\n",
    "                wav_path = tmp_path.replace(Path(blob_path).suffix, '.wav')\n",
    "                audio_seg.export(wav_path, format='wav')\n",
    "                print(f\"  - Conversion done in {time.time() - convert_start:.2f}s\")\n",
    "                \n",
    "                # Load as numpy array\n",
    "                print(f\"  - Loading audio with librosa...\")\n",
    "                load_start = time.time()\n",
    "                audio_data, sr = librosa.load(wav_path, sr=16000)\n",
    "                print(f\"  - Loaded audio: {len(audio_data)} samples in {time.time() - load_start:.2f}s\")\n",
    "                \n",
    "                # For non-segmented data, skip if audio is too long\n",
    "                if not is_segmented:\n",
    "                    audio_duration_sec = len(audio_data) / 16000\n",
    "                    if audio_duration_sec > max_duration_sec:\n",
    "                        print(f\"  - SKIP: audio too long ({audio_duration_sec:.1f}s > {max_duration_sec}s)\")\n",
    "                        skipped_too_long += 1\n",
    "                        audio_data = None\n",
    "                        os.unlink(tmp_path)\n",
    "                        if os.path.exists(wav_path):\n",
    "                            os.unlink(wav_path)\n",
    "                        break\n",
    "                \n",
    "                # Cleanup temp files\n",
    "                os.unlink(tmp_path)\n",
    "                if os.path.exists(wav_path):\n",
    "                    os.unlink(wav_path)\n",
    "                \n",
    "                print(f\"  - SUCCESS: Processed in {time.time() - start_time:.2f}s total\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"  - ERROR downloading {blob_path}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        if audio_data is None:\n",
    "            print(f\"  - SKIP: No valid audio data obtained\")\n",
    "            continue\n",
    "        \n",
    "        records.append({\n",
    "            \"audio\": {\"array\": audio_data, \"sampling_rate\": 16000},\n",
    "            \"sentence\": transcript\n",
    "        })\n",
    "        \n",
    "        print(f\"  - Added to records (total: {len(records)})\")\n",
    "    \n",
    "    print(f\"\\n[DEBUG] Final summary:\")\n",
    "    print(f\"  - Total valid samples: {len(records)}\")\n",
    "    if not is_segmented:\n",
    "        print(f\"  - Skipped (too long): {skipped_too_long}\")\n",
    "    \n",
    "    if len(records) == 0:\n",
    "        if is_segmented:\n",
    "            raise ValueError(\"No valid segmented samples found. Check segmented_audio_url and segmented_audio_transcript columns.\")\n",
    "        else:\n",
    "            raise ValueError(f\"No samples found with duration <= {max_duration_sec}s. \"\n",
    "                           \"VHP files are typically 30-60+ minute interviews. \"\n",
    "                           \"Use NeMo Forced Aligner to create segmented parquets.\")\n",
    "    \n",
    "    # Create HuggingFace dataset\n",
    "    print(f\"[DEBUG] Creating HuggingFace dataset...\")\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"audio\": [r[\"audio\"] for r in records],\n",
    "        \"sentence\": [r[\"sentence\"] for r in records]\n",
    "    })\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    print(f\"[DEBUG] HuggingFace dataset created successfully\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"../data/raw/loc/veterans_history_project_resources_pre2010_train_nfa_segmented_demo.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare HuggingFace datasets\n",
    "print(\"Preparing training dataset (downloading audio from Azure)...\")\n",
    "train_dataset = prepare_hf_dataset(df_train, CONFIG[\"blob_prefix\"])\n",
    "\n",
    "print(\"\\nPreparing validation dataset...\")\n",
    "val_dataset = prepare_hf_dataset(df_val, CONFIG[\"blob_prefix\"])\n",
    "\n",
    "print(f\"\\nFinal dataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a random sample\n",
    "import random\n",
    "random.seed(CONFIG[\"random_seed\"])\n",
    "\n",
    "sample_idx = random.randint(0, len(train_dataset) - 1)\n",
    "sample = train_dataset[sample_idx]\n",
    "\n",
    "print(f\"Sample {sample_idx}:\")\n",
    "print(f\"  Audio duration: {len(sample['audio']['array']) / sample['audio']['sampling_rate']:.1f}s\")\n",
    "print(f\"  Transcript preview: {sample['sentence'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model\n",
    "\n",
    "**Note on model choice**: For fine-tuning we use `openai/whisper-large-v3` from HuggingFace transformers. This is different from inference where we use `faster-whisper` (CTranslate2 optimized). The fine-tuned weights can later be converted to faster-whisper format for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor\n",
    "processor = WhisperProcessor.from_pretrained(CONFIG[\"model_name\"])\n",
    "\n",
    "# Load model in fp16\n",
    "print(f\"Loading model: {CONFIG['model_name']}\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=False,  # IMPORTANT: Don't use 8-bit for V3 (causes hallucinations)\n",
    ")\n",
    "\n",
    "# Clear forced decoder IDs (important for fine-tuning)\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "print(f\"Model loaded. Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply LoRA\n",
    "\n",
    "### Why LoRA for Whisper Fine-tuning?\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is chosen over full fine-tuning for several reasons:\n",
    "\n",
    "1. **Memory Efficiency**: Full fine-tuning of Whisper large-v3 (1.5B params) requires 30-40GB VRAM. LoRA reduces this to ~20GB by only training adapter weights.\n",
    "\n",
    "2. **Catastrophic Forgetting Prevention**: LoRA preserves the base model's general ASR capabilities while adapting to domain-specific audio. Full fine-tuning risks losing pre-trained knowledge.\n",
    "\n",
    "3. **Faster Training**: Only ~1% of parameters are trainable (15.7M vs 1.5B), significantly reducing training time.\n",
    "\n",
    "4. **Easy Model Merging**: LoRA weights can be merged with base model for deployment, or kept separate for A/B testing.\n",
    "\n",
    "**References:**\n",
    "- [HuggingFace PEFT Whisper Training](https://github.com/huggingface/peft/blob/main/examples/int8_training/peft_bnb_whisper_large_v2_training.ipynb)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False  # Disable cache for training\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    \"\"\"Preprocess audio and text for training.\"\"\"\n",
    "    audio = batch[\"audio\"]\n",
    "    \n",
    "    # Extract features from audio\n",
    "    input_features = processor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features[0]\n",
    "    \n",
    "    # Tokenize transcription\n",
    "    labels = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
    "    \n",
    "    # Return ONLY the processed features (no audio, sentence, etc.)\n",
    "    return {\n",
    "        \"input_features\": input_features,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "print(\"Preprocessing training data...\")\n",
    "print(f\"Dataset size: {len(train_dataset)} samples\")\n",
    "\n",
    "# Process manually to avoid multiprocessing crashes\n",
    "processed_train = {\"input_features\": [], \"labels\": []}\n",
    "for i in range(len(train_dataset)):\n",
    "    sample = train_dataset[i]\n",
    "    processed = prepare_dataset(sample)\n",
    "    processed_train[\"input_features\"].append(processed[\"input_features\"])\n",
    "    processed_train[\"labels\"].append(processed[\"labels\"])\n",
    "\n",
    "# Create dataset from dict (this ensures ONLY these columns exist)\n",
    "import datasets\n",
    "train_dataset = datasets.Dataset.from_dict(processed_train)\n",
    "print(f\"Training preprocessing complete: {len(train_dataset)} samples\")\n",
    "print(f\"Columns: {train_dataset.column_names}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Preprocessing validation data...\")\n",
    "\n",
    "processed_val = {\"input_features\": [], \"labels\": []}\n",
    "for i in range(len(val_dataset)):\n",
    "    sample = val_dataset[i]\n",
    "    processed = prepare_dataset(sample)\n",
    "    processed_val[\"input_features\"].append(processed[\"input_features\"])\n",
    "    processed_val[\"labels\"].append(processed[\"labels\"])\n",
    "\n",
    "val_dataset = datasets.Dataset.from_dict(processed_val)\n",
    "print(f\"Validation preprocessing complete: {len(val_dataset)} samples\")\n",
    "print(f\"Columns: {val_dataset.column_names}\")\n",
    "print(f\"\\nAll preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Check dataset columns after preprocessing\n",
    "print(\"Train dataset columns:\", train_dataset.column_names)\n",
    "print(\"Val dataset columns:\", val_dataset.column_names)\n",
    "print(\"\\nTrain dataset features:\")\n",
    "print(train_dataset.features)\n",
    "print(\"\\nSample from train_dataset[0]:\")\n",
    "print(train_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"Data collator for Whisper seq2seq training.\"\"\"\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]):\n",
    "        # Extract input_features (mel spectrograms) for padding\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Extract labels (token ids) for padding\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding token id with -100 (ignored by loss)\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # If bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        # CRITICAL: Return ONLY input_features and labels\n",
    "        # Don't return the whole batch which might have extra keys\n",
    "        return {\n",
    "            \"input_features\": batch[\"input_features\"],\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "# WER metric (using HuggingFace evaluate library imported at top of notebook)\n",
    "wer_metric = hf_evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute WER for evaluation.\"\"\"\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 with pad token\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Decode\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "print(\"Data collator and metrics ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Configuration\n",
    "\n",
    "### Why Seq2SeqTrainer?\n",
    "\n",
    "Whisper is a sequence-to-sequence (encoder-decoder) model that takes audio input and generates text output. `Seq2SeqTrainer` from HuggingFace is specifically designed for this architecture and provides:\n",
    "\n",
    "1. **Proper generation during evaluation**: Uses `model.generate()` instead of forward pass\n",
    "2. **Label handling**: Correctly handles the decoder input/output shift\n",
    "3. **Beam search support**: For better generation quality during eval\n",
    "\n",
    "**Reference**: [HuggingFace Fine-tune Whisper Guide](https://huggingface.co/blog/fine-tune-whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    max_steps=CONFIG[\"max_steps\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    bf16=CONFIG[\"bf16\"],\n",
    "    weight_decay=0.01,\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,  # Required for PEFT - PeftModel forward signature differs from base model\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']} x {CONFIG['gradient_accumulation']} = {CONFIG['batch_size'] * CONFIG['gradient_accumulation']}\")\n",
    "print(f\"  Max steps: {CONFIG['max_steps']}\")\n",
    "print(f\"  Precision: {'fp16' if CONFIG['fp16'] else 'fp32'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,  # CRITICAL: Use feature_extractor, not tokenizer!\n",
    ")\n",
    "\n",
    "# Clear GPU cache before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fine-tuning Summary\n",
    "\n",
    "**Is LoRA \"real\" fine-tuning?**\n",
    "\n",
    "LoRA is a form of **parameter-efficient fine-tuning (PEFT)**, not full fine-tuning. The distinction:\n",
    "\n",
    "- **Full fine-tuning**: Updates all 1.5B parameters. Higher capacity but requires more VRAM and risks overfitting.\n",
    "- **LoRA**: Updates only ~15M adapter parameters (~1%). More efficient, preserves base knowledge, still achieves strong domain adaptation.\n",
    "\n",
    "For domain adaptation (like VHP historical audio), LoRA is often preferred because:\n",
    "1. The base model already has strong ASR capabilities\n",
    "2. We want to adapt to acoustic characteristics, not relearn language\n",
    "3. Limited training data makes full fine-tuning prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA weights\n",
    "lora_path = os.path.join(CONFIG[\"output_dir\"], \"lora-weights\")\n",
    "model.save_pretrained(lora_path)\n",
    "processor.save_pretrained(lora_path)\n",
    "\n",
    "print(f\"LoRA weights saved to: {lora_path}\")\n",
    "\n",
    "# Optionally merge and save full model\n",
    "print(\"\\nMerging LoRA weights with base model...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_path = os.path.join(CONFIG[\"output_dir\"], \"merged-model\")\n",
    "merged_model.save_pretrained(merged_path)\n",
    "processor.save_pretrained(merged_path)\n",
    "\n",
    "print(f\"Merged model saved to: {merged_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"=\"*60)\n",
    "print(\"FINE-TUNING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBase Model: {CONFIG['model_name']}\")\n",
    "print(f\"LoRA config: r={CONFIG['lora_r']}, alpha={CONFIG['lora_alpha']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Train parquet: {CONFIG['train_parquet']}\")\n",
    "print(f\"  Val parquet: {CONFIG['val_parquet']}\")\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  LoRA weights: {lora_path}\")\n",
    "print(f\"  Merged model: {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Inference\n",
    "\n",
    "Quick test of the fine-tuned model on samples from the test set.\n",
    "\n",
    "**Note**: The merged model is in HuggingFace format. Our production `infer_whisper.py` uses faster-whisper (CTranslate2 format) for speed. For this quick test, we use HuggingFace transformers pipeline directly. See \"Next Steps\" for converting to faster-whisper format for full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test using HuggingFace transformers pipeline\n",
    "# (For production inference, convert to faster-whisper format - see Next Steps)\n",
    "\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "# Load the merged fine-tuned model\n",
    "pipe = hf_pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=merged_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "# Load test parquet (small sample)\n",
    "test_parquet = CONFIG[\"train_parquet\"].replace(\"_train\", \"_test\")\n",
    "df_test = data_loader.load_vhp_dataset(test_parquet, sample_size=10)\n",
    "\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "print(f\"Model: {merged_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test samples\n",
    "from tempfile import NamedTemporaryFile\n",
    "from pydub import AudioSegment\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for idx, row in df_test.iterrows():\n",
    "    blob_paths = data_loader.get_blob_path_for_row(row, idx, CONFIG[\"blob_prefix\"])\n",
    "    \n",
    "    for blob_path in blob_paths:\n",
    "        if azure_utils.blob_exists(blob_path):\n",
    "            print(f\"[{idx}] Processing: {blob_path}\")\n",
    "            \n",
    "            try:\n",
    "                # Download audio\n",
    "                audio_bytes = azure_utils.download_blob_to_memory(blob_path)\n",
    "                \n",
    "                # Save to temp file\n",
    "                with NamedTemporaryFile(suffix=Path(blob_path).suffix, delete=False) as tmp:\n",
    "                    tmp.write(audio_bytes)\n",
    "                    tmp_path = tmp.name\n",
    "                \n",
    "                # Convert to wav 16kHz mono (like infer_whisper.py)\n",
    "                audio_seg = AudioSegment.from_file(tmp_path)\n",
    "                audio_seg = audio_seg.set_frame_rate(16000).set_channels(1)\n",
    "                \n",
    "                # Limit to first 5 minutes for quick test\n",
    "                if len(audio_seg) > 300000:  # 300 seconds in ms\n",
    "                    audio_seg = audio_seg[:300000]\n",
    "                \n",
    "                wav_path = tmp_path.replace(Path(blob_path).suffix, '.wav')\n",
    "                audio_seg.export(wav_path, format='wav')\n",
    "                \n",
    "                # Run inference\n",
    "                result = pipe(wav_path, return_timestamps=True)\n",
    "                hypothesis = result[\"text\"]\n",
    "                \n",
    "                # Get ground truth\n",
    "                gt = clean_raw_transcript_str(row.get('fulltext_file_str', ''))\n",
    "                \n",
    "                test_results.append({\n",
    "                    \"file_id\": idx,\n",
    "                    \"hypothesis\": hypothesis,\n",
    "                    \"ground_truth\": gt,\n",
    "                    \"blob_path\": blob_path\n",
    "                })\n",
    "                \n",
    "                # Cleanup\n",
    "                os.unlink(tmp_path)\n",
    "                if os.path.exists(wav_path):\n",
    "                    os.unlink(wav_path)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "            \n",
    "            break  # Only process first available blob path\n",
    "\n",
    "print(f\"\\nCompleted: {len(test_results)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View test results\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST INFERENCE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for r in test_results[:3]:  # Show first 3\n",
    "    print(f\"\\nFile ID: {r['file_id']}\")\n",
    "    print(f\"Blob: {r['blob_path']}\")\n",
    "    print(f\"\\nHypothesis (first 300 chars):\")\n",
    "    print(r['hypothesis'][:300] + \"...\" if len(r['hypothesis']) > 300 else r['hypothesis'])\n",
    "    print(f\"\\nGround truth (first 300 chars):\")\n",
    "    print(r['ground_truth'][:300] + \"...\" if len(r['ground_truth']) > 300 else r['ground_truth'])\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Save test results\n",
    "test_output_dir = Path(CONFIG[\"output_dir\"]) / \"test-inference\"\n",
    "test_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_test_results = pd.DataFrame(test_results)\n",
    "df_test_results.to_parquet(test_output_dir / \"test_results.parquet\", index=False)\n",
    "print(f\"\\nTest results saved to: {test_output_dir / 'test_results.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "To run full evaluation on the test set using the production inference pipeline:\n",
    "\n",
    "1. **Convert to faster-whisper format** (optional, for speed):\n",
    "   ```bash\n",
    "   ct2-transformers-converter --model {merged_path} --output_dir models/whisper-large-v3-vhp-lora\n",
    "   ```\n",
    "\n",
    "2. **Create inference config** (e.g., `configs/runs/vhp-pre2010-whisper-large-v3-lora-sample100.yaml`):\n",
    "   ```yaml\n",
    "   experiment_id: vhp-pre2010-whisper-large-v3-lora-sample100\n",
    "   model:\n",
    "     name: \"whisper-large-v3-lora\"\n",
    "     dir: \"./models/whisper-large-v3-vhp-lora\"  # or use HF path\n",
    "     batch_size: 12\n",
    "     device: \"cuda\"\n",
    "     compute_type: \"float16\"\n",
    "   input:\n",
    "     source: \"azure_blob\"\n",
    "     parquet_path: \"data/raw/loc/veterans_history_project_resources_pre2010_test.parquet\"\n",
    "     blob_prefix: \"loc_vhp\"\n",
    "     sample_size: 100\n",
    "   output:\n",
    "     dir: \"outputs/vhp-pre2010-whisper-large-v3-lora-sample100\"\n",
    "   ```\n",
    "\n",
    "3. **Run inference**:\n",
    "   ```bash\n",
    "   uv run python scripts/infer_whisper.py --config configs/runs/vhp-pre2010-whisper-large-v3-lora-sample100.yaml\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
