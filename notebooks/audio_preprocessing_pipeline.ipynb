{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Audio Preprocessing Pipeline - Interactive Walkthrough\n",
    "\n",
    "**Purpose:** Understand and test audio preprocessing methods for archival speech-to-text.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Review audio quality analysis results from 1000 VHP test files\n",
    "2. Step through preprocessing functions on a single example\n",
    "3. Compare raw vs. preprocessed audio (spectrograms, metrics, transcriptions)\n",
    "4. Test different preprocessing variants as a \"playground\"\n",
    "\n",
    "**Preprocessing Methods:**\n",
    "- **Loudness Normalization** - Normalize to target LUFS\n",
    "- **High-Pass Filter** - Remove low-frequency rumble (<80 Hz)\n",
    "- **Noise Reduction** - Remove background noise\n",
    "- **EQ High-Frequency Boost** - Enhance bandwidth-limited audio (TODO)\n",
    "\n",
    "**For production batch processing:** Use `scripts/preprocess_audio.py` (functions are decoupled from this notebook)\n",
    "\n",
    "**For STT inference:** Use existing `infer_*.ipynb` notebooks (they support custom blob prefixes for preprocessed audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyloudnorm as pyln\n",
    "from scipy import signal\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "\n",
    "# Project utilities\n",
    "from cloud.azure_utils import download_blob_to_memory\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../credentials/creds.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Load & Review Analysis Results\n",
    "\n",
    "Review the audio quality analysis to understand what preprocessing is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analysis results\n",
    "analysis_path = Path('../learnings/audio_quality_analysis/audio_quality_analysis.parquet')\n",
    "df_analysis = pd.read_parquet(analysis_path)\n",
    "\n",
    "print(f\"Loaded analysis for {len(df_analysis)} files\")\n",
    "print(f\"\\nColumns: {df_analysis.columns.tolist()}\")\n",
    "print(f\"\\nSuccess rate: {(df_analysis['status'] == 'success').sum()} / {len(df_analysis)}\")\n",
    "\n",
    "# Show first few rows\n",
    "df_analysis.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for successful files\n",
    "df_success = df_analysis[df_analysis['status'] == 'success']\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"AUDIO QUALITY METRICS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(df_success[[\n",
    "    'snr_db', \n",
    "    'spectral_rolloff_hz', \n",
    "    'spectral_flatness',\n",
    "    'spectral_centroid_hz',\n",
    "    'zcr_mean',\n",
    "    'loudness_lufs',\n",
    "    'low_freq_energy_ratio'\n",
    "]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of issues\n",
    "from collections import Counter\n",
    "\n",
    "# Flatten all issues\n",
    "all_issues = [issue for issues in df_success['issues'] for issue in issues]\n",
    "issue_counts = Counter(all_issues)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ISSUE DISTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "for issue, count in issue_counts.most_common():\n",
    "    pct = count / len(df_success) * 100\n",
    "    print(f\"{issue:<30s}: {count:4d} files ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nFiles with NO issues: {(df_success['issues'].str.len() == 0).sum()}\")\n",
    "print(f\"Files with issues: {(df_success['issues'].str.len() > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of preprocessing recommendations\n",
    "all_recs = [rec for recs in df_success['recommended_preprocessing'] for rec in recs]\n",
    "rec_counts = Counter(all_recs)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREPROCESSING RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "for rec, count in rec_counts.most_common():\n",
    "    pct = count / len(df_success) * 100\n",
    "    print(f\"{rec:<30s}: {count:4d} files ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample representative files for each issue category\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE FILES FOR TESTING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Bandwidth-limited severe\n",
    "bandwidth_severe = df_success[df_success['issues'].apply(lambda x: 'bandwidth_limited_severe' in x)]\n",
    "if len(bandwidth_severe) > 0:\n",
    "    sample = bandwidth_severe.iloc[0]\n",
    "    print(f\"\\nBandwidth-limited (severe): {sample['audio_id']}\")\n",
    "    print(f\"  Rolloff: {sample['spectral_rolloff_hz']:.0f} Hz\")\n",
    "    print(f\"  Centroid: {sample['spectral_centroid_hz']:.0f} Hz\")\n",
    "\n",
    "# High noise\n",
    "high_noise = df_success[df_success['issues'].apply(lambda x: 'high_noise_snr' in x or 'high_noise_zcr' in x)]\n",
    "if len(high_noise) > 0:\n",
    "    sample = high_noise.iloc[0]\n",
    "    print(f\"\\nHigh noise: {sample['audio_id']}\")\n",
    "    print(f\"  SNR: {sample['snr_db']:.1f} dB\")\n",
    "    print(f\"  ZCR: {sample['zcr_mean']:.4f}\")\n",
    "\n",
    "# Low loudness\n",
    "low_loud = df_success[df_success['issues'].apply(lambda x: 'low_loudness' in x)]\n",
    "if len(low_loud) > 0:\n",
    "    sample = low_loud.iloc[0]\n",
    "    print(f\"\\nLow loudness: {sample['audio_id']}\")\n",
    "    print(f\"  Loudness: {sample['loudness_lufs']:.1f} LUFS\")\n",
    "\n",
    "# Good quality (no issues)\n",
    "good_quality = df_success[df_success['issues'].str.len() == 0]\n",
    "if len(good_quality) > 0:\n",
    "    sample = good_quality.iloc[0]\n",
    "    print(f\"\\nGood quality (no issues): {sample['audio_id']}\")\n",
    "    print(f\"  SNR: {sample['snr_db']:.1f} dB, Rolloff: {sample['spectral_rolloff_hz']:.0f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Preprocessing Functions - Step-by-Step Walkthrough\n",
    "\n",
    "**Goal:** Understand each preprocessing method by applying it to a single example audio file.\n",
    "\n",
    "**Pattern:** \n",
    "1. Show the **core code** (2-3 lines) that does the preprocessing\n",
    "2. Apply it to the example audio\n",
    "3. Show before/after metrics\n",
    "4. (Optional) Play the audio to hear the difference\n",
    "\n",
    "**Note:** After understanding the logic, these functions are available in `scripts/preprocess_audio.py` for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Step 1: Choose an Example Audio File\n",
    "\n",
    "Pick a file from the analysis results. You can either:\n",
    "- Pick a random file: `df_success.sample(1).iloc[0]`\n",
    "- Pick a specific VHP index: `df_success[df_success['audio_id'] == '1234'].iloc[0]`\n",
    "- Pick by issue type: `df_success[df_success['issues'].apply(lambda x: 'bandwidth_limited' in str(x))].iloc[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Pick a file with bandwidth limitation (recommended for testing)\n",
    "example_file = df_success[df_success['issues'].apply(lambda x: 'bandwidth_limited' in str(x))].iloc[0]\n",
    "\n",
    "# Option 2: Pick a random file\n",
    "# example_file = df_success.sample(1, random_state=42).iloc[0]\n",
    "\n",
    "# Option 3: Pick specific VHP index\n",
    "# example_file = df_success[df_success['audio_id'] == '1234'].iloc[0]\n",
    "\n",
    "print(f\"Selected file: {example_file['audio_id']}\")\n",
    "print(f\"\\nIssues detected: {example_file['issues']}\")\n",
    "print(f\"Recommended preprocessing: {example_file['recommended_preprocessing']}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  SNR: {example_file['snr_db']:.1f} dB\")\n",
    "print(f\"  Rolloff: {example_file['spectral_rolloff_hz']:.0f} Hz\")\n",
    "print(f\"  Centroid: {example_file['spectral_centroid_hz']:.0f} Hz\")\n",
    "print(f\"  Loudness: {example_file['loudness_lufs']:.1f} LUFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Step 2: Download the Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load audio from bytes (we'll use this throughout)\n",
    "def load_audio_bytes(audio_bytes, target_sr=16000):\n",
    "    \"\"\"Load audio from bytes to numpy array.\"\"\"\n",
    "    from pydub import AudioSegment\n",
    "    audio = AudioSegment.from_file(io.BytesIO(audio_bytes))\n",
    "    audio = audio.set_channels(1).set_frame_rate(target_sr)\n",
    "    samples = np.array(audio.get_array_of_samples())\n",
    "    \n",
    "    # Normalize to float32 [-1, 1]\n",
    "    if audio.sample_width == 2:\n",
    "        waveform = samples.astype(np.float32) / 32768.0\n",
    "    elif audio.sample_width == 4:\n",
    "        waveform = samples.astype(np.float32) / 2147483648.0\n",
    "    else:\n",
    "        waveform = samples.astype(np.float32)\n",
    "    \n",
    "    return waveform, target_sr\n",
    "\n",
    "print(\"‚úì Helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from Azure blob\n",
    "audio_id = example_file['audio_id']\n",
    "\n",
    "# Try audio.mp3 first, then video.mp4\n",
    "try:\n",
    "    blob_path = f\"loc_vhp/{audio_id}/audio.mp3\"\n",
    "    audio_bytes_raw = download_blob_to_memory(blob_path)\n",
    "    print(f\"‚úì Downloaded: {blob_path}\")\n",
    "except:\n",
    "    blob_path = f\"loc_vhp/{audio_id}/video.mp4\"\n",
    "    audio_bytes_raw = download_blob_to_memory(blob_path)\n",
    "    print(f\"‚úì Downloaded: {blob_path}\")\n",
    "\n",
    "print(f\"  Size: {len(audio_bytes_raw)/1024/1024:.1f} MB\")\n",
    "\n",
    "# Load to waveform for analysis\n",
    "waveform_raw, sr = load_audio_bytes(audio_bytes_raw, target_sr=16000)\n",
    "print(f\"  Duration: {len(waveform_raw)/sr:.1f} seconds\")\n",
    "print(f\"  Sample rate: {sr} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Step 3: Apply Preprocessing Functions One-by-One\n",
    "\n",
    "Test each preprocessing function independently to understand what it does.\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** Based on research, Whisper and Parakeet already include optimized preprocessing. These examples are educational. For production inference with those models, use raw audio.\n",
    "\n",
    "**When to use these:** AWS Transcribe (test with validation), or understanding preprocessing concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a. Loudness Normalization\n",
    "print(\"=\" * 70)\n",
    "print(\"LOUDNESS NORMALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Core logic (2 lines):\n",
    "meter = pyln.Meter(sr)\n",
    "current_lufs = meter.integrated_loudness(waveform_raw)\n",
    "waveform_normalized = pyln.normalize.loudness(waveform_raw.copy(), current_lufs, -16.0)  # positional args\n",
    "waveform_normalized = np.clip(waveform_normalized, -1.0, 1.0)  # Prevent clipping\n",
    "\n",
    "# Measure after\n",
    "after_lufs = meter.integrated_loudness(waveform_normalized)\n",
    "\n",
    "print(f\"Before: {current_lufs:.1f} LUFS\")\n",
    "print(f\"After:  {after_lufs:.1f} LUFS\")\n",
    "print(f\"Change: {after_lufs - current_lufs:+.1f} LUFS\")\n",
    "print(f\"\\n‚úì Normalized to target -16.0 LUFS\")\n",
    "\n",
    "print(\"\\nüìù RESEARCH NOTE:\")\n",
    "print(\"   - Whisper: Already does amplitude normalization to [-1, 1]\")\n",
    "print(\"   - Parakeet: Already includes normalization (per-feature or all-features)\")\n",
    "print(\"   - Literature: Conservative normalization helps, aggressive can hurt WER\")\n",
    "print(\"   - For Whisper/Parakeet: This step is REDUNDANT (educational only)\")\n",
    "print(\"   - For AWS Transcribe: TEST with validation before using\")\n",
    "\n",
    "# Optional: Play audio (uncomment if running locally)\n",
    "# import IPython.display as ipd\n",
    "# print(\"\\nPlay BEFORE:\")\n",
    "# ipd.display(ipd.Audio(waveform_raw, rate=sr))\n",
    "# print(\"\\nPlay AFTER:\")\n",
    "# ipd.display(ipd.Audio(waveform_normalized, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b. High-Pass Filter (remove low-frequency rumble)\n",
    "print(\"=\" * 70)\n",
    "print(\"HIGH-PASS FILTER (80 Hz cutoff)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Core logic (3 lines):\n",
    "nyquist = sr / 2.0\n",
    "b, a = signal.butter(5, 80 / nyquist, btype='high', analog=False)\n",
    "waveform_highpassed = signal.filtfilt(b, a, waveform_raw.copy())\n",
    "\n",
    "# Calculate low-frequency energy before/after\n",
    "freqs = np.fft.rfftfreq(len(waveform_raw), 1/sr)\n",
    "fft_before = np.abs(np.fft.rfft(waveform_raw))\n",
    "fft_after = np.abs(np.fft.rfft(waveform_highpassed))\n",
    "\n",
    "low_freq_mask = freqs < 80\n",
    "energy_before = np.sum(fft_before[low_freq_mask])\n",
    "energy_after = np.sum(fft_after[low_freq_mask])\n",
    "\n",
    "print(f\"Low-freq energy (<80 Hz) before: {energy_before:.2e}\")\n",
    "print(f\"Low-freq energy (<80 Hz) after:  {energy_after:.2e}\")\n",
    "print(f\"Reduction: {(1 - energy_after/energy_before)*100:.1f}%\")\n",
    "print(f\"\\n‚úì Removed low-frequency rumble\")\n",
    "\n",
    "print(\"\\nüìù RESEARCH NOTE:\")\n",
    "print(\"   - Literature: High-pass filtering shows CONSISTENT improvements\")\n",
    "print(\"   - Removes low-frequency ambient noise (AC hum, tape rumble)\")\n",
    "print(\"   - SAFE: Does not harm speech content (human voice >100 Hz)\")\n",
    "print(\"   - Chu et al.: 'Significant improvements' for low-freq noise\")\n",
    "print(\"   - Recommended for: AWS Transcribe, or any model with archival audio\")\n",
    "\n",
    "# Optional: Play audio\n",
    "# import IPython.display as ipd\n",
    "# print(\"\\nPlay BEFORE:\")\n",
    "# ipd.display(ipd.Audio(waveform_raw, rate=sr))\n",
    "# print(\"\\nPlay AFTER (rumble removed):\")\n",
    "# ipd.display(ipd.Audio(waveform_highpassed, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3c. Noise Reduction\n",
    "print(\"=\" * 70)\n",
    "print(\"NOISE REDUCTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Core logic (1 line - using noisereduce library):\n",
    "import noisereduce as nr\n",
    "waveform_denoised = nr.reduce_noise(y=waveform_raw.copy(), sr=sr, stationary=True, prop_decrease=1.0)\n",
    "\n",
    "# Calculate SNR before/after (simple RMS-based estimate)\n",
    "def simple_snr(waveform, sr):\n",
    "    # Assume first 0.5s is noise\n",
    "    noise_samples = int(0.5 * sr)\n",
    "    noise_rms = np.sqrt(np.mean(waveform[:noise_samples]**2))\n",
    "    signal_rms = np.sqrt(np.mean(waveform**2))\n",
    "    return 20 * np.log10(signal_rms / noise_rms) if noise_rms > 0 else 0\n",
    "\n",
    "snr_before = simple_snr(waveform_raw, sr)\n",
    "snr_after = simple_snr(waveform_denoised, sr)\n",
    "\n",
    "print(f\"SNR before: {snr_before:.1f} dB\")\n",
    "print(f\"SNR after:  {snr_after:.1f} dB\")\n",
    "print(f\"Improvement: {snr_after - snr_before:+.1f} dB\")\n",
    "print(f\"\\n‚úì Noise reduced\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è RESEARCH WARNING:\")\n",
    "print(\"   - Models: Whisper/Parakeet rely on robustness, NO explicit denoising\")\n",
    "print(\"   - Literature: RNNoise and ASTEROID shown to HARM ASR\")\n",
    "print(\"   - Problem: Aggressive noise reduction removes speech info\")\n",
    "print(\"   - This library (noisereduce): NOT tested in research papers\")\n",
    "print(\"   - Current setting (prop_decrease=1.0): May be too aggressive\")\n",
    "print(\"   - Recommendation:\")\n",
    "print(\"     * For Whisper/Parakeet: SKIP this step (use raw audio)\")\n",
    "print(\"     * For AWS Transcribe: TEST with conservative settings (prop_decrease=0.5)\")\n",
    "print(\"     * ALWAYS validate with WER before using in production\")\n",
    "\n",
    "# Optional: Play audio\n",
    "# import IPython.display as ipd\n",
    "# print(\"\\nPlay BEFORE:\")\n",
    "# ipd.display(ipd.Audio(waveform_raw, rate=sr))\n",
    "# print(\"\\nPlay AFTER (denoised):\")\n",
    "# ipd.display(ipd.Audio(waveform_denoised, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "<cell_type>markdown</cell_type>---\n",
    "\n",
    "### Using the Preprocessing Functions in Production\n",
    "\n",
    "**You've now seen the core logic!** These functions are available in `scripts/preprocess_audio.py` for production use.\n",
    "\n",
    "---\n",
    "\n",
    "#### üéØ Decision Matrix: Which Preprocessing for Which Model?\n",
    "\n",
    "Based on research findings (see `learnings/preprocessing_research_findings.md`):\n",
    "\n",
    "| Model | Loudness Norm | High-Pass Filter | Noise Reduction | Recommendation |\n",
    "|-------|---------------|------------------|-----------------|----------------|\n",
    "| **Whisper (pretrained/fine-tuned)** | ‚ùå Redundant | ‚ö†Ô∏è Test cautiously | ‚ùå Skip | **Use raw audio** |\n",
    "| **Parakeet-TDT** | ‚ùå Redundant | ‚ö†Ô∏è Test cautiously | ‚ùå Skip | **Use raw audio** |\n",
    "| **Google Chirp 2/3** | ‚ùå Skip | ‚ùå Skip | ‚úÖ Use built-in | **Use API params** (`denoise_audio=true`) |\n",
    "| **AWS Transcribe** | ‚ö†Ô∏è Test | ‚úÖ Safe | ‚ö†Ô∏è Test conservatively | **Test with validation** |\n",
    "\n",
    "**Key insights:**\n",
    "- ‚úÖ **High-pass filter (80 Hz)**: Safe for all models, removes rumble without harming speech\n",
    "- ‚ö†Ô∏è **Loudness normalization**: Whisper/Parakeet already do this (redundant)\n",
    "- ‚ùå **Noise reduction**: Literature shows RNNoise/ASTEROID harm ASR; this library untested\n",
    "- üéØ **Google Chirp**: Use built-in preprocessing (`denoise_audio=true`, `snr_threshold=150`)\n",
    "\n",
    "**For this project:**\n",
    "- Primary models (Whisper/Parakeet): Use **raw audio** only\n",
    "- AWS Transcribe experiments: Test high-pass filter with WER validation\n",
    "- Educational purpose: These examples help understand preprocessing concepts\n",
    "\n",
    "---\n",
    "\n",
    "#### Option 1: Import and use in notebooks\n",
    "```python\n",
    "from scripts.preprocess_audio import (\n",
    "    load_audio_bytes,\n",
    "    save_audio_bytes,\n",
    "    loudness_normalize,\n",
    "    highpass_filter,\n",
    "    noise_reduce,\n",
    "    preprocess_audio\n",
    ")\n",
    "\n",
    "# Apply full pipeline\n",
    "audio_bytes_processed = preprocess_audio(\n",
    "    audio_bytes_raw,\n",
    "    methods=['highpass_filter', 'noise_reduction', 'loudness_normalization']\n",
    ")\n",
    "```\n",
    "\n",
    "#### Option 2: Batch process via CLI (for AWS Transcribe experiments)\n",
    "\n",
    "Run this on a cloud GPU instance (RunPod A6000, Azure VM, etc.) for fast batch processing:\n",
    "\n",
    "```bash\n",
    "# Example: Process entire test set with full pipeline\n",
    "python scripts/preprocess_audio.py \\\n",
    "    --parquet data/raw/loc/veterans_history_project_resources_pre2010_test.parquet \\\n",
    "    --variant full_pipeline \\\n",
    "    --output_prefix loc_vhp_preprocessed/full_pipeline \\\n",
    "    --batch_size 100\n",
    "\n",
    "# Available variants:\n",
    "# - raw: No preprocessing (baseline - use this for Whisper/Parakeet)\n",
    "# - normalized: Loudness normalization only\n",
    "# - normalized_eq: Loudness + EQ boost (TODO: implement EQ)\n",
    "# - normalized_denoised: Loudness + noise reduction\n",
    "# - full_pipeline: Highpass + denoise + EQ + loudness\n",
    "```\n",
    "\n",
    "#### Option 3: Google Chirp API (recommended approach)\n",
    "\n",
    "```python\n",
    "# For Chirp, use built-in preprocessing parameters (NO upstream preprocessing needed)\n",
    "from google.cloud import speech_v2\n",
    "\n",
    "config = speech_v2.RecognitionConfig(\n",
    "    auto_decoding_config=speech_v2.AutoDetectDecodingConfig(),\n",
    "    features=speech_v2.RecognitionFeatures(\n",
    "        enable_automatic_punctuation=True,\n",
    "        denoise_audio=True,  # Use built-in denoising\n",
    "        snr_threshold=150     # SNR threshold for noise reduction\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "**You can test the functions here, or skip to Section 3 for inference playground!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Test the imported functions (you can skip this)\n",
    "# Uncomment to run:\n",
    "\n",
    "# from scripts.preprocess_audio import preprocess_audio, save_audio_bytes\n",
    "\n",
    "# # Apply full pipeline using the imported function\n",
    "# audio_bytes_preprocessed = preprocess_audio(\n",
    "#     audio_bytes_raw,\n",
    "#     methods=['highpass_filter', 'noise_reduction', 'loudness_normalization'],\n",
    "#     audio_id=audio_id,\n",
    "#     metrics=example_file.to_dict()\n",
    "# )\n",
    "\n",
    "# # Load it back to waveform for comparison\n",
    "# from scripts.preprocess_audio import load_audio_bytes\n",
    "# waveform_full_pipeline, _ = load_audio_bytes(audio_bytes_preprocessed)\n",
    "\n",
    "# print(f\"‚úì Preprocessed audio ready ({len(audio_bytes_preprocessed)/1024/1024:.1f} MB WAV)\")\n",
    "\n",
    "# # Play it\n",
    "# import IPython.display as ipd\n",
    "# ipd.display(ipd.Audio(waveform_full_pipeline, rate=sr))\n",
    "\n",
    "print(\"‚úì Skipped (optional cell)\")\n",
    "print(\"  Uncomment the code above to test the imported preprocessing function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Step 4: Visualize Before/After (Optional)\n",
    "\n",
    "Compare spectrograms to see the effect of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full pipeline by chaining the preprocessing steps\n",
    "waveform_full_pipeline = waveform_highpassed.copy()  # Start with highpassed\n",
    "waveform_full_pipeline = nr.reduce_noise(y=waveform_full_pipeline, sr=sr, stationary=True)  # Then denoise\n",
    "# Finally normalize (use positional args)\n",
    "current_lufs = pyln.Meter(sr).integrated_loudness(waveform_full_pipeline)\n",
    "waveform_full_pipeline = pyln.normalize.loudness(waveform_full_pipeline, current_lufs, -16.0)\n",
    "\n",
    "# Plot spectrograms: Raw vs. Full Pipeline\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Raw\n",
    "D_raw = librosa.stft(waveform_raw)\n",
    "S_db_raw = librosa.amplitude_to_db(np.abs(D_raw), ref=np.max)\n",
    "img1 = librosa.display.specshow(S_db_raw, sr=sr, x_axis='time', y_axis='hz', ax=axes[0], cmap='viridis')\n",
    "axes[0].set_ylim([0, 4000])\n",
    "axes[0].set_title('RAW AUDIO (No Preprocessing)', fontsize=14, fontweight='bold')\n",
    "fig.colorbar(img1, ax=axes[0], format='%+2.0f dB')\n",
    "\n",
    "# Preprocessed\n",
    "D_processed = librosa.stft(waveform_full_pipeline)\n",
    "S_db_processed = librosa.amplitude_to_db(np.abs(D_processed), ref=np.max)\n",
    "img2 = librosa.display.specshow(S_db_processed, sr=sr, x_axis='time', y_axis='hz', ax=axes[1], cmap='viridis')\n",
    "axes[1].set_ylim([0, 4000])\n",
    "axes[1].set_title('PREPROCESSED (Highpass ‚Üí Denoise ‚Üí Normalize)', fontsize=14, fontweight='bold')\n",
    "fig.colorbar(img2, ax=axes[1], format='%+2.0f dB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Spectrogram comparison complete\")\n",
    "\n",
    "# Optional: Play both versions\n",
    "# import IPython.display as ipd\n",
    "# print(\"\\nPlay RAW:\")\n",
    "# ipd.display(ipd.Audio(waveform_raw, rate=sr))\n",
    "# print(\"\\nPlay PREPROCESSED (full pipeline):\")\n",
    "# ipd.display(ipd.Audio(waveform_full_pipeline, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Step 5: Intelligibility Check with STOI (Optional)\n",
    "\n",
    "**STOI** (Short-Time Objective Intelligibility) predicts whether preprocessing helps or hurts ASR.\n",
    "\n",
    "**From research:** Pearson r=0.79 correlation between STOI and ASR performance.\n",
    "\n",
    "**How it works:** Compares preprocessed audio to a reference. Higher score (0-1) means better intelligibility.\n",
    "\n",
    "**Limitation for archival audio:** STOI expects a \"clean reference\" signal. For VHP degraded audio, we use raw as reference (pragmatic workaround).\n",
    "\n",
    "**Best validation:** Test WER directly in Section 3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOI - Intelligibility Check\n",
    "# Predicts if preprocessing maintains speech intelligibility (r=0.79 with ASR)\n",
    "\n",
    "try:\n",
    "    from pystoi import stoi\n",
    "    \n",
    "    # For degraded archival audio, we use raw as \"reference\" (not ideal, but pragmatic)\n",
    "    # STOI score > 0.7: preprocessing maintains intelligibility\n",
    "    # STOI score 0.5-0.7: moderate intelligibility (validate with WER)\n",
    "    # STOI score < 0.5: preprocessing likely degrades quality\n",
    "    \n",
    "    stoi_score = stoi(waveform_raw, waveform_full_pipeline, sr, extended=False)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"STOI INTELLIGIBILITY CHECK\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"STOI Score: {stoi_score:.3f} (range: 0-1, higher = better)\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    \n",
    "    if stoi_score > 0.7:\n",
    "        print(\"  ‚úì High intelligibility maintained\")\n",
    "        print(\"  ‚Üí Preprocessing likely safe for ASR\")\n",
    "    elif stoi_score > 0.5:\n",
    "        print(\"  ‚ö† Moderate intelligibility\")\n",
    "        print(\"  ‚Üí Validate with WER testing in Section 3\")\n",
    "    else:\n",
    "        print(\"  ‚úó Low intelligibility\")\n",
    "        print(\"  ‚Üí Preprocessing may hurt ASR performance\")\n",
    "    \n",
    "    print(f\"\\nüìù Note: For archival audio, STOI has limitations.\")\n",
    "    print(f\"   Best validation: Compare WER on sample set (Section 3)\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STOI NOT AVAILABLE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Install with: pip install pystoi\")\n",
    "    print(\"\\nSTOI (Short-Time Objective Intelligibility) helps predict\")\n",
    "    print(\"if preprocessing will help or hurt ASR (r=0.79 correlation)\")\n",
    "    print(\"\\nFor VHP project: WER testing in Section 3 is the gold standard!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Inference Playground - Test Preprocessing Impact\n",
    "\n",
    "**Goal:** Run STT inference on the same audio with different preprocessing to see transcription differences.\n",
    "\n",
    "**Pattern:** Raw audio ‚Üí transcribe ‚Üí Preprocessed audio ‚Üí transcribe ‚Üí Compare outputs\n",
    "\n",
    "**Note:** This is a \"smoke test\" playground. For large-scale inference, use the decoupled `infer_*.ipynb` notebooks and `scripts/run_inference.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Choose a Model for Testing\n",
    "\n",
    "Pick one of the available STT models. You can test different models to see which benefits most from preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model to test\n",
    "# Options: 'whisper-large-v3', 'whisper-large-v3-lora', 'parakeet', 'chirp', 'aws'\n",
    "\n",
    "MODEL_CHOICE = 'whisper-large-v3'  # Change this to test different models\n",
    "\n",
    "print(f\"Selected model: {MODEL_CHOICE}\")\n",
    "print(f\"\\nAvailable models:\")\n",
    "print(\"  - whisper-large-v3: Pretrained Whisper (baseline)\")\n",
    "print(\"  - whisper-large-v3-lora: Fine-tuned on VHP data\")\n",
    "print(\"  - parakeet: NVIDIA Parakeet-TDT (optimized for noisy audio)\")\n",
    "print(\"  - chirp: Google Chirp 3 (commercial API)\")\n",
    "print(\"  - aws: AWS Transcribe (commercial API)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Transcribe Raw Audio (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe raw audio\n",
    "# TODO: Implement model inference logic based on MODEL_CHOICE\n",
    "# This is a placeholder - replace with actual inference code\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"TRANSCRIBING RAW AUDIO with {MODEL_CHOICE}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Placeholder - replace with actual inference\n",
    "if MODEL_CHOICE == 'whisper-large-v3':\n",
    "    # from faster_whisper import WhisperModel\n",
    "    # model = WhisperModel(\"models/faster-whisper/whisper-large-v3\")\n",
    "    # segments, info = model.transcribe(audio_bytes_raw)\n",
    "    # transcript_raw = \" \".join([s.text for s in segments])\n",
    "    transcript_raw = \"[TODO] Implement Whisper inference\"\n",
    "    \n",
    "elif MODEL_CHOICE == 'whisper-large-v3-lora':\n",
    "    # Load fine-tuned model\n",
    "    transcript_raw = \"[TODO] Implement fine-tuned Whisper inference\"\n",
    "    \n",
    "elif MODEL_CHOICE == 'parakeet':\n",
    "    # Load Parakeet model\n",
    "    transcript_raw = \"[TODO] Implement Parakeet inference\"\n",
    "    \n",
    "else:\n",
    "    transcript_raw = \"[TODO] Implement API-based inference\"\n",
    "\n",
    "print(f\"Raw transcript: {transcript_raw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Transcribe Preprocessed Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have waveform_full_pipeline from Section 2!\n",
    "# Let's convert it to bytes for inference (if needed by some models)\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Convert waveform to bytes (WAV format)\n",
    "waveform_int16 = (waveform_full_pipeline * 32767).astype(np.int16)\n",
    "audio_segment = AudioSegment(\n",
    "    waveform_int16.tobytes(),\n",
    "    frame_rate=sr,\n",
    "    sample_width=2,\n",
    "    channels=1\n",
    ")\n",
    "buffer = io.BytesIO()\n",
    "audio_segment.export(buffer, format='wav')\n",
    "audio_bytes_preprocessed = buffer.getvalue()\n",
    "\n",
    "print(\"‚úì Using preprocessed audio from Section 2\")\n",
    "print(f\"  Methods applied: Highpass ‚Üí Denoise ‚Üí Normalize\")\n",
    "print(f\"  Size: {len(audio_bytes_preprocessed)/1024/1024:.1f} MB (WAV format)\")\n",
    "print(f\"\\nReady for inference comparison!\")\n",
    "\n",
    "# Transcribe preprocessed audio\n",
    "print(\"=\" * 70)\n",
    "print(f\"TRANSCRIBING PREPROCESSED AUDIO with {MODEL_CHOICE}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TODO: Same inference logic as above\n",
    "transcript_preprocessed = \"[TODO] Implement inference on preprocessed audio\"\n",
    "\n",
    "print(f\"Preprocessed transcript: {transcript_preprocessed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Compare Transcriptions\n",
    "\n",
    "Compare the raw vs. preprocessed transcriptions to see if preprocessing improved accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare transcriptions\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_CHOICE}\")\n",
    "print(f\"Audio: {audio_id}\")\n",
    "print(f\"\\nRaw transcript:\")\n",
    "print(f\"  {transcript_raw}\")\n",
    "print(f\"\\nPreprocessed transcript:\")\n",
    "print(f\"  {transcript_preprocessed}\")\n",
    "\n",
    "# TODO: If ground truth is available, calculate WER for both\n",
    "# from scripts.eval.evaluate import calculate_wer\n",
    "# ground_truth = example_file['transcript']  # Get from parquet\n",
    "# wer_raw = calculate_wer(ground_truth, transcript_raw)\n",
    "# wer_preprocessed = calculate_wer(ground_truth, transcript_preprocessed)\n",
    "# print(f\"\\nWER (raw): {wer_raw:.1%}\")\n",
    "# print(f\"WER (preprocessed): {wer_preprocessed:.1%}\")\n",
    "# print(f\"Improvement: {(wer_raw - wer_preprocessed)*100:+.1f} percentage points\")\n",
    "\n",
    "print(\"\\n[TODO] Add WER calculation if ground truth is available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**You've completed the interactive walkthrough!** Here's what to do next:\n",
    "\n",
    "### 1. Complete the TODOs in this notebook:\n",
    "- Implement EQ high-frequency boost in `scripts/preprocess_audio.py`\n",
    "- Add inference logic for your chosen models\n",
    "- Add WER calculation for comparing raw vs. preprocessed\n",
    "\n",
    "### 2. For production batch processing:\n",
    "Use the decoupled script with your preferred preprocessing variant:\n",
    "```bash\n",
    "# Example: Process test set with full pipeline\n",
    "python scripts/preprocess_audio.py \\\n",
    "    --parquet data/raw/loc/veterans_history_project_resources_pre2010_test.parquet \\\n",
    "    --variant full_pipeline \\\n",
    "    --output_prefix loc_vhp_preprocessed/full_pipeline\n",
    "```\n",
    "\n",
    "### 3. For STT inference on preprocessed audio:\n",
    "Use existing `infer_*.ipynb` notebooks - they support custom blob prefixes:\n",
    "```python\n",
    "# In infer_whisper.ipynb or similar:\n",
    "CONFIG = {\n",
    "    \"blob_prefix\": \"loc_vhp_preprocessed/full_pipeline\",  # Point to preprocessed audio\n",
    "    # ... other config\n",
    "}\n",
    "```\n",
    "\n",
    "### 4. Generate WER Matrix:\n",
    "Run inference for each model √ó preprocessing variant combination to build your evaluation matrix:\n",
    "\n",
    "| Model | Raw | Normalized | Normalized+EQ | Normalized+Denoised | Full Pipeline |\n",
    "|-------|-----|------------|---------------|---------------------|---------------|\n",
    "| Whisper v3 | ? | ? | ? | ? | ? |\n",
    "| Whisper v3 LoRA | ? | ? | ? | ? | ? |\n",
    "| Parakeet | ? | ? | ? | ? | ? |\n",
    "| Chirp 3 | ? | ? | ? | ? | ? |\n",
    "| AWS Transcribe | ? | ? | ? | ? | ? |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
