{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Whisper Inference Pipeline\n",
    "\n",
    "This notebook demonstrates how to run the Whisper inference pipeline on a single sample file.\n",
    "\n",
    "**What this notebook does:**\n",
    "- Uses production orchestration (Azure blob storage)\n",
    "- Supports both faster-whisper (pretrained) and HuggingFace transformers (fine-tuned)\n",
    "- Processes just 1 sample file\n",
    "- Shows outputs (inference results, hypothesis text)\n",
    "\n",
    "**Models supported:**\n",
    "- Standard Whisper models (tiny, base, small, medium, large-v3) via faster-whisper\n",
    "- Fine-tuned VHP model (large-v3-vhp-lora) via HuggingFace transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Import and Configure Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Load Azure credentials from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../credentials/creds.env')\n",
    "\n",
    "# Disable wandb for notebook runs (tutorials don't need tracking)\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "# Add scripts directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"scripts\"))\n",
    "\n",
    "# Import both inference modules - we'll choose which one to use based on model\n",
    "from infer_whisper import run as run_faster_whisper\n",
    "from infer_whisper_hf import run as run_hf_whisper\n",
    "\n",
    "print(\"ðŸ’¡ Note: Wandb logging is disabled for tutorial runs\")\n",
    "print(\"ðŸ’¡ Note: Both faster-whisper and HuggingFace inference available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select Model\n",
    "\n",
    "Choose a Whisper model to use (including the fine-tuned model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available Whisper models\n",
    "MODEL_OPTIONS = {\n",
    "    \"tiny\": (\"../models/faster-whisper/models--Systran--faster-whisper-tiny\", \"faster-whisper\"),\n",
    "    \"base\": (\"../models/faster-whisper/models--Systran--faster-whisper-base\", \"faster-whisper\"),\n",
    "    \"small\": (\"../models/faster-whisper/models--Systran--faster-whisper-small\", \"faster-whisper\"),\n",
    "    \"medium\": (\"../models/faster-whisper/models--Systran--faster-whisper-medium\", \"faster-whisper\"),\n",
    "    \"large-v3\": (\"../models/faster-whisper/models--Systran--faster-whisper-large-v3\", \"faster-whisper\"),\n",
    "    \"large-v3-vhp-lora\": (\"../models/hf-whisper/whisper-large-v3-vhp-lora\", \"hf\")  # Fine-tuned (HF format)\n",
    "}\n",
    "\n",
    "# SELECT YOUR MODEL HERE\n",
    "SELECTED_MODEL = \"large-v3-vhp-lora\"  # Change to \"large-v3-vhp-lora\" to test fine-tuned model\n",
    "\n",
    "model_path, inference_type = MODEL_OPTIONS[SELECTED_MODEL]\n",
    "print(f\"Selected model: {SELECTED_MODEL}\")\n",
    "print(f\"Model path: {model_path}\")\n",
    "print(f\"Inference type: {inference_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Inference Parameters\n",
    "\n",
    "Set parameters directly in Python (no yaml file needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build configuration dictionary\n",
    "# Choose model name based on inference type\n",
    "if inference_type == \"hf\":\n",
    "    model_name = \"whisper-hf\"  # Use HuggingFace inference\n",
    "    compute_type_default = \"float32\"  # HF uses float32 on CPU\n",
    "else:\n",
    "    model_name = \"whisper\"  # Use faster-whisper\n",
    "    compute_type_default = \"int8\"  # faster-whisper uses int8 on CPU\n",
    "\n",
    "cfg = {\n",
    "    \"experiment_id\": \"tutorial-whisper-sample1\",\n",
    "    \n",
    "    # Model settings\n",
    "    \"model\": {\n",
    "        \"name\": model_name,\n",
    "        \"dir\": model_path,  # Use selected model path\n",
    "        \"device\": \"cpu\",  # Use \"cuda\" if GPU available\n",
    "        \"compute_type\": compute_type_default,  # Optimized for inference type\n",
    "        \"batch_size\": 16 if inference_type == \"faster-whisper\" else 1,  # HF doesn't batch\n",
    "        \n",
    "        # Transcription parameters (explicit for reproducibility)\n",
    "        \"beam_size\": 5,  # Beam search width (1=greedy, 5=default balance)\n",
    "        \"temperature\": 0.0,  # Sampling temperature (0.0=deterministic)\n",
    "        \"vad_filter\": True,  # Voice activity detection to skip silence (faster-whisper only)\n",
    "        \"no_speech_threshold\": 0.6,  # Threshold for detecting no speech\n",
    "        \"word_timestamps\": False,  # Don't generate word-level timestamps (faster)\n",
    "        \"initial_prompt\": None,  # No initial prompt\n",
    "        \"suppress_tokens\": [-1],  # Default: suppress only special tokens\n",
    "        \"condition_on_previous_text\": True,  # Use context from previous segments (important for long-form!)\n",
    "    },\n",
    "    \n",
    "    # Input settings\n",
    "    \"input\": {\n",
    "        \"source\": \"azure_blob\",\n",
    "        \"parquet_path\": \"../data/raw/loc/veterans_history_project_resources_pre2010_test.parquet\",  # Test set only\n",
    "        \"blob_prefix\": \"loc_vhp\",\n",
    "        \"sample_size\": 1,  # Just 1 file for tutorial\n",
    "        \"duration_sec\": 30, \n",
    "        \"sample_rate\": 16000\n",
    "    },\n",
    "    \n",
    "    # Output settings\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/tutorial-whisper-sample1\",  # Relative to notebook dir\n",
    "        \"save_per_file\": True  # Save individual hypothesis files\n",
    "    },\n",
    "    \n",
    "    # Evaluation (optional)\n",
    "    \"evaluation\": {\n",
    "        \"use_whisper_normalizer\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {SELECTED_MODEL}\")\n",
    "print(f\"  Inference type: {inference_type}\")\n",
    "print(f\"  Model path: {cfg['model']['dir']}\")\n",
    "print(f\"  Device: {cfg['model']['device']}\")\n",
    "print(f\"  Compute type: {cfg['model']['compute_type']}\")\n",
    "print(f\"  Beam size: {cfg['model']['beam_size']}\")\n",
    "print(f\"  Sample size: {cfg['input']['sample_size']} file(s)\")\n",
    "print(f\"  Duration: {cfg['input']['duration_sec']}s\")\n",
    "print(f\"  Output: {cfg['output']['dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference\n",
    "\n",
    "This will:\n",
    "1. Load 1 sample from the parquet file (test set)\n",
    "2. Download audio from Azure blob storage\n",
    "3. Preprocess audio (trim to 300s, convert to 16kHz mono WAV)\n",
    "4. Load Whisper model (using faster-whisper for efficiency)\n",
    "5. Run transcription with the configured parameters\n",
    "6. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the appropriate inference pipeline based on model type\n",
    "if inference_type == \"hf\":\n",
    "    print(\"Using HuggingFace transformers inference (for fine-tuned model)...\")\n",
    "    run_hf_whisper(cfg)\n",
    "else:\n",
    "    print(\"Using faster-whisper inference (optimized for pretrained models)...\")\n",
    "    run_faster_whisper(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results parquet\n",
    "results_path = Path(cfg['output']['dir']) / \"inference_results.parquet\"\n",
    "df_results = pd.read_parquet(results_path)\n",
    "\n",
    "print(f\"Results shape: {df_results.shape}\")\n",
    "print(f\"\\nColumns: {list(df_results.columns)}\")\n",
    "print(f\"\\nFirst row:\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View Results\n",
    "\n",
    "### 5.1 Inference Results (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the transcription\n",
    "row = df_results.iloc[0]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"File ID: {row['file_id']}\")\n",
    "print(f\"Collection: {row['collection_number']}\")\n",
    "print(f\"Duration: {row['duration_sec']:.1f}s\")\n",
    "print(f\"Processing time: {row['processing_time_sec']:.1f}s\")\n",
    "print(f\"Status: {row['status']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTranscription (hypothesis):\\n\")\n",
    "print(row['hypothesis'][:500] + \"...\" if len(row['hypothesis']) > 500 else row['hypothesis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Hypothesis Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check individual hypothesis file\n",
    "hyp_file = Path(cfg['output']['dir']) / f\"hyp_{row['file_id']}.txt\"\n",
    "\n",
    "if hyp_file.exists():\n",
    "    print(f\"Hypothesis file: {hyp_file}\")\n",
    "    print(f\"\\nContent:\\n\")\n",
    "    print(hyp_file.read_text()[:500] + \"...\" if len(hyp_file.read_text()) > 500 else hyp_file.read_text())\n",
    "else:\n",
    "    print(f\"Hypothesis file not found: {hyp_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Individual Hypothesis File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding the Output\n",
    "\n",
    "**Key output files:**\n",
    "- `inference_results.parquet`: All results in structured format (hypothesis, duration, status, etc.)\n",
    "- `hyp_{file_id}.txt`: Individual hypothesis text files (one per audio file)\n",
    "- `hyp_whisper-{size}.txt`: Combined hypothesis text (all transcriptions concatenated)\n",
    "- `whisper_log_*.txt`: Detailed inference log\n",
    "\n",
    "**Key result columns:**\n",
    "- `file_id`: Unique identifier\n",
    "- `collection_number`: VHP collection number\n",
    "- `hypothesis`: Model transcription output\n",
    "- `ground_truth`: Reference transcript (for evaluation)\n",
    "- `duration_sec`: Audio duration processed\n",
    "- `processing_time_sec`: Time taken for inference\n",
    "- `status`: success/error\n",
    "- `model_name`: Model identifier (e.g., whisper-base, whisper-large-v3-vhp-lora)\n",
    "\n",
    "**Whisper model sizes:**\n",
    "- `tiny`: ~39MB, fastest, lowest accuracy\n",
    "- `base`: ~74MB\n",
    "- `small`: ~244MB\n",
    "- `medium`: ~769MB\n",
    "- `large-v3`: ~1550MB, best accuracy, slowest\n",
    "- `large-v3-vhp-lora`: ~1550MB, fine-tuned on VHP archival audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "To run on more files:\n",
    "1. Increase `sample_size` (e.g., 10, 50, 100)\n",
    "2. Set `duration_sec: null` for full audio\n",
    "3. Use `device: \"cuda\"` if you have a GPU (and `compute_type: \"float16\"`)\n",
    "4. Try the fine-tuned model: Set `SELECTED_MODEL = \"large-v3-vhp-lora\"`\n",
    "5. Compare results between pretrained `\"large-v3\"` and fine-tuned `\"large-v3-vhp-lora\"`\n",
    "\n",
    "**Note on fine-tuned model:**\n",
    "- The fine-tuned model uses HuggingFace transformers (not faster-whisper)\n",
    "- It has 128 mel filterbanks instead of standard 80\n",
    "- Inference is slower but works with the LoRA-merged weights\n",
    "- The notebook automatically selects the correct inference engine\n",
    "\n",
    "To evaluate results:\n",
    "```python\n",
    "from scripts.eval.evaluate import evaluate_results\n",
    "metrics = evaluate_results(df_results, use_whisper_normalizer=True)\n",
    "print(f\"WER: {metrics['wer']:.2%}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
