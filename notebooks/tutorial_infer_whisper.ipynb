{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Whisper Inference Pipeline\n",
    "\n",
    "This notebook demonstrates how to run the Whisper inference pipeline on a single sample file.\n",
    "\n",
    "**What this notebook does:**\n",
    "- Uses production orchestration (Azure blob storage)\n",
    "- Runs the smallest model (`openai/whisper-tiny`)\n",
    "- Processes just 1 sample file\n",
    "- Shows outputs (inference results, hypothesis text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Import and Configure Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Load Azure credentials from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../credentials/creds.env')\n",
    "\n",
    "# Disable wandb for notebook runs (tutorials don't need tracking)\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "# Add scripts directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"scripts\"))\n",
    "\n",
    "from infer_whisper import run\n",
    "\n",
    "print(\"ðŸ’¡ Note: Wandb logging is disabled for tutorial runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Inference Parameters\n",
    "\n",
    "Set parameters directly in Python (no yaml file needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build configuration dictionary\n",
    "cfg = {\n",
    "    \"experiment_id\": \"tutorial-whisper-sample1\",\n",
    "    \n",
    "    # Model settings\n",
    "    \"model\": {\n",
    "        \"name\": \"whisper\",\n",
    "        \"dir\": \"../models/faster-whisper/models--Systran--faster-whisper-base\",  # Local model directory\n",
    "        \"device\": \"cpu\",  # Use \"cuda\" if GPU available\n",
    "        \"compute_type\": \"int8\",  # Options: int8, float16, float32\n",
    "        \"batch_size\": 16\n",
    "    },\n",
    "    \n",
    "    # Input settings\n",
    "    \"input\": {\n",
    "        \"source\": \"azure_blob\",\n",
    "        \"parquet_path\": \"../data/raw/loc/veterans_history_project_resources_pre2010.parquet\",  # Relative to notebook dir\n",
    "        \"blob_prefix\": \"loc_vhp\",\n",
    "        \"sample_size\": 1,  # Just 1 file for tutorial\n",
    "        \"duration_sec\": 300,  # First 5 minutes only (faster)\n",
    "        \"sample_rate\": 16000\n",
    "    },\n",
    "    \n",
    "    # Output settings\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/tutorial-whisper-sample1\",  # Relative to notebook dir\n",
    "        \"save_per_file\": True  # Save individual hypothesis files\n",
    "    },\n",
    "    \n",
    "    # Evaluation (optional)\n",
    "    \"evaluation\": {\n",
    "        \"use_whisper_normalizer\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {cfg['model']['dir']}\")\n",
    "print(f\"  Device: {cfg['model']['device']}\")\n",
    "print(f\"  Compute type: {cfg['model']['compute_type']}\")\n",
    "print(f\"  Sample size: {cfg['input']['sample_size']} file(s)\")\n",
    "print(f\"  Duration: {cfg['input']['duration_sec']}s (first 5 minutes)\")\n",
    "print(f\"  Output: {cfg['output']['dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Inference\n",
    "\n",
    "This will:\n",
    "1. Load 1 sample from the parquet file\n",
    "2. Download audio from Azure blob storage\n",
    "3. Preprocess audio (trim to 300s, convert to 16kHz mono WAV)\n",
    "4. Load Whisper model (using faster-whisper for efficiency)\n",
    "5. Run transcription\n",
    "6. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference pipeline\n",
    "run(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View Results\n",
    "\n",
    "### 4.1 Inference Results (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results parquet\n",
    "results_path = Path(cfg['output']['dir']) / \"inference_results.parquet\"\n",
    "df_results = pd.read_parquet(results_path)\n",
    "\n",
    "print(f\"Results shape: {df_results.shape}\")\n",
    "print(f\"\\nColumns: {list(df_results.columns)}\")\n",
    "print(f\"\\nFirst row:\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hypothesis Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the transcription\n",
    "row = df_results.iloc[0]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"File ID: {row['file_id']}\")\n",
    "print(f\"Collection: {row['collection_number']}\")\n",
    "print(f\"Duration: {row['duration_sec']:.1f}s\")\n",
    "print(f\"Processing time: {row['processing_time_sec']:.1f}s\")\n",
    "print(f\"Status: {row['status']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTranscription (hypothesis):\\n\")\n",
    "print(row['hypothesis'][:500] + \"...\" if len(row['hypothesis']) > 500 else row['hypothesis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Individual Hypothesis File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check individual hypothesis file\n",
    "hyp_file = Path(cfg['output']['dir']) / f\"hyp_{row['file_id']}.txt\"\n",
    "\n",
    "if hyp_file.exists():\n",
    "    print(f\"Hypothesis file: {hyp_file}\")\n",
    "    print(f\"\\nContent:\\n\")\n",
    "    print(hyp_file.read_text()[:500] + \"...\" if len(hyp_file.read_text()) > 500 else hyp_file.read_text())\n",
    "else:\n",
    "    print(f\"Hypothesis file not found: {hyp_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Output\n",
    "\n",
    "**Key output files:**\n",
    "- `inference_results.parquet`: All results in structured format (hypothesis, duration, status, etc.)\n",
    "- `hyp_{file_id}.txt`: Individual hypothesis text files (one per audio file)\n",
    "- `hyp_whisper-{size}.txt`: Combined hypothesis text (all transcriptions concatenated)\n",
    "- `whisper_log_*.txt`: Detailed inference log\n",
    "\n",
    "**Key result columns:**\n",
    "- `file_id`: Unique identifier\n",
    "- `collection_number`: VHP collection number\n",
    "- `hypothesis`: Model transcription output\n",
    "- `ground_truth`: Reference transcript (for evaluation)\n",
    "- `duration_sec`: Audio duration processed\n",
    "- `processing_time_sec`: Time taken for inference\n",
    "- `status`: success/error\n",
    "- `model_name`: Model identifier (e.g., whisper-tiny)\n",
    "\n",
    "**Whisper model sizes:**\n",
    "- `tiny`: ~39MB, fastest, lowest accuracy\n",
    "- `base`: ~74MB\n",
    "- `small`: ~244MB\n",
    "- `medium`: ~769MB\n",
    "- `large-v3`: ~1550MB, best accuracy, slowest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "To run on more files:\n",
    "1. Increase `sample_size` (e.g., 10, 50, 100)\n",
    "2. Set `duration_sec: null` for full audio\n",
    "3. Use `device: \"cuda\"` if you have a GPU\n",
    "4. Try larger models: `\"large-v3\"` for best accuracy\n",
    "5. Use `compute_type: \"float16\"` on GPU for faster inference\n",
    "\n",
    "To evaluate results:\n",
    "```python\n",
    "from scripts.eval.evaluate import evaluate_results\n",
    "metrics = evaluate_results(df_results, use_whisper_normalizer=True)\n",
    "print(f\"WER: {metrics['wer']:.2%}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
