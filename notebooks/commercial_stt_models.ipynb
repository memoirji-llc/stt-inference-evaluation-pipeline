{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commercial STT Models Reference Notebook\n",
    "\n",
    "This notebook provides working examples for calling commercial Speech-to-Text APIs available on Azure, GCP, and AWS.\n",
    "\n",
    "## Models Covered (based on [Artificial Analysis STT Benchmark](https://artificialanalysis.ai/speech-to-text))\n",
    "\n",
    "### Available on Cloud Platforms:\n",
    "- **AWS**: Amazon Transcribe (WER: 14.01%)\n",
    "- **GCP**: Google Chirp 2 (WER: 11.58%), Google Chirp 3 (WER: 14.97%)\n",
    "- **Azure**: Azure Speech Services (not in benchmark but widely used)\n",
    "\n",
    "### Third-Party APIs (accessible from any cloud):\n",
    "- **AssemblyAI**: Universal-2 (WER: 14.49%)\n",
    "- **Rev AI** (WER: 15.21%)\n",
    "- **Speechmatics**: Enhanced (WER: 14.41%, $6.70/1000 min)\n",
    "- **ElevenLabs**: Scribe (WER: 15.07%)\n",
    "- **Deepgram**: Nova-2 (commonly used, check current benchmark)\n",
    "\n",
    "## Setup\n",
    "\n",
    "This notebook assumes you have credentials configured for:\n",
    "1. AWS (via boto3/environment variables)\n",
    "2. GCP (via service account JSON)\n",
    "3. Azure (via environment variables)\n",
    "4. API keys for third-party services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages via terminal\n",
    "# uv add boto3 google-cloud-storage google-cloud-speech azure-cognitiveservices-speech assemblyai deepgram-sdk python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# For testing - point to a sample audio file\n",
    "SAMPLE_AUDIO_PATH = \"../data/audio/testing/transcript_audio_sample.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../credentials/creds.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials Setup\n",
    "\n",
    "Load credentials from environment variables or a secure config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials (update paths as needed)\n",
    "# For production, use environment variables or secret management services\n",
    "\n",
    "# AWS - uses default credential chain (IAM role, ~/.aws/credentials, env vars)\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\", \"access_key_id\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"secret_key\")\n",
    "# GCP - set path to service account JSON\n",
    "GCP_CREDENTIALS_PATH = os.getenv(\"GCP_CREDENTIALS_PATH\", \"path/to/service-account.json\")\n",
    "\n",
    "# Azure\n",
    "AZURE_SPEECH_KEY = os.getenv(\"AZURE_SPEECH_KEY\", \"your-key-here\")\n",
    "AZURE_REGION = os.getenv(\"AZURE_REGION\", \"eastus\")\n",
    "\n",
    "# Third-party API keys\n",
    "ASSEMBLYAI_API_KEY = os.getenv(\"ASSEMBLYAI_API_KEY\", \"your-key-here\")\n",
    "DEEPGRAM_API_KEY = os.getenv(\"DEEPGRAM_API_KEY\", \"your-key-here\")\n",
    "REVAI_API_KEY = os.getenv(\"REVAI_API_KEY\", \"your-key-here\")\n",
    "SPEECHMATICS_API_KEY = os.getenv(\"SPEECHMATICS_API_KEY\", \"your-key-here\")\n",
    "ELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\", \"your-key-here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# GCP - Google Cloud Speech-to-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "from google.cloud import speech_v2\n",
    "from google.cloud.speech_v2 import SpeechClient\n",
    "from google.cloud.speech_v2.types import cloud_speech\n",
    "from google.api_core.client_options import ClientOptions\n",
    "\n",
    "# in case no bucket is created\n",
    "# storage_client = storage.Client()\n",
    "# bucket = storage_client.create_bucket(\"memoirji-amia-2025-temp\", location = \"us-central1\")\n",
    "# helper to delete bucket\n",
    "# bucket = storage_client.get_bucket(\"memoirji-amia-2025-temp\")\n",
    "# bucket.delete()\n",
    "\n",
    "def upload_file(storage_client, bucket_name, source_file_path, target_filename):\n",
    "    \"\"\"Uploads a file to GCS and returns the GCS URI.\"\"\"\n",
    "    try:\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        print(\"Using bucket\")\n",
    "        blob = bucket.blob(target_filename)\n",
    "        blob.upload_from_filename(source_file_path)\n",
    "        print(f\"File {source_file_path} uploaded.\")\n",
    "        gcs_uri = f\"gs://{bucket_name}/{target_filename}\"\n",
    "        return gcs_uri\n",
    "    except Exception as e:\n",
    "        print(f\"Upload error: {e}\")\n",
    "        return False\n",
    "    \n",
    "def delete_file(storage_client, bucket_name, target_filename):\n",
    "    \"\"\"Deletes a blob from the bucket.\"\"\"\n",
    "    try:\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(target_filename)\n",
    "        blob.delete()\n",
    "        print(f\"Blob {target_filename} deleted from {bucket_name}.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Delete error: {e}\")\n",
    "        return False\n",
    "\n",
    "def transcribe_with_gcp_chirp(audio_uri: str, speech_client, model: str, recognizer_location: str):\n",
    "    \"\"\"\n",
    "    Transcribe audio using Google Cloud Speech-to-Text with Chirp models.\n",
    "    \n",
    "    Args:\n",
    "        audio_uri: GCS URI (gs://bucket/file.mp3)\n",
    "        speech_client: SpeechClient instance\n",
    "        model: Model to use ('chirp' = Chirp 2, 'chirp_2' explicitly, 'chirp_3' = Chirp 3)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with transcription results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Starting transcription with {model}...\")\n",
    "        \n",
    "        config = cloud_speech.RecognitionConfig(\n",
    "            auto_decoding_config=cloud_speech.AutoDetectDecodingConfig(),\n",
    "            language_codes=[\"en-US\"],\n",
    "            model=model,\n",
    "        )\n",
    "        print(\"Config created\")\n",
    "        \n",
    "        file_metadata = cloud_speech.BatchRecognizeFileMetadata(uri=audio_uri)\n",
    "        print(f\"Processing: {audio_uri}\")\n",
    "        \n",
    "        request = cloud_speech.BatchRecognizeRequest(\n",
    "            recognizer=f\"projects/memoirji-amia-2025/locations/{recognizer_location}/recognizers/_\",\n",
    "            config=config,\n",
    "            files=[file_metadata],\n",
    "            recognition_output_config=cloud_speech.RecognitionOutputConfig(\n",
    "                inline_response_config=cloud_speech.InlineOutputConfig(),\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        operation = speech_client.batch_recognize(request=request)\n",
    "        print(\"Waiting for operation to complete...\")\n",
    "        response = operation.result(timeout=600)\n",
    "        \n",
    "        # Parse the response correctly\n",
    "        # Structure: response.results[uri].transcript.results[...].alternatives[0].transcript\n",
    "        transcript_segments = []\n",
    "        \n",
    "        # Get the result for our URI\n",
    "        if audio_uri in response.results:\n",
    "            uri_result = response.results[audio_uri]\n",
    "            \n",
    "            # Access the transcript results\n",
    "            if hasattr(uri_result, 'transcript') and hasattr(uri_result.transcript, 'results'):\n",
    "                for result in uri_result.transcript.results:\n",
    "                    if result.alternatives:\n",
    "                        transcript_segments.append(result.alternatives[0].transcript)\n",
    "        \n",
    "        full_transcript = ' '.join(transcript_segments)\n",
    "        \n",
    "        print(f\"✓ Transcription complete! ({len(transcript_segments)} segments)\")\n",
    "        \n",
    "        return {\n",
    "            'provider': f'GCP Speech-to-Text ({model})',\n",
    "            'text': full_transcript,\n",
    "            'segments': transcript_segments,\n",
    "            'full_response': response\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Transcription error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'provider': f'GCP Speech-to-Text ({model})',\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GCP_CREDENTIALS_PATH\n",
    "bucket_name = \"memoirji-amia-2025-temp\"\n",
    "\n",
    "source_file_path = SAMPLE_AUDIO_PATH\n",
    "target_filename = os.path.basename(source_file_path)\n",
    "\n",
    "# Model selection determines endpoint and recognizer location\n",
    "google_stt_model = \"chirp_3\"  # chirp = Chirp 2, chirp_2 = Chirp 2 explicit, chirp_3 = Chirp 3\n",
    "\n",
    "# IMPORTANT: Different models are available in different locations\n",
    "if google_stt_model in [\"chirp\", \"chirp_2\"]:\n",
    "    # Chirp 2: Available in us-central1\n",
    "    api_endpoint = \"us-central1-speech.googleapis.com\"\n",
    "    recognizer_location = \"us-central1\"\n",
    "    print(f\"Using Chirp 2 with location: {recognizer_location}\")\n",
    "elif google_stt_model == \"chirp_3\":\n",
    "    # Chirp 3: Available in global (NOT us-central1)\n",
    "    api_endpoint = \"us-speech.googleapis.com\"  # Global endpoint\n",
    "    recognizer_location = \"us\"\n",
    "    print(f\"Using Chirp 3 with location: {recognizer_location}\")\n",
    "else:\n",
    "    # Default fallback\n",
    "    api_endpoint = \"speech.googleapis.com\"\n",
    "    recognizer_location = \"us\"\n",
    "    print(f\"Using {google_stt_model} with location: {recognizer_location}\")\n",
    "\n",
    "speech_client = SpeechClient(\n",
    "    client_options=ClientOptions(\n",
    "        api_endpoint=api_endpoint,\n",
    "    )\n",
    ")\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Upload and transcribe\n",
    "gcs_filepath = upload_file(storage_client, bucket_name, source_file_path, target_filename)\n",
    "result_gcp_chirp = transcribe_with_gcp_chirp(\n",
    "        gcs_filepath, \n",
    "        speech_client, \n",
    "        model=google_stt_model,\n",
    "        recognizer_location=recognizer_location\n",
    "    )\n",
    "\n",
    "if gcs_filepath:\n",
    "    # Clean up\n",
    "    delete_file(storage_client, bucket_name, target_filename)\n",
    "    \n",
    "    # Show result\n",
    "    if 'text' in result_gcp_chirp:\n",
    "        print(f\"\\n✓ Transcription successful!\")\n",
    "        print(f\"Preview: {result_gcp_chirp['text'][:200]}...\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Error: {result_gcp_chirp.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# AWS - Amazon Transcribe\n",
    "claim:\n",
    "Easily embed voice technologies in your applications with Amazon Transcribe, a fully managed, multi-billion parameter speech foundation model that instantly converts real-time or recorded speech into text. It is trained on millions of hours of audio data across a variety of languages.\n",
    "\n",
    "**WER: 14.01%** (from Artificial Analysis)\n",
    "\n",
    "Amazon Transcribe offers both batch and streaming transcription. This example shows batch transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "# define bucket/ target obj key\n",
    "bucket_name = \"amia2025-test-bucket\"\n",
    "# define aws transcribe job name\n",
    "aws_transcribe_job_name = \"transcribe-job-test\"\n",
    "\n",
    "source_file_path = SAMPLE_AUDIO_PATH\n",
    "target_filename = os.path.basename(source_file_path)\n",
    "s3_object_key = target_filename\n",
    "\n",
    "# set up s3 client and test upload\n",
    "s3 = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "try:\n",
    "    s3.upload_file(SAMPLE_AUDIO_PATH, bucket_name, s3_object_key)\n",
    "    print(f\"Successfully uploaded {SAMPLE_AUDIO_PATH} to {bucket_name}/{s3_object_key}\")\n",
    "    s3_uri = f\"s3://{bucket_name}/{s3_object_key}\"\n",
    "    print(f\"S3 URI: {s3_uri}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file: {e}\")\n",
    "\n",
    "# set up AWS transcribe service client\n",
    "aws_transcribe = boto3.client(\n",
    "    'transcribe',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_REGION)\n",
    "# start transcription job\n",
    "\n",
    "l_media_objects = [s3_uri]\n",
    "\n",
    "for object in l_media_objects:\n",
    "    try:\n",
    "        job = aws_transcribe.start_transcription_job(\n",
    "            TranscriptionJobName=aws_transcribe_job_name,\n",
    "            Media={'MediaFileUri': s3_uri},\n",
    "            # do not specify media format as mp3, as we will normalizing audio, either mp3 or mp4, to 16kHz mono WAV in prod)\n",
    "            # MediaFormat='mp3',\n",
    "            LanguageCode='en-US')\n",
    "        while True:\n",
    "            # job status monitoring and transcription results parsing\n",
    "            job = aws_transcribe.get_transcription_job(TranscriptionJobName=aws_transcribe_job_name)\n",
    "            status = job['TranscriptionJob']['TranscriptionJobStatus']\n",
    "            if status == 'COMPLETED':\n",
    "                print(f\"Job {aws_transcribe_job_name} completed\")\n",
    "                with urllib.request.urlopen(job['TranscriptionJob']['Transcript']['TranscriptFileUri']) as r:\n",
    "                    data = json.loads(r.read())\n",
    "                print(data['results']['transcripts'][0]['transcript'][:50] + \"...\")\n",
    "                response = s3.delete_object(Bucket=bucket_name, Key=s3_object_key)\n",
    "                print(f\"Object '{s3_object_key}' deleted successfully from bucket '{bucket_name}'.\")\n",
    "                break\n",
    "            elif status == 'FAILED':\n",
    "                print(f\"Job {aws_transcribe_job_name} failed\")\n",
    "                print(None)\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Status of job {aws_transcribe_job_name}: {status}\")\n",
    "                time.sleep(10)\n",
    "        # remove job\n",
    "        aws_transcribe.delete_transcription_job(TranscriptionJobName=aws_transcribe_job_name)\n",
    "        print(f\"Job {aws_transcribe_job_name} removed after transcription completed\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Azure - Azure Speech Services\n",
    "\n",
    "Azure Speech Services offers competitive accuracy and is well-integrated with the Azure ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Batch Testing & Comparison\n",
    "\n",
    "Helper functions to test multiple providers and compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Integration with Pipeline\n",
    "\n",
    "When integrating these models into your benchmarking pipeline, consider:\n",
    "\n",
    "1. **Standardized Interface**: Create a wrapper that normalizes outputs across providers\n",
    "2. **Rate Limiting**: Implement backoff/retry for API limits\n",
    "3. **Cost Tracking**: Log API calls and costs for budget management\n",
    "4. **Error Handling**: Graceful fallbacks when services are unavailable\n",
    "5. **Batch Processing**: Some providers offer batch discounts\n",
    "\n",
    "Example standardized interface:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Artificial Analysis STT Benchmark](https://artificialanalysis.ai/speech-to-text)\n",
    "- [AWS Transcribe Docs](https://docs.aws.amazon.com/transcribe/)\n",
    "- [GCP Speech-to-Text Docs](https://cloud.google.com/speech-to-text)\n",
    "- [Azure Speech Services Docs](https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/)\n",
    "- [AssemblyAI Docs](https://www.assemblyai.com/docs)\n",
    "- [Deepgram Docs](https://developers.deepgram.com/)\n",
    "- [Rev AI Docs](https://docs.rev.ai/)\n",
    "- [Speechmatics Docs](https://docs.speechmatics.com/)\n",
    "- [ElevenLabs Docs](https://elevenlabs.io/docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
