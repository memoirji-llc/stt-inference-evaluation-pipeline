{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: NVIDIA Canary-Qwen Inference Pipeline\n",
    "\n",
    "This notebook demonstrates how to run the NVIDIA Canary-Qwen inference pipeline on a single sample file.\n",
    "\n",
    "**What this notebook does:**\n",
    "- Uses production orchestration (Azure blob storage)\n",
    "- Runs the smallest model (`nvidia/canary-1b`)\n",
    "- Processes just 1 sample file\n",
    "- Shows outputs (inference results, hypothesis text)\n",
    "\n",
    "**Note**: Canary-Qwen is a speech language model (SALM) that combines ASR with language understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Import and Configure Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Disable wandb for notebook runs (tutorials don't need tracking)\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "# Set HuggingFace cache to use local models directory (avoid re-downloading)\n",
    "os.environ['HF_HOME'] = str(Path.cwd().parent / \"models/canary\")\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(Path.cwd().parent / \"models/canary\")\n",
    "\n",
    "# Add scripts directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"scripts\"))\n",
    "\n",
    "from infer_canary import run\n",
    "\n",
    "# System and GPU Memory Check\n",
    "print(\"=\" * 70)\n",
    "print(\"Memory Status Check\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check system RAM\n",
    "try:\n",
    "    import psutil\n",
    "    mem = psutil.virtual_memory()\n",
    "    ram_used = mem.used / 1024**3\n",
    "    ram_total = mem.total / 1024**3\n",
    "    ram_available = mem.available / 1024**3\n",
    "    \n",
    "    print(f\"System RAM: {ram_used:.1f}/{ram_total:.1f} GB used\")\n",
    "    print(f\"Available: {ram_available:.1f} GB\")\n",
    "    \n",
    "    if ram_available < 10:\n",
    "        print(f\"âš ï¸  WARNING: Low RAM! Canary needs ~10 GB free for model loading\")\n",
    "        print(f\"   Current available: {ram_available:.1f} GB\")\n",
    "except ImportError:\n",
    "    print(\"psutil not installed, skipping RAM check\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print()\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        free = total - reserved\n",
    "        \n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Total: {total:.2f} GB\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  Free: {free:.2f} GB\")\n",
    "        \n",
    "        if free < 3.0:\n",
    "            print(f\"  âš ï¸  WARNING: Low GPU memory! Only {free:.2f} GB free\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No CUDA GPU available - Canary requires GPU!\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nðŸ’¡ Note: Wandb logging is disabled for tutorial runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1 GPU Memory Cleanup (Run if needed)\n",
    "\n",
    "If you see \"CUDA out of memory\" errors, run this cell to clear GPU memory from previous runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Force Python garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ“ GPU memory cleared\")\n",
    "\n",
    "# Show updated memory status\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        free = total - reserved\n",
    "        print(f\"GPU {i}: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved, {free:.2f} GB free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Inference Parameters\n",
    "\n",
    "Set parameters directly in Python (no yaml file needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build configuration dictionary\n",
    "cfg = {\n",
    "    \"experiment_id\": \"tutorial-canary-sample1\",\n",
    "    \n",
    "    # Model settings\n",
    "    \"model\": {\n",
    "        \"name\": \"canary\",\n",
    "        \"path\": \"nvidia/canary-qwen-2.5b\",  # Options: nvidia/canary-1b-v2, nvidia/canary-1b-flash, nvidia/canary-qwen-2.5b\n",
    "        \"device\": \"cuda\",  # Canary requires GPU (use \"cuda\")\n",
    "        \"prompt\": \"Transcribe the following:\",  # User prompt for the model\n",
    "        \"max_new_tokens\": 512,  # Max tokens to generate\n",
    "        \"batch_size\": 1  # Process one file at a time\n",
    "    },\n",
    "    \n",
    "    # Input settings\n",
    "    \"input\": {\n",
    "        \"source\": \"azure_blob\",\n",
    "        \"parquet_path\": \"../data/raw/loc/veterans_history_project_resources_pre2010.parquet\",  # Relative to notebook dir\n",
    "        \"blob_prefix\": \"loc_vhp\",\n",
    "        \"sample_size\": 1,  # Just 1 file for tutorial\n",
    "        \"duration_sec\": 300,  # First 5 minutes only (faster)\n",
    "        \"sample_rate\": 16000  # Canary expects 16kHz\n",
    "    },\n",
    "    \n",
    "    # Output settings\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/tutorial-canary-sample1\",  # Relative to notebook dir\n",
    "        \"save_per_file\": True  # Save individual hypothesis files\n",
    "    },\n",
    "    \n",
    "    # Evaluation (optional)\n",
    "    \"evaluation\": {\n",
    "        \"use_whisper_normalizer\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {cfg['model']['path']} (~2.5GB)\")\n",
    "print(f\"  Device: {cfg['model']['device']} (GPU required)\")\n",
    "print(f\"  Sample size: {cfg['input']['sample_size']} file(s)\")\n",
    "print(f\"  Duration: {cfg['input']['duration_sec']}s (first 5 minutes)\")\n",
    "print(f\"  Output: {cfg['output']['dir']}\")\n",
    "print(f\"\\nâš ï¸  Note: Canary requires a CUDA-enabled GPU\")\n",
    "print(f\"\\nðŸ’¡ Tip: First run will download the model (~2.5GB). Subsequent runs use cached model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Inference\n",
    "\n",
    "This will:\n",
    "1. Load 1 sample from the parquet file\n",
    "2. Download audio from Azure blob storage\n",
    "3. Preprocess audio (trim to 300s, convert to 16kHz mono WAV)\n",
    "4. Load Canary-Qwen model\n",
    "5. Run transcription with speech-language model\n",
    "6. Save results\n",
    "\n",
    "**Note**: First run will download the model (~1GB for canary-1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference pipeline\n",
    "run(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View Results\n",
    "\n",
    "### 4.1 Inference Results (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results parquet\n",
    "results_path = Path(cfg['output']['dir']) / \"inference_results.parquet\"\n",
    "df_results = pd.read_parquet(results_path)\n",
    "\n",
    "print(f\"Results shape: {df_results.shape}\")\n",
    "print(f\"\\nColumns: {list(df_results.columns)}\")\n",
    "print(f\"\\nFirst row:\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hypothesis Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the transcription\n",
    "row = df_results.iloc[0]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"File ID: {row['file_id']}\")\n",
    "print(f\"Collection: {row['collection_number']}\")\n",
    "print(f\"Duration: {row['duration_sec']:.1f}s\")\n",
    "print(f\"Processing time: {row['processing_time_sec']:.1f}s\")\n",
    "print(f\"Status: {row['status']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTranscription (hypothesis):\\n\")\n",
    "print(row['hypothesis'][:500] + \"...\" if len(row['hypothesis']) > 500 else row['hypothesis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Individual Hypothesis File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check individual hypothesis file\n",
    "hyp_file = Path(cfg['output']['dir']) / f\"hyp_{row['file_id']}.txt\"\n",
    "\n",
    "if hyp_file.exists():\n",
    "    print(f\"Hypothesis file: {hyp_file}\")\n",
    "    print(f\"\\nContent:\\n\")\n",
    "    print(hyp_file.read_text()[:500] + \"...\" if len(hyp_file.read_text()) > 500 else hyp_file.read_text())\n",
    "else:\n",
    "    print(f\"Hypothesis file not found: {hyp_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Output\n",
    "\n",
    "**Key output files:**\n",
    "- `inference_results.parquet`: All results in structured format (hypothesis, duration, status, etc.)\n",
    "- `hyp_{file_id}.txt`: Individual hypothesis text files (one per audio file)\n",
    "- `hyp_canary.txt`: Combined hypothesis text (all transcriptions concatenated)\n",
    "- `canary_log_*.txt`: Detailed inference log\n",
    "\n",
    "**Key result columns:**\n",
    "- `file_id`: Unique identifier\n",
    "- `collection_number`: VHP collection number\n",
    "- `hypothesis`: Model transcription output\n",
    "- `ground_truth`: Reference transcript (for evaluation)\n",
    "- `duration_sec`: Audio duration processed\n",
    "- `processing_time_sec`: Time taken for inference\n",
    "- `status`: success/error\n",
    "- `model_name`: Model identifier\n",
    "\n",
    "**Available Canary models:**\n",
    "- `nvidia/canary-1b-v2`: ~1GB, multilingual (25 languages), good accuracy\n",
    "- `nvidia/canary-1b-flash`: ~883M params, fastest (1000+ RTFx), 4 languages\n",
    "- `nvidia/canary-qwen-2.5b`: ~2.5GB, best accuracy, English with punctuation (recommended for benchmarking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Canary-Qwen\n",
    "\n",
    "Canary-Qwen is a **Speech Language Model (SALM)** that combines:\n",
    "- **ASR (Automatic Speech Recognition)**: Transcribes audio to text\n",
    "- **Language understanding**: Can follow instructions and provide structured outputs\n",
    "\n",
    "**Key features:**\n",
    "- Supports multiple languages\n",
    "- Can be prompted with instructions (e.g., \"Transcribe the following:\", \"Summarize this audio:\")\n",
    "- Better at handling conversational speech\n",
    "- Requires GPU (CUDA)\n",
    "\n",
    "**Performance** (from Artificial Analysis benchmark):\n",
    "- Canary performs competitively with Whisper and other open-source models\n",
    "- Better at preserving speaker turns and conversational structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "To run on more files:\n",
    "1. Increase `sample_size` (e.g., 10, 50, 100)\n",
    "2. Set `duration_sec: null` for full audio\n",
    "3. Try the larger model: `\"nvidia/canary-qwen-2.5b\"` for better accuracy\n",
    "4. Customize the prompt: `\"prompt\": \"Transcribe this interview:\"`\n",
    "5. Adjust `max_new_tokens` for longer transcriptions (default: 512)\n",
    "\n",
    "To evaluate results:\n",
    "```python\n",
    "from scripts.evaluate import evaluate_results\n",
    "metrics = evaluate_results(df_results, use_whisper_normalizer=True)\n",
    "print(f\"WER: {metrics['wer']:.2%}\")\n",
    "```\n",
    "\n",
    "**Recommended config for benchmarking:**\n",
    "```python\n",
    "cfg = {\n",
    "    \"model\": {\n",
    "        \"path\": \"nvidia/canary-qwen-2.5b\",  # Better accuracy\n",
    "        \"device\": \"cuda\",\n",
    "        \"max_new_tokens\": 1024  # Longer transcriptions\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"sample_size\": 500,  # Full benchmark\n",
    "        \"duration_sec\": null  # Full audio\n",
    "    }\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
