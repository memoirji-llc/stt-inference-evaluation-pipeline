{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: NVIDIA Canary-Qwen-2.5B Inference Pipeline\n",
    "\n",
    "This notebook demonstrates how to run the NVIDIA Canary-Qwen-2.5B inference pipeline on a single sample file.\n",
    "\n",
    "**What this notebook does:**\n",
    "- Uses production orchestration (Azure blob storage)\n",
    "- Runs Canary-Qwen-2.5B model (best accuracy)\n",
    "- Processes just 1 sample file for testing\n",
    "- Shows outputs (inference results, hypothesis text)\n",
    "\n",
    "**Requirements:**\n",
    "- GPU: ~10 GB VRAM\n",
    "- RAM: ~5 GB system memory\n",
    "- Azure credentials configured (for blob storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Import and Configure Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Disable wandb for notebook runs (tutorials don't need tracking)\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "# Set HuggingFace cache to use local models directory (avoid re-downloading)\n",
    "os.environ['HF_HOME'] = str(Path.cwd().parent / \"models/canary\")\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(Path.cwd().parent / \"models/canary\")\n",
    "\n",
    "# Add scripts directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"scripts\"))\n",
    "\n",
    "from infer_canary import run\n",
    "\n",
    "# System and GPU Memory Check\n",
    "print(\"=\" * 70)\n",
    "print(\"Memory Status Check\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check system RAM\n",
    "try:\n",
    "    import psutil\n",
    "    mem = psutil.virtual_memory()\n",
    "    ram_used = mem.used / 1024**3\n",
    "    ram_total = mem.total / 1024**3\n",
    "    ram_available = mem.available / 1024**3\n",
    "    \n",
    "    print(f\"System RAM: {ram_used:.1f}/{ram_total:.1f} GB used\")\n",
    "    print(f\"Available: {ram_available:.1f} GB\")\n",
    "    \n",
    "    if ram_available < 10:\n",
    "        print(f\"âš ï¸  WARNING: Only {ram_available:.1f} GB RAM available\")\n",
    "        print(f\"   Canary-Qwen needs ~5 GB free for model loading\")\n",
    "except ImportError:\n",
    "    print(\"psutil not installed, skipping RAM check\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print()\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        free = total - reserved\n",
    "        \n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Total: {total:.2f} GB\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  Free: {free:.2f} GB\")\n",
    "        \n",
    "        if free < 10.0:\n",
    "            print(f\"  âš ï¸  WARNING: Only {free:.2f} GB GPU free\")\n",
    "            print(f\"     Canary-Qwen needs ~10 GB GPU VRAM\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No CUDA GPU available - Canary requires GPU!\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nðŸ’¡ Note: Wandb logging is disabled for tutorial runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 GPU Memory Cleanup (Run if needed)\n",
    "\n",
    "If you see \"CUDA out of memory\" errors, run this cell to clear GPU memory from previous runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Force Python garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ“ GPU memory cleared\")\n",
    "\n",
    "# Show updated memory status\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        free = total - reserved\n",
    "        print(f\"GPU {i}: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved, {free:.2f} GB free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Inference Parameters\n",
    "\n",
    "Set parameters directly in Python (no yaml file needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build configuration dictionary\n",
    "cfg = {\n",
    "    \"experiment_id\": \"tutorial-canary-qwen-sample1\",\n",
    "    \n",
    "    # Model settings\n",
    "    \"model\": {\n",
    "        \"name\": \"canary\",\n",
    "        \"path\": \"nvidia/canary-qwen-2.5b\",  # Best accuracy model\n",
    "        \"device\": \"cuda\",  # Canary requires GPU (use \"cuda\")\n",
    "        \"prompt\": \"Transcribe the following:\",  # User prompt for the model\n",
    "        \"max_new_tokens\": 512,  # Max tokens to generate\n",
    "        \"batch_size\": 1  # Process one file at a time\n",
    "    },\n",
    "    \n",
    "    # Input settings\n",
    "    \"input\": {\n",
    "        \"source\": \"azure_blob\",\n",
    "        \"parquet_path\": \"../data/raw/loc/veterans_history_project_resources_pre2010.parquet\",  # Relative to notebook dir\n",
    "        \"blob_prefix\": \"loc_vhp\",\n",
    "        \"sample_size\": 1,  # Just 1 file for tutorial\n",
    "        \"duration_sec\": 300,  # First 5 minutes only (faster)\n",
    "        \"sample_rate\": 16000  # Canary expects 16kHz\n",
    "    },\n",
    "    \n",
    "    # Output settings\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/tutorial-canary-qwen-sample1\",  # Relative to notebook dir\n",
    "        \"save_per_file\": True  # Save individual hypothesis files\n",
    "    },\n",
    "    \n",
    "    # Evaluation (optional)\n",
    "    \"evaluation\": {\n",
    "        \"use_whisper_normalizer\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {cfg['model']['path']}\")\n",
    "print(f\"  Memory: ~5 GB RAM, ~10 GB GPU\")\n",
    "print(f\"  Device: {cfg['model']['device']} (GPU required)\")\n",
    "print(f\"  Sample size: {cfg['input']['sample_size']} file(s)\")\n",
    "print(f\"  Duration: {cfg['input']['duration_sec']}s (first 5 minutes)\")\n",
    "print(f\"  Output: {cfg['output']['dir']}\")\n",
    "print(f\"\\nâš ï¸  Note: Canary requires a CUDA-enabled GPU\")\n",
    "print(f\"\\nðŸ’¡ Tip: First run will download the model (~5GB). Subsequent runs use cached model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Inference\n",
    "\n",
    "This will:\n",
    "1. Load 1 sample from the parquet file\n",
    "2. Download audio from Azure blob storage\n",
    "3. Preprocess audio (trim to 300s, convert to 16kHz mono WAV)\n",
    "4. Load Canary-Qwen-2.5B model (~5GB RAM, ~10GB GPU)\n",
    "5. Run transcription with speech-language model\n",
    "6. Save results\n",
    "\n",
    "**Note**: First run will download the model (~5GB). Cached after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference pipeline\n",
    "run(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View Results\n",
    "\n",
    "### 4.1 Inference Results (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results parquet\n",
    "results_path = Path(cfg['output']['dir']) / \"inference_results.parquet\"\n",
    "df_results = pd.read_parquet(results_path)\n",
    "\n",
    "print(f\"Results shape: {df_results.shape}\")\n",
    "print(f\"\\nColumns: {list(df_results.columns)}\")\n",
    "print(f\"\\nFirst row:\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hypothesis Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the transcription\n",
    "row = df_results.iloc[0]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"File ID: {row['file_id']}\")\n",
    "print(f\"Collection: {row['collection_number']}\")\n",
    "print(f\"Duration: {row['duration_sec']:.1f}s\")\n",
    "print(f\"Processing time: {row['processing_time_sec']:.1f}s\")\n",
    "print(f\"Status: {row['status']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTranscription (hypothesis):\\n\")\n",
    "print(row['hypothesis'][:500] + \"...\" if len(row['hypothesis']) > 500 else row['hypothesis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Individual Hypothesis File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check individual hypothesis file\n",
    "hyp_file = Path(cfg['output']['dir']) / f\"hyp_{row['file_id']}.txt\"\n",
    "\n",
    "if hyp_file.exists():\n",
    "    print(f\"Hypothesis file: {hyp_file}\")\n",
    "    print(f\"\\nContent:\\n\")\n",
    "    print(hyp_file.read_text()[:500] + \"...\" if len(hyp_file.read_text()) > 500 else hyp_file.read_text())\n",
    "else:\n",
    "    print(f\"Hypothesis file not found: {hyp_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Output\n",
    "\n",
    "**Key output files:**\n",
    "- `inference_results.parquet`: All results in structured format (hypothesis, duration, status, etc.)\n",
    "- `hyp_{file_id}.txt`: Individual hypothesis text files (one per audio file)\n",
    "- `hyp_canary.txt`: Combined hypothesis text (all transcriptions concatenated)\n",
    "- `canary_log_*.txt`: Detailed inference log\n",
    "\n",
    "**Key result columns:**\n",
    "- `file_id`: Unique identifier\n",
    "- `collection_number`: VHP collection number\n",
    "- `hypothesis`: Model transcription output\n",
    "- `ground_truth`: Reference transcript (for evaluation)\n",
    "- `duration_sec`: Audio duration processed\n",
    "- `processing_time_sec`: Time taken for inference\n",
    "- `status`: success/error\n",
    "- `model_name`: Model identifier\n",
    "\n",
    "**Model used:**\n",
    "- `nvidia/canary-qwen-2.5b`: 2.5B params, English with punctuation/capitalization, top OpenASR leaderboard (WER: 5.63%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Canary-Qwen-2.5B\n",
    "\n",
    "Canary-Qwen is a **Speech Audio Language Model (SALM)** that combines:\n",
    "- **NVIDIA Canary-1B** (ASR encoder): Fast-Conformer architecture for speech recognition\n",
    "- **Qwen3-1.7B** (LLM decoder): Language model with LoRA adaptation for text generation\n",
    "\n",
    "**Key features:**\n",
    "- 2.5 billion parameters total\n",
    "- English speech-to-text with punctuation and capitalization\n",
    "- Top performer on OpenASR leaderboard (WER: 5.63%)\n",
    "- 418Ã— RTFx (processes audio 418Ã— faster than real-time)\n",
    "- Trained on 234K hours of public speech data\n",
    "\n",
    "**Memory requirements (tested):**\n",
    "- GPU: ~10 GB VRAM\n",
    "- RAM: ~5 GB system memory\n",
    "- Fits on T4 16GB GPU\n",
    "\n",
    "**Performance:**\n",
    "- Better at handling degraded/archival audio than Whisper\n",
    "- Preserves conversational structure and speaker patterns\n",
    "- Good for oral history transcription tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "To run on more files:\n",
    "1. Increase `sample_size` (e.g., 10, 50, 500)\n",
    "2. Set `duration_sec: None` for full audio\n",
    "3. Customize the prompt: `\"prompt\": \"Transcribe this interview:\"`\n",
    "4. Adjust `max_new_tokens` for longer transcriptions (default: 512, max: 1024)\n",
    "\n",
    "To evaluate results:\n",
    "```python\n",
    "# This will be available after implementing evaluation\n",
    "from scripts.evaluate import evaluate_results\n",
    "metrics = evaluate_results(df_results, use_whisper_normalizer=True)\n",
    "print(f\"WER: {metrics['wer']:.2%}\")\n",
    "```\n",
    "\n",
    "**Recommended config for 500-sample benchmark:**\n",
    "```python\n",
    "cfg = {\n",
    "    \"experiment_id\": \"vhp-canary-qwen-500\",\n",
    "    \"model\": {\n",
    "        \"path\": \"nvidia/canary-qwen-2.5b\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"max_new_tokens\": 1024  # Longer transcriptions\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"sample_size\": 500,  # Full benchmark\n",
    "        \"duration_sec\": None  # Full audio (not just 5 minutes)\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/vhp-canary-qwen-500\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
