{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "REF: \"I went to the store\"\n",
    "HYP: \"I go to store\"\n",
    "\n",
    "- SUBSTITUTION: \"went\" → \"go\" (wrong word)\n",
    "- DELETION: \"the\" missing (model skipped a word)\n",
    "- INSERTION: (none in this example)\n",
    "\n",
    "WER = (S + D + I) / N = (1 + 1 + 0) / 5 = 0.4 (40% error)\n",
    "\n",
    "Example 1: \"I went to the store\"\n",
    "         → \"I go to the shop\"\n",
    "         S=2, D=0, I=0, WER=0.4\n",
    "\n",
    "Example 2: \"I went to the store\"  \n",
    "         → \"I to store\"\n",
    "         S=0, D=2, I=0, WER=0.4\n",
    "\n",
    "Same WER, but is the meaning preservation the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "4. WER Variations & Alternatives\n",
    "Research and document: a) Weighted WER (WWER)\n",
    "Can you assign different costs to different error types?\n",
    "Example: Substitution = 1, Deletion = 0.5, Insertion = 0.5\n",
    "b) Character Error Rate (CER)\n",
    "Word-level vs character-level\n",
    "When is CER more appropriate?\n",
    "c) Match Error Rate (MER)\n",
    "Doesn't count insertions in denominator\n",
    "MER = (S + D) / N\n",
    "d) Normalized WER\n",
    "Accounting for different normalization strategies\n",
    "5. Parameters in jiwer\n",
    "Look at the jiwer documentation and explain:\n",
    "What transformations are available?\n",
    "RemovePunctuation() - does this hide real errors?\n",
    "ToLowerCase() - is \"Apple\" vs \"apple\" really an error?\n",
    "RemoveMultipleSpaces() - formatting vs content\n",
    "6. Critical Analysis for Your AMIA Report\n",
    "Questions to explore:\n",
    "For degraded VHP audio, which error types dominate? (S, D, or I?)\n",
    "Does this tell you something about the failure mode?\n",
    "High deletions = model gave up on unclear audio?\n",
    "High substitutions = model heard something but got it wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook is created to learn about evaluating stt results.\n",
    "# there are some inferences run on wandb, with evals being done which can be further explored here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = \"vhp-pre2010-distil-large-v3-full-gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_inf = pd.read_parquet(f\"../outputs/{experiment_id}/inference_results.parquet\")\n",
    "df_inf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv(f\"../outputs/{experiment_id}/evaluation_results.csv\")\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all scripts for the experiments are put under /scripts.\n",
    "# the evaluations bits are under evaluate.py\n",
    "# here we are explaining step by step of how we compare the inference results (hypothesis) against gt (df_gt in the code):\n",
    "# (assume args.parque is \"../data/veterans_history_project_resources.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "df_gt = pd.read_parquet(\"../data/raw/loc/veterans_history_project_resources_pre2010.parquet\")\n",
    "\n",
    "# Pick one example to walk through (file_id=0)\n",
    "file_id = 0\n",
    "gt_row = df_gt.iloc[file_id]\n",
    "\n",
    "print(f\"=== GROUND TRUTH FOR FILE {file_id} ===\")\n",
    "print(f\"Collection: {gt_row['collection_number']}\")\n",
    "print()\n",
    "print(f\"Raw XML transcript (first 500 chars):\")\n",
    "print(gt_row['fulltext_file_str'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the cleaning process step-by-step\n",
    "def clean_raw_transcript_str(fulltext_file_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean raw XML transcript from VHP dataset.\n",
    "    This function is from scripts/evaluate.py\n",
    "    \"\"\"\n",
    "    if not fulltext_file_str or pd.isna(fulltext_file_str):\n",
    "        return \"\"\n",
    "    \n",
    "    l_transcript_lines = []\n",
    "    \n",
    "    # Step 1: Parse XML to extract speaker and text\n",
    "    soup = BeautifulSoup(fulltext_file_str, 'xml')\n",
    "    \n",
    "    for sp in soup.find_all('sp'):\n",
    "        try:\n",
    "            speaker = sp.find('speaker').get_text(strip=True)\n",
    "        except:\n",
    "            speaker = \"speaker_unknown\"\n",
    "        try:\n",
    "            spoken_text = sp.find('p').get_text(strip=True)\n",
    "        except:\n",
    "            spoken_text = \"\"\n",
    "        \n",
    "        l_transcript_lines.append(f\"<{speaker}>{spoken_text}</{speaker}> \")\n",
    "    \n",
    "    # Step 2: Merge lines\n",
    "    transcript_lines = ''.join(l_transcript_lines)\n",
    "    print(\"STEP 1 - After parsing XML:\")\n",
    "    print(transcript_lines[:300])\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Remove annotations: (), [], {}\n",
    "    transcript_lines_stripped = re.sub(r'\\([^)]*\\)', '', transcript_lines)\n",
    "    transcript_lines_stripped = re.sub(r'\\[[^]]*\\]', '', transcript_lines_stripped)\n",
    "    transcript_lines_stripped = re.sub(r'\\{[^}]*\\)\\}', '', transcript_lines_stripped)\n",
    "    print(\"STEP 2 - After removing annotations:\")\n",
    "    print(transcript_lines_stripped[:300])\n",
    "    print()\n",
    "    \n",
    "    # Step 4: Remove dashes and ellipsis\n",
    "    transcript_lines_stripped = re.sub(r'--+', '', transcript_lines_stripped)\n",
    "    transcript_lines_stripped = re.sub(r'\\.{2,}', '', transcript_lines_stripped)\n",
    "    \n",
    "    # Step 5: Clean whitespace\n",
    "    transcript_lines_stripped = re.sub(r'\\s+', ' ', transcript_lines_stripped).strip()\n",
    "    \n",
    "    # Step 6: Remove speaker tags\n",
    "    transcript_lines_stripped = re.sub(r'\\<[^>]*\\>', '', transcript_lines_stripped)\n",
    "    print(\"STEP 3 - After removing speaker tags:\")\n",
    "    print(transcript_lines_stripped[:300])\n",
    "    print()\n",
    "    \n",
    "    return transcript_lines_stripped\n",
    "\n",
    "# Apply cleaning\n",
    "reference = clean_raw_transcript_str(gt_row['fulltext_file_str'])\n",
    "print(\"=== FINAL CLEANED REFERENCE ===\")\n",
    "print(f\"Length: {len(reference)} characters\")\n",
    "print(f\"First 500 chars: {reference[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hypothesis and show normalization\n",
    "hypothesis = df_inf.iloc[file_id]['hypothesis']\n",
    "\n",
    "print(\"=== HYPOTHESIS (Model Output) ===\")\n",
    "print(f\"Length: {len(hypothesis)} characters\")\n",
    "print(f\"First 500 chars: {hypothesis[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show normalization process\n",
    "import jiwer\n",
    "\n",
    "def normalize(s):\n",
    "    \"\"\"Normalize text for WER calculation (from scripts/evaluate.py)\"\"\"\n",
    "    tx = jiwer.Compose([\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemovePunctuation(),\n",
    "        jiwer.Strip(),\n",
    "        jiwer.RemoveMultipleSpaces()\n",
    "    ])\n",
    "    return tx(s)\n",
    "\n",
    "# Normalize both gt and predictions (gt was cleaned already in production setting)\n",
    "ref_norm = normalize(reference)\n",
    "hyp_norm = normalize(hypothesis)\n",
    "\n",
    "print(\"=== NORMALIZATION PROCESS ===\")\n",
    "print()\n",
    "print(\"REFERENCE BEFORE:\")\n",
    "print(reference[:200])\n",
    "print()\n",
    "print(\"REFERENCE AFTER:\")\n",
    "print(ref_norm[:200])\n",
    "print()\n",
    "print(\"HYPOTHESIS BEFORE:\")\n",
    "print(hypothesis[:200])\n",
    "print()\n",
    "print(\"HYPOTHESIS AFTER:\")\n",
    "print(hyp_norm[:200])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WER step-by-step\n",
    "m = jiwer.process_words(ref_norm, hyp_norm)\n",
    "\n",
    "print(\"=== WER CALCULATION WITH jiwer(from scripts/evaluate.py)===\")\n",
    "print()\n",
    "print(f\"Reference words: {len(ref_norm.split())}\")\n",
    "print(f\"Hypothesis words: {len(hyp_norm.split())}\")\n",
    "print()\n",
    "print(f\"Substitutions: {m.substitutions} (wrong word)\")\n",
    "print(f\"Deletions: {m.deletions} (model missed word)\")\n",
    "print(f\"Insertions: {m.insertions} (model added extra word)\")\n",
    "print()\n",
    "print(f\"Total errors: {m.substitutions + m.deletions + m.insertions}\")\n",
    "print(f\"WER = Total errors / Reference words\")\n",
    "print(f\"WER = {m.substitutions + m.deletions + m.insertions} / {len(ref_norm.split())}\")\n",
    "print(f\"WER = {m.wer:.3f} ({m.wer*100:.1f}%)\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "if m.wer < 0.1:\n",
    "    print(\"  → Excellent (< 10% error)\")\n",
    "elif m.wer < 0.3:\n",
    "    print(\"  → Good (10-30% error)\")\n",
    "elif m.wer < 0.5:\n",
    "    print(\"  → Fair (30-50% error)\")\n",
    "else:\n",
    "    print(\"  → Poor (> 50% error)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize alignment on word-to-word level\n",
    "alignment = jiwer.process_words(ref_norm, hyp_norm)\n",
    "\n",
    "print(\"=== WORD ALIGNMENT (First 50 words) ===\")\n",
    "print()\n",
    "ref_words = ref_norm.split()[:50]\n",
    "hyp_words = hyp_norm.split()[:50]\n",
    "\n",
    "# Simple alignment display\n",
    "for i, (r, h) in enumerate(zip(ref_words, hyp_words)):\n",
    "    if r == h:\n",
    "        status = \"✓\"\n",
    "    else:\n",
    "        status = \"✗ SUB\"\n",
    "    print(f\"{i:3d}: REF='{r:15s}' HYP='{h:15s}' {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of WER's problems:\n",
    "\n",
    "# Problem 1: Equal weighting\n",
    "print(\"=== PROBLEM 1: All errors weighted equally ===\")\n",
    "print()\n",
    "print(\"Example A: Meaning-preserving error\")\n",
    "print(\"  REF: 'I am processing the data'\")\n",
    "print(\"  HYP: 'I am procesing the data'\")\n",
    "print(\"  → 1 substitution, WER = 0.2\")\n",
    "print(\"  → BUT: Meaning unchanged (typo)\")\n",
    "print()\n",
    "print(\"Example B: Meaning-changing error\")  \n",
    "print(\"  REF: 'I have a car'\")\n",
    "print(\"  HYP: 'I have a scar'\")\n",
    "print(\"  → 1 substitution, WER = 0.2\")\n",
    "print(\"  → BUT: Meaning completely changed!\")\n",
    "print()\n",
    "print(\"WER treats these the same!\")\n",
    "\n",
    "# Problem 2: Contractions\n",
    "print(\"\\n=== PROBLEM 2: Contractions (from our data) ===\")\n",
    "print(\"  REF: 'we're going'\")\n",
    "print(\"  HYP: 'were going'\")\n",
    "print(\"  → WER counts as 1 substitution\")\n",
    "print(\"  → BUT: Almost identical meaning\")\n",
    "print(\"  → Is this really an error?\")\n",
    "\n",
    "# Problem 3: No upper bound\n",
    "print(\"\\n=== PROBLEM 3: WER can exceed 100% ===\")\n",
    "print(\"  When model hallucinates lots of insertions:\")\n",
    "print(\"  REF: 'hello'\")\n",
    "print(\"  HYP: 'hello my dear friend how are you today'\")\n",
    "print(\"  → WER = (0 + 0 + 7) / 1 = 7.0 (700%!)\")\n",
    "\n",
    "# Show CER calculation\n",
    "print(\"=== CHARACTER ERROR RATE (CER) ===\")\n",
    "print()\n",
    "print(\"For languages like Mandarin/Japanese, or when typos matter:\")\n",
    "print()\n",
    "print(\"REF: 'processing'\")\n",
    "print(\"HYP: 'procesing'\")\n",
    "print()\n",
    "print(\"WER: 1/1 = 1.0 (100% error)\")\n",
    "print(\"CER: 1/10 = 0.1 (10% error)\")\n",
    "print()\n",
    "print(\"Which better reflects the actual error severity?\")\n",
    "\n",
    "# Show MER\n",
    "print(\"\\n=== MATCH ERROR RATE (MER) ===\")\n",
    "print(\"MER = (S + D) / N  (doesn't count insertions in denominator)\")\n",
    "print(\"Bounded between 0 and 1\")\n",
    "print()\n",
    "print(\"Your file_id=0:\")\n",
    "print(f\"  WER = {eval_row['wer']:.3f}\")\n",
    "print(f\"  MER = ({eval_row['substitutions']} + {eval_row['deletions']}) / N\")\n",
    "print(\"  → Which is more interpretable?\")\n",
    "\n",
    "# What do the error types tell you?\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=== ERROR TYPE DISTRIBUTION (Your Data) ===\")\n",
    "print()\n",
    "\n",
    "# Calculate proportions\n",
    "total_errors = df_eval['substitutions'].sum() + df_eval['deletions'].sum() + df_eval['insertions'].sum()\n",
    "sub_pct = df_eval['substitutions'].sum() / total_errors * 100\n",
    "del_pct = df_eval['deletions'].sum() / total_errors * 100\n",
    "ins_pct = df_eval['insertions'].sum() / total_errors * 100\n",
    "\n",
    "print(f\"Substitutions: {sub_pct:.1f}%\")\n",
    "print(f\"Deletions: {del_pct:.1f}%\")\n",
    "print(f\"Insertions: {ins_pct:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(['Substitutions', 'Deletions', 'Insertions'], \n",
    "        [sub_pct, del_pct, ins_pct],\n",
    "        color=['#ff6b6b', '#4ecdc4', '#45b7d1'])\n",
    "plt.ylabel('Percentage of Total Errors')\n",
    "plt.title('Error Type Distribution in VHP Audio')\n",
    "plt.show()\n",
    "\n",
    "print(\"INTERPRETATION:\")\n",
    "if del_pct > 50:\n",
    "    print(\"  → High deletions: Model gave up on unclear audio\")\n",
    "    print(\"  → Suggests: Audio quality issues (bandwidth-limited?)\")\n",
    "elif sub_pct > 50:\n",
    "    print(\"  → High substitutions: Model heard something but got it wrong\")\n",
    "    print(\"  → Suggests: Acoustic confusion (similar sounding words)\")\n",
    "elif ins_pct > 30:\n",
    "    print(\"  → High insertions: Model hallucinating extra content\")\n",
    "    print(\"  → Suggests: Model uncertainty or language model bias\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
