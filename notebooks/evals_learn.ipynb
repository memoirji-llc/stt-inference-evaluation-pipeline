{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook is created to learn about evaluating stt results.\n",
    "# there are some inferences run on wandb, with evals being done which can be further explored here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_inf = pd.read_parquet(\"../outputs/vhp-whisper-azure-sample/inference_results.parquet\")\n",
    "df_inf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv(\"../outputs/vhp-whisper-azure-sample/evaluation_results.csv\")\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all scripts for the experiments are put under /scripts.\n",
    "# the evaluations bits are under evaluate.py\n",
    "# here we are explaining step by step of how we compare the inference results (hypothesis) against gt (df_gt in the code):\n",
    "# (assume args.parque is \"../data/veterans_history_project_resources.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "df_gt = pd.read_parquet(\"../data/veterans_history_project_resources.parquet\")\n",
    "\n",
    "# Pick one example to walk through (file_id=0)\n",
    "file_id = 0\n",
    "gt_row = df_gt.iloc[file_id]\n",
    "\n",
    "print(f\"=== GROUND TRUTH FOR FILE {file_id} ===\")\n",
    "print(f\"Collection: {gt_row['collection_number']}\")\n",
    "print()\n",
    "print(f\"Raw XML transcript (first 500 chars):\")\n",
    "print(gt_row['fulltext_file_str'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Show the cleaning process step-by-step\n",
    "def clean_raw_transcript_str(fulltext_file_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean raw XML transcript from VHP dataset.\n",
    "    This function is from scripts/evaluate.py\n",
    "    \"\"\"\n",
    "    if not fulltext_file_str or pd.isna(fulltext_file_str):\n",
    "        return \"\"\n",
    "    \n",
    "    l_transcript_lines = []\n",
    "    \n",
    "    # Step 1: Parse XML to extract speaker and text\n",
    "    soup = BeautifulSoup(fulltext_file_str, 'xml')\n",
    "    \n",
    "    for sp in soup.find_all('sp'):\n",
    "        try:\n",
    "            speaker = sp.find('speaker').get_text(strip=True)\n",
    "        except:\n",
    "            speaker = \"speaker_unknown\"\n",
    "        try:\n",
    "            spoken_text = sp.find('p').get_text(strip=True)\n",
    "        except:\n",
    "            spoken_text = \"\"\n",
    "        \n",
    "        l_transcript_lines.append(f\"<{speaker}>{spoken_text}</{speaker}> \")\n",
    "    \n",
    "    # Step 2: Merge lines\n",
    "    transcript_lines = ''.join(l_transcript_lines)\n",
    "    print(\"STEP 1 - After parsing XML:\")\n",
    "    print(transcript_lines[:300])\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Remove annotations: (), [], {}\n",
    "    transcript_lines_stripped = re.sub(r'\\([^)]*\\)', '', transcript_lines)\n",
    "    transcript_lines_stripped = re.sub(r'\\[[^]]*\\]', '', transcript_lines_stripped)\n",
    "    transcript_lines_stripped = re.sub(r'\\{[^}]*\\)\\}', '', transcript_lines_stripped)\n",
    "    print(\"STEP 2 - After removing annotations:\")\n",
    "    print(transcript_lines_stripped[:300])\n",
    "    print()\n",
    "    \n",
    "    # Step 4: Remove dashes and ellipsis\n",
    "    transcript_lines_stripped = re.sub(r'--+', '', transcript_lines_stripped)\n",
    "    transcript_lines_stripped = re.sub(r'\\.{2,}', '', transcript_lines_stripped)\n",
    "    \n",
    "    # Step 5: Clean whitespace\n",
    "    transcript_lines_stripped = re.sub(r'\\s+', ' ', transcript_lines_stripped).strip()\n",
    "    \n",
    "    # Step 6: Remove speaker tags\n",
    "    transcript_lines_stripped = re.sub(r'\\<[^>]*\\>', '', transcript_lines_stripped)\n",
    "    print(\"STEP 3 - After removing speaker tags:\")\n",
    "    print(transcript_lines_stripped[:300])\n",
    "    print()\n",
    "    \n",
    "    return transcript_lines_stripped\n",
    "\n",
    "# Apply cleaning\n",
    "reference = clean_raw_transcript_str(gt_row['fulltext_file_str'])\n",
    "print(\"=== FINAL CLEANED REFERENCE ===\")\n",
    "print(f\"Length: {len(reference)} characters\")\n",
    "print(f\"First 500 chars: {reference[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Get hypothesis and show normalization\n",
    "hypothesis = df_inf.iloc[file_id]['hypothesis']\n",
    "\n",
    "print(\"=== HYPOTHESIS (Model Output) ===\")\n",
    "print(f\"Length: {len(hypothesis)} characters\")\n",
    "print(f\"First 500 chars: {hypothesis[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Show normalization process\n",
    "import jiwer\n",
    "\n",
    "def normalize(s):\n",
    "    \"\"\"Normalize text for WER calculation (from scripts/evaluate.py)\"\"\"\n",
    "    tx = jiwer.Compose([\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemovePunctuation(),\n",
    "        jiwer.Strip(),\n",
    "        jiwer.RemoveMultipleSpaces()\n",
    "    ])\n",
    "    return tx(s)\n",
    "\n",
    "# Normalize both\n",
    "ref_norm = normalize(reference)\n",
    "hyp_norm = normalize(hypothesis)\n",
    "\n",
    "print(\"=== NORMALIZATION PROCESS ===\")\n",
    "print()\n",
    "print(\"REFERENCE BEFORE:\")\n",
    "print(reference[:200])\n",
    "print()\n",
    "print(\"REFERENCE AFTER:\")\n",
    "print(ref_norm[:200])\n",
    "print()\n",
    "print(\"HYPOTHESIS BEFORE:\")\n",
    "print(hypothesis[:200])\n",
    "print()\n",
    "print(\"HYPOTHESIS AFTER:\")\n",
    "print(hyp_norm[:200])\n",
    "print()\n",
    "print(f\"Why normalize? Removes case, punctuation, extra spaces so we compare WORDS only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Calculate WER step-by-step\n",
    "m = jiwer.process_words(ref_norm, hyp_norm)\n",
    "\n",
    "print(\"=== WER CALCULATION ===\")\n",
    "print()\n",
    "print(f\"Reference words: {len(ref_norm.split())}\")\n",
    "print(f\"Hypothesis words: {len(hyp_norm.split())}\")\n",
    "print()\n",
    "print(f\"Substitutions: {m.substitutions} (wrong word)\")\n",
    "print(f\"Deletions: {m.deletions} (model missed word)\")\n",
    "print(f\"Insertions: {m.insertions} (model added extra word)\")\n",
    "print()\n",
    "print(f\"Total errors: {m.substitutions + m.deletions + m.insertions}\")\n",
    "print(f\"WER = Total errors / Reference words\")\n",
    "print(f\"WER = {m.substitutions + m.deletions + m.insertions} / {len(ref_norm.split())}\")\n",
    "print(f\"WER = {m.wer:.3f} ({m.wer*100:.1f}%)\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "if m.wer < 0.1:\n",
    "    print(\"  → Excellent (< 10% error)\")\n",
    "elif m.wer < 0.3:\n",
    "    print(\"  → Good (10-30% error)\")\n",
    "elif m.wer < 0.5:\n",
    "    print(\"  → Fair (30-50% error)\")\n",
    "else:\n",
    "    print(\"  → Poor (> 50% error)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Show alignment visualization (which words are wrong)\n",
    "alignment = jiwer.process_words(ref_norm, hyp_norm)\n",
    "\n",
    "print(\"=== WORD ALIGNMENT (First 50 words) ===\")\n",
    "print()\n",
    "ref_words = ref_norm.split()[:50]\n",
    "hyp_words = hyp_norm.split()[:50]\n",
    "\n",
    "# Simple alignment display\n",
    "for i, (r, h) in enumerate(zip(ref_words, hyp_words)):\n",
    "    if r == h:\n",
    "        status = \"✓\"\n",
    "    else:\n",
    "        status = \"✗ SUB\"\n",
    "    print(f\"{i:3d}: REF='{r:15s}' HYP='{h:15s}' {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Compare with evaluation results\n",
    "eval_row = df_eval.iloc[file_id]\n",
    "\n",
    "print(\"=== VERIFICATION ===\")\n",
    "print()\n",
    "print(\"Our calculation:\")\n",
    "print(f\"  WER: {m.wer:.3f}\")\n",
    "print(f\"  Substitutions: {m.substitutions}\")\n",
    "print(f\"  Deletions: {m.deletions}\")\n",
    "print(f\"  Insertions: {m.insertions}\")\n",
    "print()\n",
    "print(\"evaluate.py results:\")\n",
    "print(f\"  WER: {eval_row['wer']:.3f}\")\n",
    "print(f\"  Substitutions: {eval_row['substitutions']}\")\n",
    "print(f\"  Deletions: {eval_row['deletions']}\")\n",
    "print(f\"  Insertions: {eval_row['insertions']}\")\n",
    "print()\n",
    "print(f\"Match: {abs(m.wer - eval_row['wer']) < 0.001}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
