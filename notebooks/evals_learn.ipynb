{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook is created to learn about evaluating stt results.\n",
    "# there are some inferences run on wandb, with evals being done which can be further explored here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f8649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = \"vhp-pre2010-distil-large-v3-full-gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>collection_number</th>\n",
       "      <th>blob_path</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>processing_time_sec</th>\n",
       "      <th>language</th>\n",
       "      <th>status</th>\n",
       "      <th>error_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AFC/2001/001/44003</td>\n",
       "      <td>loc_vhp/0/video.mp4</td>\n",
       "      <td>Okay, today is October 29th, 2004. We're with ...</td>\n",
       "      <td>1986.815437</td>\n",
       "      <td>32.305685</td>\n",
       "      <td>en</td>\n",
       "      <td>success</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AFC/2001/001/51925</td>\n",
       "      <td>loc_vhp/1/audio.mp3</td>\n",
       "      <td>I'm speaking with John Aaron Jr. John Aaron Jr...</td>\n",
       "      <td>618.144000</td>\n",
       "      <td>10.379629</td>\n",
       "      <td>en</td>\n",
       "      <td>success</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AFC/2001/001/32482</td>\n",
       "      <td>loc_vhp/2/video.mp4</td>\n",
       "      <td>First thing I'm going to ask is your name. Alo...</td>\n",
       "      <td>2880.365750</td>\n",
       "      <td>52.445759</td>\n",
       "      <td>en</td>\n",
       "      <td>success</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AFC/2001/001/41825</td>\n",
       "      <td>loc_vhp/3/audio.mp3</td>\n",
       "      <td>In 2012, 2005, I am Brent Wains, and I'm inter...</td>\n",
       "      <td>1023.024000</td>\n",
       "      <td>17.514904</td>\n",
       "      <td>en</td>\n",
       "      <td>success</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AFC/2001/001/18519</td>\n",
       "      <td>loc_vhp/4/audio.mp3</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "      <td>failed: FileNotFoundError</td>\n",
       "      <td>None of the candidate paths exist: ['loc_vhp/4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_id   collection_number            blob_path  \\\n",
       "0        0  AFC/2001/001/44003  loc_vhp/0/video.mp4   \n",
       "1        1  AFC/2001/001/51925  loc_vhp/1/audio.mp3   \n",
       "2        2  AFC/2001/001/32482  loc_vhp/2/video.mp4   \n",
       "3        3  AFC/2001/001/41825  loc_vhp/3/audio.mp3   \n",
       "4        4  AFC/2001/001/18519  loc_vhp/4/audio.mp3   \n",
       "\n",
       "                                          hypothesis  duration_sec  \\\n",
       "0  Okay, today is October 29th, 2004. We're with ...   1986.815437   \n",
       "1  I'm speaking with John Aaron Jr. John Aaron Jr...    618.144000   \n",
       "2  First thing I'm going to ask is your name. Alo...   2880.365750   \n",
       "3  In 2012, 2005, I am Brent Wains, and I'm inter...   1023.024000   \n",
       "4                                                         0.000000   \n",
       "\n",
       "   processing_time_sec language                     status  \\\n",
       "0            32.305685       en                    success   \n",
       "1            10.379629       en                    success   \n",
       "2            52.445759       en                    success   \n",
       "3            17.514904       en                    success   \n",
       "4             0.000000           failed: FileNotFoundError   \n",
       "\n",
       "                                       error_message  \n",
       "0                                               None  \n",
       "1                                               None  \n",
       "2                                               None  \n",
       "3                                               None  \n",
       "4  None of the candidate paths exist: ['loc_vhp/4...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_inf = pd.read_parquet(f\"../outputs/{experiment_id}/inference_results.parquet\")\n",
    "df_inf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>collection_number</th>\n",
       "      <th>wer</th>\n",
       "      <th>substitutions</th>\n",
       "      <th>deletions</th>\n",
       "      <th>insertions</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>processing_time_sec</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AFC/2001/001/44003</td>\n",
       "      <td>0.111906</td>\n",
       "      <td>163.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1986.815437</td>\n",
       "      <td>32.305685</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AFC/2001/001/51925</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>60.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>618.144000</td>\n",
       "      <td>10.379629</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AFC/2001/001/32482</td>\n",
       "      <td>0.051098</td>\n",
       "      <td>148.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2880.365750</td>\n",
       "      <td>52.445759</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AFC/2001/001/41825</td>\n",
       "      <td>0.121491</td>\n",
       "      <td>132.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1023.024000</td>\n",
       "      <td>17.514904</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AFC/2001/001/18519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>failed: FileNotFoundError</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_id   collection_number       wer  substitutions  deletions  \\\n",
       "0        0  AFC/2001/001/44003  0.111906          163.0      234.0   \n",
       "1        1  AFC/2001/001/51925  0.147541           60.0       58.0   \n",
       "2        2  AFC/2001/001/32482  0.051098          148.0      115.0   \n",
       "3        3  AFC/2001/001/41825  0.121491          132.0       85.0   \n",
       "4        4  AFC/2001/001/18519       NaN            NaN        NaN   \n",
       "\n",
       "   insertions  duration_sec  processing_time_sec                     status  \n",
       "0        41.0   1986.815437            32.305685                    success  \n",
       "1        17.0    618.144000            10.379629                    success  \n",
       "2        72.0   2880.365750            52.445759                    success  \n",
       "3        34.0   1023.024000            17.514904                    success  \n",
       "4         NaN      0.000000             0.000000  failed: FileNotFoundError  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval = pd.read_csv(f\"../outputs/{experiment_id}/evaluation_results.csv\")\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all scripts for the experiments are put under /scripts.\n",
    "# the evaluations bits are under evaluate.py\n",
    "# here we are explaining step by step of how we compare the inference results (hypothesis) against gt (df_gt in the code):\n",
    "# (assume args.parque is \"../data/veterans_history_project_resources.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GROUND TRUTH FOR FILE 0 ===\n",
      "Collection: AFC/2001/001/44003\n",
      "\n",
      "Raw XML transcript (first 500 chars):\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<!DOCTYPE TEI.2\n",
      "  PUBLIC \"-//TEI//DTD TEI Lite XML ver. 1//EN\" \"http://lcweb2.loc.gov/natlib/schemas/teixlite.dtd\">\n",
      "<TEI.2>\n",
      "   <teiHeader>\n",
      "      <fileDesc>\n",
      "         <titleStmt>\n",
      "            <title>Interview with Thorvald Aanden, 10/29/2004</title>\n",
      "            <author>Carole Larson and Christine Champeau [transcriber]</author>\n",
      "            <respStmt>\n",
      "               <resp>Encoded by</resp>\n",
      "               <name>Matthew McCrady</name>\n",
      "            </respStmt>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "df_gt = pd.read_parquet(\"../data/raw/loc/veterans_history_project_resources_pre2010.parquet\")\n",
    "\n",
    "# Pick one example to walk through (file_id=0)\n",
    "file_id = 0\n",
    "gt_row = df_gt.iloc[file_id]\n",
    "\n",
    "print(f\"=== GROUND TRUTH FOR FILE {file_id} ===\")\n",
    "print(f\"Collection: {gt_row['collection_number']}\")\n",
    "print()\n",
    "print(f\"Raw XML transcript (first 500 chars):\")\n",
    "print(gt_row['fulltext_file_str'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1 - After parsing XML:\n",
      "<Carole Larson>Okay. Today is October 29th, 2004. We're with Thorvald Aanden from Fertile. Where were you born and raised?</Carole Larson> <Thorvald Aanden>Oh, I was born in South Dakota.</Thorvald Aanden> <Carole Larson>And you were raised there too?</Carole Larson> <Thorvald Aanden>Pardon? No, no.\n",
      "\n",
      "STEP 2 - After removing annotations:\n",
      "<Carole Larson>Okay. Today is October 29th, 2004. We're with Thorvald Aanden from Fertile. Where were you born and raised?</Carole Larson> <Thorvald Aanden>Oh, I was born in South Dakota.</Thorvald Aanden> <Carole Larson>And you were raised there too?</Carole Larson> <Thorvald Aanden>Pardon? No, no.\n",
      "\n",
      "STEP 3 - After removing speaker tags:\n",
      "Okay. Today is October 29th, 2004. We're with Thorvald Aanden from Fertile. Where were you born and raised? Oh, I was born in South Dakota. And you were raised there too? Pardon? No, no. You grew up there? No. I my folks moved back to to Underwood. We I had to work. Okay. And what were your parents'\n",
      "\n",
      "=== FINAL CLEANED REFERENCE ===\n",
      "Length: 20185 characters\n",
      "First 500 chars: Okay. Today is October 29th, 2004. We're with Thorvald Aanden from Fertile. Where were you born and raised? Oh, I was born in South Dakota. And you were raised there too? Pardon? No, no. You grew up there? No. I my folks moved back to to Underwood. We I had to work. Okay. And what were your parents' names? Henry and Tillie Aanden. And did you have brothers and sisters? Yeah. I had eight of them. Wow. I was the oldest. Oh, okay. And but three three of them are already dead. Okay. Do you remember \n"
     ]
    }
   ],
   "source": [
    "# Show the cleaning process step-by-step\n",
    "def clean_raw_transcript_str(fulltext_file_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean raw XML transcript from VHP dataset.\n",
    "    This function is from scripts/evaluate.py\n",
    "    \"\"\"\n",
    "    if not fulltext_file_str or pd.isna(fulltext_file_str):\n",
    "        return \"\"\n",
    "    \n",
    "    l_transcript_lines = []\n",
    "    \n",
    "    # Step 1: Parse XML to extract speaker and text\n",
    "    soup = BeautifulSoup(fulltext_file_str, 'xml')\n",
    "    \n",
    "    for sp in soup.find_all('sp'):\n",
    "        try:\n",
    "            speaker = sp.find('speaker').get_text(strip=True)\n",
    "        except:\n",
    "            speaker = \"speaker_unknown\"\n",
    "        try:\n",
    "            spoken_text = sp.find('p').get_text(strip=True)\n",
    "        except:\n",
    "            spoken_text = \"\"\n",
    "        \n",
    "        l_transcript_lines.append(f\"<{speaker}>{spoken_text}</{speaker}> \")\n",
    "    \n",
    "    # Step 2: Merge lines\n",
    "    transcript_lines = ''.join(l_transcript_lines)\n",
    "    print(\"STEP 1 - After parsing XML:\")\n",
    "    print(transcript_lines[:300])\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Remove annotations: (), [], {}\n",
    "    transcript_lines_stripped = re.sub(r'\\([^)]*\\)', '', transcript_lines)\n",
    "    transcript_lines_stripped = re.sub(r'\\[[^]]*\\]', '', transcript_lines_stripped)\n",
    "    transcript_lines_stripped = re.sub(r'\\{[^}]*\\)\\}', '', transcript_lines_stripped)\n",
    "    print(\"STEP 2 - After removing annotations:\")\n",
    "    print(transcript_lines_stripped[:300])\n",
    "    print()\n",
    "    \n",
    "    # Step 4: Remove dashes and ellipsis\n",
    "    transcript_lines_stripped = re.sub(r'--+', '', transcript_lines_stripped)\n",
    "    transcript_lines_stripped = re.sub(r'\\.{2,}', '', transcript_lines_stripped)\n",
    "    \n",
    "    # Step 5: Clean whitespace\n",
    "    transcript_lines_stripped = re.sub(r'\\s+', ' ', transcript_lines_stripped).strip()\n",
    "    \n",
    "    # Step 6: Remove speaker tags\n",
    "    transcript_lines_stripped = re.sub(r'\\<[^>]*\\>', '', transcript_lines_stripped)\n",
    "    print(\"STEP 3 - After removing speaker tags:\")\n",
    "    print(transcript_lines_stripped[:300])\n",
    "    print()\n",
    "    \n",
    "    return transcript_lines_stripped\n",
    "\n",
    "# Apply cleaning\n",
    "reference = clean_raw_transcript_str(gt_row['fulltext_file_str'])\n",
    "print(\"=== FINAL CLEANED REFERENCE ===\")\n",
    "print(f\"Length: {len(reference)} characters\")\n",
    "print(f\"First 500 chars: {reference[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HYPOTHESIS (Model Output) ===\n",
      "Length: 19260 characters\n",
      "First 500 chars: Okay, today is October 29th, 2004. We're with Thorvald Andon from Furtle. Where were you born and raised? Oh, I was born in South Dakota. And you were raised there too? Pardon? No. You grew up there? No, I, the folks moved back to, uh, go to Underwood. My head don't work. Okay. And what were your parents' names? Henry and Tilly And did you have brothers and sisters? Yeah, I had eight of them. Wow. I was the oldest. Oh, okay. And what, uh, three of them are already dead. Okay. Do you remember Pea\n"
     ]
    }
   ],
   "source": [
    "# Get hypothesis and show normalization\n",
    "hypothesis = df_inf.iloc[file_id]['hypothesis']\n",
    "\n",
    "print(\"=== HYPOTHESIS (Model Output) ===\")\n",
    "print(f\"Length: {len(hypothesis)} characters\")\n",
    "print(f\"First 500 chars: {hypothesis[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NORMALIZATION PROCESS ===\n",
      "\n",
      "REFERENCE BEFORE:\n",
      "Okay. Today is October 29th, 2004. We're with Thorvald Aanden from Fertile. Where were you born and raised? Oh, I was born in South Dakota. And you were raised there too? Pardon? No, no. You grew up t\n",
      "\n",
      "REFERENCE AFTER:\n",
      "okay today is october 29th 2004 were with thorvald aanden from fertile where were you born and raised oh i was born in south dakota and you were raised there too pardon no no you grew up there no i my\n",
      "\n",
      "HYPOTHESIS BEFORE:\n",
      "Okay, today is October 29th, 2004. We're with Thorvald Andon from Furtle. Where were you born and raised? Oh, I was born in South Dakota. And you were raised there too? Pardon? No. You grew up there? \n",
      "\n",
      "HYPOTHESIS AFTER:\n",
      "okay today is october 29th 2004 were with thorvald andon from furtle where were you born and raised oh i was born in south dakota and you were raised there too pardon no you grew up there no i the fol\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show normalization process\n",
    "import jiwer\n",
    "\n",
    "def normalize(s):\n",
    "    \"\"\"Normalize text for WER calculation (from scripts/evaluate.py)\"\"\"\n",
    "    tx = jiwer.Compose([\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemovePunctuation(),\n",
    "        jiwer.Strip(),\n",
    "        jiwer.RemoveMultipleSpaces()\n",
    "    ])\n",
    "    return tx(s)\n",
    "\n",
    "# Normalize both gt and predictions (gt was cleaned already in production setting)\n",
    "ref_norm = normalize(reference)\n",
    "hyp_norm = normalize(hypothesis)\n",
    "\n",
    "print(\"=== NORMALIZATION PROCESS ===\")\n",
    "print()\n",
    "print(\"REFERENCE BEFORE:\")\n",
    "print(reference[:200])\n",
    "print()\n",
    "print(\"REFERENCE AFTER:\")\n",
    "print(ref_norm[:200])\n",
    "print()\n",
    "print(\"HYPOTHESIS BEFORE:\")\n",
    "print(hypothesis[:200])\n",
    "print()\n",
    "print(\"HYPOTHESIS AFTER:\")\n",
    "print(hyp_norm[:200])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WER CALCULATION WITH jiwer===\n",
      "\n",
      "Reference words: 3914\n",
      "Hypothesis words: 3721\n",
      "\n",
      "Substitutions: 163 (wrong word)\n",
      "Deletions: 234 (model missed word)\n",
      "Insertions: 41 (model added extra word)\n",
      "\n",
      "Total errors: 438\n",
      "WER = Total errors / Reference words\n",
      "WER = 438 / 3914\n",
      "WER = 0.112 (11.2%)\n",
      "\n",
      "Interpretation:\n",
      "  → Good (10-30% error)\n"
     ]
    }
   ],
   "source": [
    "# Calculate WER step-by-step\n",
    "m = jiwer.process_words(ref_norm, hyp_norm)\n",
    "\n",
    "print(\"=== WER CALCULATION WITH jiwer(from scripts/evaluate.py)===\")\n",
    "print()\n",
    "print(f\"Reference words: {len(ref_norm.split())}\")\n",
    "print(f\"Hypothesis words: {len(hyp_norm.split())}\")\n",
    "print()\n",
    "print(f\"Substitutions: {m.substitutions} (wrong word)\")\n",
    "print(f\"Deletions: {m.deletions} (model missed word)\")\n",
    "print(f\"Insertions: {m.insertions} (model added extra word)\")\n",
    "print()\n",
    "print(f\"Total errors: {m.substitutions + m.deletions + m.insertions}\")\n",
    "print(f\"WER = Total errors / Reference words\")\n",
    "print(f\"WER = {m.substitutions + m.deletions + m.insertions} / {len(ref_norm.split())}\")\n",
    "print(f\"WER = {m.wer:.3f} ({m.wer*100:.1f}%)\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "if m.wer < 0.1:\n",
    "    print(\"  → Excellent (< 10% error)\")\n",
    "elif m.wer < 0.3:\n",
    "    print(\"  → Good (10-30% error)\")\n",
    "elif m.wer < 0.5:\n",
    "    print(\"  → Fair (30-50% error)\")\n",
    "else:\n",
    "    print(\"  → Poor (> 50% error)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WORD ALIGNMENT (First 50 words) ===\n",
      "\n",
      "  0: REF='okay           ' HYP='okay           ' ✓\n",
      "  1: REF='today          ' HYP='today          ' ✓\n",
      "  2: REF='is             ' HYP='is             ' ✓\n",
      "  3: REF='october        ' HYP='october        ' ✓\n",
      "  4: REF='29th           ' HYP='29th           ' ✓\n",
      "  5: REF='2004           ' HYP='2004           ' ✓\n",
      "  6: REF='were           ' HYP='were           ' ✓\n",
      "  7: REF='with           ' HYP='with           ' ✓\n",
      "  8: REF='thorvald       ' HYP='thorvald       ' ✓\n",
      "  9: REF='aanden         ' HYP='andon          ' ✗ SUB\n",
      " 10: REF='from           ' HYP='from           ' ✓\n",
      " 11: REF='fertile        ' HYP='furtle         ' ✗ SUB\n",
      " 12: REF='where          ' HYP='where          ' ✓\n",
      " 13: REF='were           ' HYP='were           ' ✓\n",
      " 14: REF='you            ' HYP='you            ' ✓\n",
      " 15: REF='born           ' HYP='born           ' ✓\n",
      " 16: REF='and            ' HYP='and            ' ✓\n",
      " 17: REF='raised         ' HYP='raised         ' ✓\n",
      " 18: REF='oh             ' HYP='oh             ' ✓\n",
      " 19: REF='i              ' HYP='i              ' ✓\n",
      " 20: REF='was            ' HYP='was            ' ✓\n",
      " 21: REF='born           ' HYP='born           ' ✓\n",
      " 22: REF='in             ' HYP='in             ' ✓\n",
      " 23: REF='south          ' HYP='south          ' ✓\n",
      " 24: REF='dakota         ' HYP='dakota         ' ✓\n",
      " 25: REF='and            ' HYP='and            ' ✓\n",
      " 26: REF='you            ' HYP='you            ' ✓\n",
      " 27: REF='were           ' HYP='were           ' ✓\n",
      " 28: REF='raised         ' HYP='raised         ' ✓\n",
      " 29: REF='there          ' HYP='there          ' ✓\n",
      " 30: REF='too            ' HYP='too            ' ✓\n",
      " 31: REF='pardon         ' HYP='pardon         ' ✓\n",
      " 32: REF='no             ' HYP='no             ' ✓\n",
      " 33: REF='no             ' HYP='you            ' ✗ SUB\n",
      " 34: REF='you            ' HYP='grew           ' ✗ SUB\n",
      " 35: REF='grew           ' HYP='up             ' ✗ SUB\n",
      " 36: REF='up             ' HYP='there          ' ✗ SUB\n",
      " 37: REF='there          ' HYP='no             ' ✗ SUB\n",
      " 38: REF='no             ' HYP='i              ' ✗ SUB\n",
      " 39: REF='i              ' HYP='the            ' ✗ SUB\n",
      " 40: REF='my             ' HYP='folks          ' ✗ SUB\n",
      " 41: REF='folks          ' HYP='moved          ' ✗ SUB\n",
      " 42: REF='moved          ' HYP='back           ' ✗ SUB\n",
      " 43: REF='back           ' HYP='to             ' ✗ SUB\n",
      " 44: REF='to             ' HYP='uh             ' ✗ SUB\n",
      " 45: REF='to             ' HYP='go             ' ✗ SUB\n",
      " 46: REF='underwood      ' HYP='to             ' ✗ SUB\n",
      " 47: REF='we             ' HYP='underwood      ' ✗ SUB\n",
      " 48: REF='i              ' HYP='my             ' ✗ SUB\n",
      " 49: REF='had            ' HYP='head           ' ✗ SUB\n"
     ]
    }
   ],
   "source": [
    "# Visualize alignment on word-to-word level\n",
    "alignment = jiwer.process_words(ref_norm, hyp_norm)\n",
    "\n",
    "print(\"=== WORD ALIGNMENT (First 50 words) ===\")\n",
    "print()\n",
    "ref_words = ref_norm.split()[:50]\n",
    "hyp_words = hyp_norm.split()[:50]\n",
    "\n",
    "# Simple alignment display\n",
    "for i, (r, h) in enumerate(zip(ref_words, hyp_words)):\n",
    "    if r == h:\n",
    "        status = \"✓\"\n",
    "    else:\n",
    "        status = \"✗ SUB\"\n",
    "    print(f\"{i:3d}: REF='{r:15s}' HYP='{h:15s}' {status}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
