{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veterans history project dataset\n",
    "import time\n",
    "import requests\n",
    "# import error type\n",
    "from requests.exceptions import RequestException\n",
    "# chucked encoding error\n",
    "from requests.exceptions import ChunkedEncodingError\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "# utilities\n",
    "from helpers_loc import get_file_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for rate limiting and paging\n",
    "RATE_LIMIT_DELAY = 3  # Delay in seconds between requests (20 requests per minute)\n",
    "MAX_ITEMS_PER_PAGE = 1000  # Recommended maximum items per page\n",
    "MAX_TOTAL_ITEMS = 100000  # Maximum items per query !! Set low for testing !!\n",
    "RETRY_DELAY = 1  # Initial delay in seconds for retries\n",
    "MAX_RETRIES = 5  # Maximum number of retries for rate-limited requests\n",
    "DATA_URL_BASE = 'https://www.loc.gov/collections/veterans-history-project-collection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse the returned JSON\n",
    "def fetch_url_with_rate_limit(rep_url, timeout=60):\n",
    "    retries = 0\n",
    "    # Retry logic with exponential backoff for 429 status codes\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            print(f\"Attempting to fetch URL: {rep_url}\")\n",
    "            response = requests.get(rep_url, timeout=timeout)\n",
    "            if response.status_code == 429:  # Too Many Requests\n",
    "                print(\"Rate limit exceeded. Pausing...\")\n",
    "                time.sleep(RETRY_DELAY * (2 ** retries))  # Exponential backoff\n",
    "                retries += 1\n",
    "                continue\n",
    "            response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as req_err:\n",
    "            print(f\"Request error occurred: {req_err}\")\n",
    "            retries += 1\n",
    "            time.sleep(RETRY_DELAY * (2 ** retries))  # Exponential backoff\n",
    "    raise Exception(f\"Failed to fetch {rep_url} after {MAX_RETRIES} retries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding fo=json can return web representation: e.g. https://www.loc.gov/collections/veterans-history-project-collection?fo=json\n",
    "web_rep_url = f'{DATA_URL_BASE}?fo=json&fa=online-format:online+text'\n",
    "print(f\"web representation url: {web_rep_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store all results\n",
    "all_results = []\n",
    "# Loop through pages with rate limiting and paging limits\n",
    "total_items_processed = 0\n",
    "# Start with web_rep_url\n",
    "current_url = web_rep_url\n",
    "print(\"Start fetching information from each url:\")\n",
    "while current_url and total_items_processed < MAX_TOTAL_ITEMS:\n",
    "    try:\n",
    "        response = fetch_url_with_rate_limit(current_url)\n",
    "        data = response.json()\n",
    "        # Process the results on the current page\n",
    "        results = data.get('results', [])\n",
    "        all_results.extend(results)\n",
    "        total_items_processed += len(results)\n",
    "        print(f\"Processed {len(results)} results. Total: {total_items_processed}\")\n",
    "\n",
    "        # Get the URL for the next page\n",
    "        current_url = data.get('pagination', {}).get('next')\n",
    "        print(f\"Next page URL: {current_url}\")\n",
    "\n",
    "        # Delay to respect rate limits\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing page {current_url}: {e}\")\n",
    "        break\n",
    "\n",
    "# Convert the accumulated results into a DataFrame\n",
    "if all_results:\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    print(\"DataFrame created successfully.\")\n",
    "else:\n",
    "    print(\"No results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Loaded web representation with {len(data):,} entries.')\n",
    "len(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df as parquet file\n",
    "df_results.to_parquet('../data/raw/loc/veterans_history_project.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the df_results DataFrame from the parquet file\n",
    "df_results = pd.read_parquet('../data/veterans_history_project.parquet')\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each column print the first row data\n",
    "for col in df_results.columns:\n",
    "    print(f'{col}: {df_results[col].iloc[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_resource = []\n",
    "for n in range(len(df_results)):\n",
    "    collection_number = df_results['item'][n]['collection_number']\n",
    "    title = df_results['title'][n]\n",
    "    description = df_results['description'][n]\n",
    "    dates = df_results['dates'][n]\n",
    "    language = df_results['language'][n]\n",
    "    location = df_results['location'][n]\n",
    "    location_home = df_results['location_home'][n]\n",
    "    location_service = df_results['location_service'][n]\n",
    "    partof = df_results['partof'][n]\n",
    "    subject = df_results['subject'][n]\n",
    "    subject_battles = df_results['subject_battles'][n]\n",
    "    subject_branch = df_results['subject_branch'][n]\n",
    "    subject_conflict = df_results['subject_conflict'][n]\n",
    "    subject_entrance = df_results['subject_entrance'][n]\n",
    "    subject_format = df_results['subject_format'][n]\n",
    "    subject_gender = df_results['subject_gender'][n]\n",
    "    subject_rank = df_results['subject_rank'][n]\n",
    "    subject_status = df_results['subject_status'][n]\n",
    "    subject_unit = df_results['subject_unit'][n]\n",
    "    subject_race = df_results['subject_race'][n]\n",
    "    collection_resource = []\n",
    "\n",
    "    for resource in df_results['resources'][n]:\n",
    "        try:\n",
    "            fulltext_file_url = resource.get('fulltext_file', None)\n",
    "            retries = 0\n",
    "            while retries < MAX_RETRIES:\n",
    "                try:\n",
    "                    if fulltext_file_url:\n",
    "                        print(f\"Fetching fulltext file: {fulltext_file_url}\")\n",
    "                        response = requests.get(fulltext_file_url, timeout=60)\n",
    "                        if response.status_code == 429:  # Too Many Requests\n",
    "                            print(\"Rate limit exceeded. Pausing...\")\n",
    "                            time.sleep(RETRY_DELAY * (2 ** retries))  # Exponential backoff\n",
    "                            retries += 1\n",
    "                            continue\n",
    "                        response.raise_for_status()\n",
    "                        fulltext_file_str = response.text\n",
    "                        # print(f\"Successfully fetched fulltext file: {fulltext_file_str}\")\n",
    "                    else:\n",
    "                        fulltext_file_str = None\n",
    "                    break\n",
    "                except requests.exceptions.RequestException as req_err:\n",
    "                    print(f\"Request error occurred: {req_err}\")\n",
    "                    retries += 1\n",
    "                    time.sleep(RETRY_DELAY * (2 ** retries))  # Exponential backoff\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching fulltext file: {e}\")\n",
    "                    fulltext_file_str = None\n",
    "                    break\n",
    "            video_url = resource.get('video', None)\n",
    "            audio_url = resource.get('audio', None)\n",
    "        except Exception as e:\n",
    "            fulltext_file_url = None\n",
    "            fulltext_file_str = None\n",
    "            video_url = None\n",
    "            audio_url = None\n",
    "        finally:\n",
    "            collection_resource.append({\n",
    "            'collection_number': collection_number,\n",
    "            'fulltext_file_url': fulltext_file_url,\n",
    "            'fulltext_file_str': fulltext_file_str,\n",
    "            'video_url': video_url,\n",
    "            'audio_url': audio_url,\n",
    "            'title': title,\n",
    "            'description': description,\n",
    "            'dates': dates,\n",
    "            'language': language,\n",
    "            'location': location,\n",
    "            'location_home': location_home,\n",
    "            'location_service': location_service,\n",
    "            'partof': partof,\n",
    "            'subject': subject,\n",
    "            'subject_battles': subject_battles,\n",
    "            'subject_branch': subject_branch,\n",
    "            'subject_conflict': subject_conflict,\n",
    "            'subject_entrance': subject_entrance,\n",
    "            'subject_format': subject_format,\n",
    "            'subject_gender': subject_gender,\n",
    "            'subject_rank': subject_rank,\n",
    "            'subject_status': subject_status,\n",
    "            'subject_unit': subject_unit,\n",
    "            'subject_race': subject_race       \n",
    "            })\n",
    "    l_resource.append(collection_resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the list of resources into a DataFrame\n",
    "df_resources = pd.DataFrame([item for sublist in l_resource for item in sublist])\n",
    "df_resources.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_raw_transcript_str(fulltext_file_str: str) -> str:\n",
    "    l_transcript_lines = []\n",
    "    # utilize bs4 xml parser\n",
    "    soup = BeautifulSoup(fulltext_file_str, 'xml')\n",
    "    # each sp tag in the document represents a \"line\" in the transcript\n",
    "    for sp in soup.find_all('sp'):\n",
    "        \n",
    "        try:\n",
    "            speaker = sp.find('speaker').get_text(strip=True)\n",
    "        \n",
    "        except:\n",
    "            # placeholder speaker tag if not found\n",
    "            speaker = \"speaker_unknown\"\n",
    "        try:\n",
    "            # return empty text if p tag not found\n",
    "            spoken_text = sp.find('p').get_text(strip=True)\n",
    "        \n",
    "        except:\n",
    "            spoken_text = \"\"\n",
    "        \n",
    "        l_transcript_lines.append(f\"<{speaker}>{spoken_text}</{speaker}> \")\n",
    "    \n",
    "    # merge lines into one string\n",
    "    transcript_lines = ''.join(l_transcript_lines)\n",
    "    \n",
    "    # remove (), [], {} and anything in between\n",
    "    transcript_lines_stripped = re.sub(r'\\([^)]*\\)', '', transcript_lines)\n",
    "    transcript_lines_stripped = re.sub(r'\\[[^]]*\\]', '', transcript_lines_stripped)\n",
    "    transcript_lines_stripped = re.sub(r'\\{[^}]*\\)\\}', '', transcript_lines_stripped)\n",
    "\n",
    "    # remove double dashes and ellipsis\n",
    "    transcript_lines_stripped = re.sub(r'--+', '', transcript_lines_stripped)\n",
    "    transcript_lines_stripped = re.sub(r'\\.{2,}', '', transcript_lines_stripped)\n",
    "\n",
    "    # clean whitespace\n",
    "    transcript_lines_stripped = re.sub(r'\\s+', ' ', transcript_lines_stripped).strip()\n",
    "    \n",
    "    return transcript_lines_stripped\n",
    "\n",
    "def remove_speaker_tag(transcript_lines_stripped: str) -> str:\n",
    "    # remove <> and anything in between\n",
    "    try:\n",
    "        return re.sub(r'\\<[^>]*\\>', '', transcript_lines_stripped)\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "print(\"Extracting raw transcripts from dataframe\")\n",
    "df_resources['fulltext_file_str_cleaned'] = df_resources['fulltext_file_str'].astype(str).apply(clean_raw_transcript_str)\n",
    "df_resources['transcript_raw_text_only'] = df_resources['fulltext_file_str_cleaned'].apply(remove_speaker_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame to a parquet file\n",
    "df_resources.to_parquet('../data/raw/loc/veterans_history_project_resources.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve resource DataFrame from the parquet file\n",
    "df_resources = pd.read_parquet('../data/raw/loc/veterans_history_project_resources.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download audio/ video files from URLs, and store them under veteran_interviews/{idx}\n",
    "def download_media_files(df):\n",
    "    import os\n",
    "    base_dir = '/Volumes/KINGSTON/veteran_interviews'\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Prefer video if available, otherwise use audio\n",
    "        media_type = None\n",
    "        media_url = None\n",
    "        if pd.notnull(row.get('video_url')) and row['video_url']:\n",
    "            media_type = 'video'\n",
    "            media_url = row['video_url']\n",
    "            ext = '.mp4'\n",
    "        elif pd.notnull(row.get('audio_url')) and row['audio_url']:\n",
    "            media_type = 'audio'\n",
    "            media_url = row['audio_url']\n",
    "            ext = '.mp3'\n",
    "        else:\n",
    "            print(f\"No audio or video URL for index {idx}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(media_url, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            # Create a subdirectory for each index\n",
    "            sub_dir = os.path.join(base_dir, str(idx))\n",
    "            if not os.path.exists(sub_dir):\n",
    "                os.makedirs(sub_dir)\n",
    "            # skip downloading if the file already exists\n",
    "            elif os.path.exists(os.path.join(sub_dir, f'{media_type}{ext}')):\n",
    "                print(f\"File already exists for index {idx}: {media_type}{ext}\")\n",
    "                continue\n",
    "            else:\n",
    "                file_path = os.path.join(sub_dir, f'{media_type}{ext}')\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Downloaded {media_type} file for index {idx} to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {media_type} file for index {idx}: {e}\")\n",
    "            \n",
    "download_media_files(df_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # helper: create splits for df_resources (average size to run processing)\n",
    "# def create_splits(df, split_size=1000):\n",
    "#     splits = []\n",
    "#     for i in range(0, len(df), split_size):\n",
    "#         splits.append(df.iloc[i:i + split_size])\n",
    "#     return splits\n",
    "# # Create splits of the DataFrame\n",
    "# splits = create_splits(df_resources, split_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to inspect raw transcript text structure\n",
    "# import pprint\n",
    "# pprint.pprint(df_resources['fulltext_file_str'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_gt_sample = df_resources['transcript_raw_text_only'][1]\n",
    "from pprint import pprint\n",
    "pprint(transcript_gt_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_gt_sample_120 = \"\"\"\n",
    "H. Marie Thomas. I'm speaking with what's your name? John Aaron, Jr. John Aaron, Jr. And you got your information on here, your birth date. \n",
    "I've got your address and information. \n",
    "I need the birth date. . Okay. August. And where were you born, city of birthplace? Little Rock, Arkansas All right. Okay.\n",
    "What branch of service were you in? \n",
    "I was in the Navy first. You said first? Yes. I was in the Navy from 1941 to 1945. \n",
    "Then I went in the Reserve in I went back to school, and then in 1957 I received a commission, second lieutenant in the Army. \n",
    "Was that your highest ranking? I retired as a major. Oh, okay. Okay. \n",
    "Do you remember your serial number? Which one? You know, your social security number is this this is something you don't give out. Is this  No. It says serial number for service. \n",
    "It's different than social security number. Okay. That's fine. What battalion, regiment or division were you in? \n",
    "I was in the Navy. The first I was in the Navy.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
