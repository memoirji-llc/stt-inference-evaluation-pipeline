{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ?fo=json to get metadata of the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd                     # for reading, manipulating, and displaying data\n",
    "import requests\n",
    "from helpers_loc import get_file_stats\n",
    "# jukebox dataset\n",
    "DATA_URL = 'https://data.labs.loc.gov/jukebox/' # Base URL of this data package\n",
    "# Download the file manifest\n",
    "file_manifest_url = f'{DATA_URL}manifest.json'\n",
    "response = requests.get(file_manifest_url, timeout=60)\n",
    "response_json = response.json()\n",
    "# file information json\n",
    "files = [dict(zip(response_json[\"cols\"], row)) for row in response_json[\"rows\"]] # zip columns and rows\n",
    "\n",
    "# Convert to Pandas DataFrame and show stats table\n",
    "stats = get_file_stats(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(stats)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_url = f'{DATA_URL}metadata.json'\n",
    "response = requests.get(metadata_url, timeout=60)\n",
    "data = response.json()\n",
    "print(f'Loaded metadata file with {len(data):,} entries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = pd.DataFrame(data)\n",
    "print(', '.join(df.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_by_subject = df_metadata.explode('Subjects')\n",
    "df_opera = df_metadata_by_subject[df_metadata_by_subject.Subjects == 'Opera']\n",
    "print(f'Found {df_opera.shape[0]:,} items with subject \"Opera\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame from the file information\n",
    "df_files = pd.DataFrame(files)\n",
    "# join the metadata dataframe with the file information dataframe\n",
    "opera_set_with_audio = pd.merge(df_opera, df_files, left_on='Id', right_on='item_id', how='inner')\n",
    "print(f'Found {opera_set_with_audio.shape[0]:,} opera items with audio files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "opera_set_with_audio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_key contains the path to the audio file\n",
    "import io\n",
    "\n",
    "import matplotlib.pyplot as plt         # for displaying data\n",
    "import numpy as np\n",
    "from pydub import AudioSegment          # for reading and manipulating audio files\n",
    "from scipy import signal                # for visualizing audio\n",
    "\n",
    "item = opera_set_with_audio.iloc[0]\n",
    "file_url = f'https://{item[\"object_key\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the audio to memory\n",
    "response = requests.get(file_url, timeout=60)\n",
    "audio_filestream = io.BytesIO(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read as mp3\n",
    "sample_rate = 48000\n",
    "sample_width = 1\n",
    "channels = 1\n",
    "audio_filestream.seek(0)  # Ensure stream is at the beginning\n",
    "sound = AudioSegment.from_mp3(audio_filestream)\n",
    "sound = sound.set_channels(channels)\n",
    "sound = sound.set_sample_width(sample_width)\n",
    "sound = sound.set_frame_rate(sample_rate)\n",
    "\n",
    "# Get the first 10 seconds\n",
    "ten_seconds = 10 * 1000\n",
    "first_10_seconds = sound[:ten_seconds]\n",
    "\n",
    "# Get audio samples and sample rate\n",
    "samples = first_10_seconds.get_array_of_samples()\n",
    "samples = np.array(samples)\n",
    "\n",
    "# Visualize the results\n",
    "frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "plt.pcolormesh(times, frequencies, np.log(spectrogram))\n",
    "# plt.imshow(spectrogram)\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veterans history project dataset\n",
    "DATA_URL = 'https://www.loc.gov/collections/veterans-history-project-collection'\n",
    "DATA_URL = \"https://www.loc.gov/collections/veterans-history-project-collection/?fa=online-format:online+text\"\n",
    "# adding fo=json can return web representation: https://www.loc.gov/collections/veterans-history-project-collection?fo=json\n",
    "# example code to parse the returned JSON\n",
    "web_rep_url = f'{DATA_URL}?fo=json'\n",
    "web_rep_url = \"https://www.loc.gov/collections/veterans-history-project-collection/?fo=json&fa=online-format:online+text\"\n",
    "# web_rep_url = \"https://www.loc.gov/search/?q=baseball&fo=json\"\n",
    "# Start with the initial URL\n",
    "current_url = web_rep_url\n",
    "# Initialize an empty list to store all results\n",
    "all_results = []\n",
    "import time\n",
    "import requests\n",
    "# import error type\n",
    "from requests.exceptions import RequestException\n",
    "# chucked encoding error\n",
    "from requests.exceptions import ChunkedEncodingError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for rate limiting and paging\n",
    "RATE_LIMIT_DELAY = 3  # Delay in seconds between requests (20 requests per minute)\n",
    "MAX_ITEMS_PER_PAGE = 1000  # Recommended maximum items per page\n",
    "MAX_TOTAL_ITEMS = 10  # Maximum items per query !! Set low for testing !!\n",
    "RETRY_DELAY = 1  # Initial delay in seconds for retries\n",
    "MAX_RETRIES = 5  # Maximum number of retries for rate-limited requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry logic with exponential backoff for 429 status codes\n",
    "def fetch_url_with_rate_limit(url, timeout=60):\n",
    "    retries = 0\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            print(f\"Attempting to fetch URL: {url}\")\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            if response.status_code == 429:  # Too Many Requests\n",
    "                print(\"Rate limit exceeded. Pausing...\")\n",
    "                time.sleep(RETRY_DELAY * (2 ** retries))  # Exponential backoff\n",
    "                retries += 1\n",
    "                continue\n",
    "            response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as req_err:\n",
    "            print(f\"Request error occurred: {req_err}\")\n",
    "            retries += 1\n",
    "            time.sleep(RETRY_DELAY * (2 ** retries))  # Exponential backoff\n",
    "    raise Exception(f\"Failed to fetch {url} after {MAX_RETRIES} retries.\")\n",
    "\n",
    "# Loop through pages with rate limiting and paging limits\n",
    "total_items_processed = 0\n",
    "while current_url and total_items_processed < MAX_TOTAL_ITEMS:\n",
    "    try:\n",
    "        response = fetch_url_with_rate_limit(current_url)\n",
    "        data = response.json()\n",
    "\n",
    "        # Process the results on the current page\n",
    "        results = data.get('results', [])\n",
    "        all_results.extend(results)\n",
    "        total_items_processed += len(results)\n",
    "        print(f\"Processed {len(results)} results. Total: {total_items_processed}\")\n",
    "\n",
    "        # Get the URL for the next page\n",
    "        current_url = data.get('pagination', {}).get('next')\n",
    "        print(f\"Next page URL: {current_url}\")\n",
    "\n",
    "        # Delay to respect rate limits\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing page {current_url}: {e}\")\n",
    "        break\n",
    "\n",
    "# Convert the accumulated results into a DataFrame\n",
    "if all_results:\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    print(\"DataFrame created successfully.\")\n",
    "else:\n",
    "    print(\"No results found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df as parquet file\n",
    "df_results.to_parquet('veterans_history_project.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loaded web representation with {len(data):,} entries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the df_results DataFrame from the parquet file\n",
    "df_results = pd.read_parquet('veterans_history_project.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.columns\n",
    "# for each column print the first row data\n",
    "for col in df_results.columns:\n",
    "    print(f'{col}: {df_results[col].iloc[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_resource = []\n",
    "# for n in range(len(df_results)):\n",
    "for n in range(10):\n",
    "    collection_number = df_results['item'][n]['collection_number']\n",
    "    title = df_results['title'][n]\n",
    "    description = df_results['description'][n]\n",
    "    dates = df_results['dates'][n]\n",
    "    language = df_results['language'][n]\n",
    "    location = df_results['location'][n]\n",
    "    location_home = df_results['location_home'][n]\n",
    "    location_service = df_results['location_service'][n]\n",
    "    partof = df_results['partof'][n]\n",
    "    subject = df_results['subject'][n]\n",
    "    subject_battles = df_results['subject_battles'][n]\n",
    "    subject_branch = df_results['subject_branch'][n]\n",
    "    subject_conflict = df_results['subject_conflict'][n]\n",
    "    subject_entrance = df_results['subject_entrance'][n]\n",
    "    subject_format = df_results['subject_format'][n]\n",
    "    subject_gender = df_results['subject_gender'][n]\n",
    "    subject_rank = df_results['subject_rank'][n]\n",
    "    subject_status = df_results['subject_status'][n]\n",
    "    subject_unit = df_results['subject_unit'][n]\n",
    "    subject_race = df_results['subject_race'][n]\n",
    "    collection_resource = []\n",
    "\n",
    "    for resource in df_results['resources'][n]:\n",
    "        try:\n",
    "            fulltext_file_url = resource.get('fulltext_file', None)\n",
    "            retries = 0\n",
    "            while retries < MAX_RETRIES:\n",
    "                try:\n",
    "                    if fulltext_file_url:\n",
    "                        print(f\"Fetching fulltext file: {fulltext_file_url}\")\n",
    "                        response = requests.get(fulltext_file_url, timeout=60)\n",
    "                        if response.status_code == 429:  # Too Many Requests\n",
    "                            print(\"Rate limit exceeded. Pausing...\")\n",
    "                            time.sleep(RETRY_DELAY * (2 ** retries))  # Exponential backoff\n",
    "                            retries += 1\n",
    "                            continue\n",
    "                        response.raise_for_status()\n",
    "                        fulltext_file_str = response.text\n",
    "                        # print(f\"Successfully fetched fulltext file: {fulltext_file_str}\")\n",
    "                    else:\n",
    "                        fulltext_file_str = None\n",
    "                    break\n",
    "                except requests.exceptions.RequestException as req_err:\n",
    "                    print(f\"Request error occurred: {req_err}\")\n",
    "                    retries += 1\n",
    "                    time.sleep(RETRY_DELAY * (2 ** retries))  # Exponential backoff\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching fulltext file: {e}\")\n",
    "                    fulltext_file_str = None\n",
    "                    break\n",
    "            video_url = resource.get('video', None)\n",
    "            audio_url = resource.get('audio', None)\n",
    "        except Exception as e:\n",
    "            fulltext_file_url = None\n",
    "            fulltext_file_str = None\n",
    "            video_url = None\n",
    "            audio_url = None\n",
    "        finally:\n",
    "            collection_resource.append({\n",
    "            'collection_number': collection_number,\n",
    "            'fulltext_file_url': fulltext_file_url,\n",
    "            'fulltext_file_str': fulltext_file_str,\n",
    "            'video_url': video_url,\n",
    "            'audio_url': audio_url,\n",
    "            'title': title,\n",
    "            'description': description,\n",
    "            'dates': dates,\n",
    "            'language': language,\n",
    "            'location': location,\n",
    "            'location_home': location_home,\n",
    "            'location_service': location_service,\n",
    "            'partof': partof,\n",
    "            'subject': subject,\n",
    "            'subject_battles': subject_battles,\n",
    "            'subject_branch': subject_branch,\n",
    "            'subject_conflict': subject_conflict,\n",
    "            'subject_entrance': subject_entrance,\n",
    "            'subject_format': subject_format,\n",
    "            'subject_gender': subject_gender,\n",
    "            'subject_rank': subject_rank,\n",
    "            'subject_status': subject_status,\n",
    "            'subject_unit': subject_unit,\n",
    "            'subject_race': subject_race       \n",
    "            })\n",
    "    l_resource.append(collection_resource)\n",
    "#transform the list of resources into a DataFrame\n",
    "df_resources = pd.DataFrame([item for sublist in l_resource for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resources.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame to a parquet file\n",
    "df_resources.to_parquet('veterans_history_project_resources.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the df_results DataFrame from the parquet file\n",
    "df_resources = pd.read_parquet('veterans_history_project_resources.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to inspect raw transcript text structure\n",
    "# import pprint\n",
    "# pprint.pprint(df_resources['fulltext_file_str'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_raw_transcript_str(fulltext_file_str: str) -> str:\n",
    "    l_transcript_lines = []\n",
    "    # utilize bs4 xml parser\n",
    "    soup = BeautifulSoup(fulltext_file_str, 'xml')\n",
    "    # each sp tag in the document represents a \"line\" in the transcript\n",
    "    for sp in soup.find_all('sp'):\n",
    "        \n",
    "        try:\n",
    "            speaker = sp.find('speaker').get_text(strip=True)\n",
    "        \n",
    "        except:\n",
    "            # placeholder speaker tag if not found\n",
    "            speaker = \"speaker_unknown\"\n",
    "        try:\n",
    "            # return empty text if p tag not found\n",
    "            spoken_text = sp.find('p').get_text(strip=True)\n",
    "        \n",
    "        except:\n",
    "            spoken_text = \"\"\n",
    "        \n",
    "        l_transcript_lines.append(f\"<{speaker}>{spoken_text}</{speaker}> \")\n",
    "    \n",
    "    # merge lines into one string\n",
    "    transcript_lines = ''.join(l_transcript_lines)\n",
    "    \n",
    "    # remove (), [], {} and anything in between\n",
    "    transcript_lines_stripped = re.sub(r'\\([^)]*\\)', '', transcript_lines)\n",
    "    transcript_lines_stripped = re.sub(r'\\[[^]]*\\]', '', transcript_lines_stripped)\n",
    "    transcript_lines_stripped = re.sub(r'\\{[^}]*\\)\\}', '', transcript_lines_stripped)\n",
    "\n",
    "    # remove double dashes and ellipsis\n",
    "    transcript_lines_stripped = re.sub(r'--+', '', transcript_lines_stripped)\n",
    "    transcript_lines_stripped = re.sub(r'\\.{2,}', '', transcript_lines_stripped)\n",
    "\n",
    "    # clean whitespace\n",
    "    transcript_lines_stripped = re.sub(r'\\s+', ' ', transcript_lines_stripped).strip()\n",
    "    \n",
    "    return transcript_lines_stripped\n",
    "\n",
    "def remove_speaker_tag(transcript_lines_stripped: str) -> str:\n",
    "    # remove <> and anything in between\n",
    "    try:\n",
    "        return re.sub(r'\\<[^>]*\\>', '', transcript_lines_stripped)\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resources['fulltext_file_str_cleaned'] = df_resources['fulltext_file_str'].apply(clean_raw_transcript_str)\n",
    "df_resources['transcript_raw_text_only'] = df_resources['fulltext_file_str_cleaned'].apply(remove_speaker_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_gt_sample = df_resources['transcript_raw_text_only'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_gt_sample_120 = \"\"\"\n",
    "H. Marie Thomas. I'm speaking with what's your name? John Aaron, Jr. John Aaron, Jr. And you got your information on here, your birth date. \n",
    "I've got your address and information. \n",
    "I need the birth date. . Okay. August. And where were you born, city of birthplace? Little Rock, Arkansas All right. Okay.\n",
    "What branch of service were you in? \n",
    "I was in the Navy first. You said first? Yes. I was in the Navy from 1941 to 1945. \n",
    "Then I went in the Reserve in I went back to school, and then in 1957 I received a commission, second lieutenant in the Army. \n",
    "Was that your highest ranking? I retired as a major. Oh, okay. Okay. \n",
    "Do you remember your serial number? Which one? You know, your social security number is this this is something you don't give out. Is this  No. It says serial number for service. \n",
    "It's different than social security number. Okay. That's fine. What battalion, regiment or division were you in? \n",
    "I was in the Navy. The first I was in the Navy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "AUDIO_DIR = \"/Users/ac/main/amia2025-stt-benchmarking/data/audio/transcript_audio_sample.mp3\"\n",
    "# Use small, CPU/MPS friendly model for now\n",
    "MODEL = \"facebook/wav2vec2-base-960h\"\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "# keep CPU stable in notebooks\n",
    "torch.set_num_threads(4)  \n",
    "# load first 120s without reading entire file\n",
    "info = sf.info(AUDIO_DIR)\n",
    "wave, sr = librosa.load(AUDIO_DIR, sr=16000, mono=True, duration=120)\n",
    "\n",
    "if wave.ndim == 2:\n",
    "    wave = wave.mean(axis=1)\n",
    "    \n",
    "# set up processor\n",
    "processor = AutoProcessor.from_pretrained(MODEL)\n",
    "# set up model\n",
    "model = AutoModelForCTC.from_pretrained(MODEL).to(\"cpu\").eval()\n",
    "\n",
    "inputs = processor(wave, sampling_rate=sr, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values.to(\"cpu\")).logits\n",
    "\n",
    "ids = torch.argmax(logits, dim=-1)\n",
    "hyp_ctc = processor.batch_decode(ids)[0].lower()\n",
    "\n",
    "print(\"partial transcription results (Wav2Vec2)\")\n",
    "print(hyp_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "\n",
    "# Normalize beforehand if desired:\n",
    "transform = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.Strip()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_norm = transform(transcript_gt_sample_120)\n",
    "hyp_norm = transform(hyp_ctc)\n",
    "\n",
    "output = jiwer.process_words(ref_norm, hyp_norm)\n",
    "print(output)  # a dataclass with attributes\n",
    "print(\"WER:\", output.wer)\n",
    "print(\"Substitutions:\", output.substitutions)\n",
    "print(\"Deletions:\", output.deletions)\n",
    "print(\"Insertions:\", output.insertions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv run python - <<'PY'\n",
    "# from faster_whisper import WhisperModel\n",
    "# root = \"/Users/ac/main/amia2025-stt-benchmarking/models/local\"\n",
    "# for name in [\"base\", \"small\"]:   # add \"medium\" later if you want\n",
    "#     print(f\"Downloading {name} to {root} ...\")\n",
    "#     WhisperModel(name, device=\"cpu\", download_root=root)\n",
    "# print(\"Done.\")\n",
    "# PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import NamedTemporaryFile\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "ROOT = Path(\"/Users/ac/main/amia2025-stt-benchmarking/models/local/models--Systran--faster-whisper-small\")\n",
    "SNAP = max((ROOT / \"snapshots\").iterdir(), key=lambda p: p.stat().st_mtime)\n",
    "LOCAL_MODEL_DIR = str(SNAP)  # folder that contains model.bin\n",
    "print(\"Using model dir:\", LOCAL_MODEL_DIR)\n",
    "\n",
    "# load exactly first 120s as mono 16k\n",
    "wave, _ = librosa.load(AUDIO_DIR, sr=16000, mono=True, duration=120)\n",
    "\n",
    "model = WhisperModel(LOCAL_MODEL_DIR, device=\"auto\")\n",
    "\n",
    "with NamedTemporaryFile(suffix=\".wav\", delete=True) as tmp:\n",
    "    sf.write(tmp.name, wave, 16000)  # write trimmed clip\n",
    "    segments, info = model.transcribe(\n",
    "        tmp.name,\n",
    "        beam_size=1,\n",
    "        temperature=0.0,\n",
    "        vad_filter=True,\n",
    "        no_speech_threshold=0.6,\n",
    "        word_timestamps=False,\n",
    "        initial_prompt=None,\n",
    "        suppress_tokens=[-1],   # <-- FIX: list of ints, or set to None\n",
    "        condition_on_previous_text=False,\n",
    "    )\n",
    "\n",
    "hyp_whisper = \" \".join(s.text.strip().lower() for s in segments)\n",
    "print(f\"[{info.language}] {hyp_whisper[:400]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download audio/ video files from URLs, and store them under veteran_interviews/{idx}\n",
    "def download_media_files(df):\n",
    "    import os\n",
    "    base_dir = '/Volumes/KINGSTON/veteran_interviews'\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Prefer video if available, otherwise use audio\n",
    "        media_type = None\n",
    "        media_url = None\n",
    "        if pd.notnull(row.get('video_url')) and row['video_url']:\n",
    "            media_type = 'video'\n",
    "            media_url = row['video_url']\n",
    "            ext = '.mp4'\n",
    "        elif pd.notnull(row.get('audio_url')) and row['audio_url']:\n",
    "            media_type = 'audio'\n",
    "            media_url = row['audio_url']\n",
    "            ext = '.mp3'\n",
    "        else:\n",
    "            print(f\"No audio or video URL for index {idx}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(media_url, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            # Create a subdirectory for each index\n",
    "            sub_dir = os.path.join(base_dir, str(idx))\n",
    "            if not os.path.exists(sub_dir):\n",
    "                os.makedirs(sub_dir)\n",
    "            # skip downloading if the file already exists\n",
    "            elif os.path.exists(os.path.join(sub_dir, f'{media_type}{ext}')):\n",
    "                print(f\"File already exists for index {idx}: {media_type}{ext}\")\n",
    "                continue\n",
    "            else:\n",
    "                file_path = os.path.join(sub_dir, f'{media_type}{ext}')\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Downloaded {media_type} file for index {idx} to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {media_type} file for index {idx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create splits for df_resources (average size to run processing)\n",
    "def create_splits(df, split_size=1000):\n",
    "    splits = []\n",
    "    for i in range(0, len(df), split_size):\n",
    "        splits.append(df.iloc[i:i + split_size])\n",
    "    return splits\n",
    "# Create splits of the DataFrame\n",
    "splits = create_splits(df_resources, split_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_media_files(df_resources)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
