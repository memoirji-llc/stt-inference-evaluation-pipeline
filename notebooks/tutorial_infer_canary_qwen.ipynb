{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: NVIDIA Canary-Qwen-2.5B Inference\n",
    "\n",
    "**Quick Start Guide** for running Canary-Qwen-2.5B speech-to-text inference.\n",
    "\n",
    "**What is Canary-Qwen?**\n",
    "- Speech Audio Language Model (SALM) with 2.5B parameters\n",
    "- Combines NVIDIA Canary ASR with Qwen3-1.7B LLM\n",
    "- Top performer on HuggingFace OpenASR leaderboard (WER: 5.63%)\n",
    "- Supports English with punctuation and capitalization\n",
    "\n",
    "**Requirements:**\n",
    "- GPU: ~10 GB VRAM (tested on T4 16GB)\n",
    "- RAM: ~5 GB system memory\n",
    "- Audio: 16kHz mono WAV/FLAC files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Configure environment and check resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Configure caching and logging\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "os.environ['HF_HOME'] = str(Path.cwd().parent / \"models/canary\")\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(Path.cwd().parent / \"models/canary\")\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  VRAM: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"❌ No GPU found - Canary requires CUDA\")\n",
    "\n",
    "print(\"\\n✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model\n",
    "\n",
    "Load Canary-Qwen-2.5B using the SALM API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.speechlm2.models import SALM\n",
    "\n",
    "print(\"Loading Canary-Qwen-2.5B...\")\n",
    "print(\"(First run downloads ~5GB model)\\n\")\n",
    "\n",
    "model = SALM.from_pretrained(\"nvidia/canary-qwen-2.5b\")\n",
    "model = model.cuda()\n",
    "\n",
    "print(\"\\n✓ Model loaded (uses ~5 GB RAM, ~10 GB GPU)\")\n",
    "print(f\"  Audio placeholder tag: {model.audio_locator_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transcribe Audio\n",
    "\n",
    "**Method 1: Single file (simple)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Transcribe a WAV file\n",
    "audio_path = \"/path/to/your/audio.wav\"  # Change this\n",
    "\n",
    "# Format prompt with audio placeholder\n",
    "prompts = [[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Transcribe the following: {model.audio_locator_tag}\",\n",
    "        \"audio\": [audio_path]\n",
    "    }\n",
    "]]\n",
    "\n",
    "# Generate transcription\n",
    "with torch.no_grad():\n",
    "    answer_ids = model.generate(prompts=prompts, max_new_tokens=512)\n",
    "\n",
    "# Decode result\n",
    "transcript = model.tokenizer.ids_to_text(answer_ids[0].cpu()).strip()\n",
    "\n",
    "print(\"Transcript:\")\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2: Batch processing (production)**\n",
    "\n",
    "Use the production script for multiple files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, str(Path.cwd().parent / \"scripts\"))\n",
    "from infer_canary import run\n",
    "\n",
    "# Configure inference run\n",
    "config = {\n",
    "    \"experiment_id\": \"my-transcription\",\n",
    "    \n",
    "    \"model\": {\n",
    "        \"name\": \"canary\",\n",
    "        \"path\": \"nvidia/canary-qwen-2.5b\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"prompt\": \"Transcribe the following:\",\n",
    "        \"max_new_tokens\": 512\n",
    "    },\n",
    "    \n",
    "    \"input\": {\n",
    "        \"source\": \"azure_blob\",  # or \"local_files\"\n",
    "        \"parquet_path\": \"../data/raw/loc/veterans_history_project_resources_pre2010.parquet\",\n",
    "        \"blob_prefix\": \"loc_vhp\",\n",
    "        \"sample_size\": 10,  # Number of files\n",
    "        \"duration_sec\": 300,  # First 5 minutes (or None for full)\n",
    "        \"sample_rate\": 16000\n",
    "    },\n",
    "    \n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/my-transcription\",\n",
    "        \"save_per_file\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run inference\n",
    "run(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load results\n",
    "results = pd.read_parquet(\"../outputs/my-transcription/inference_results.parquet\")\n",
    "\n",
    "print(f\"Processed {len(results)} files\")\n",
    "print(f\"\\nColumns: {list(results.columns)}\")\n",
    "print(f\"\\nFirst transcript:\")\n",
    "print(results.iloc[0]['hypothesis'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips & Best Practices\n",
    "\n",
    "**Audio Requirements:**\n",
    "- Format: 16kHz, mono, WAV/FLAC\n",
    "- Max duration tested: 40 seconds per prompt\n",
    "- For longer audio: chunk into segments\n",
    "\n",
    "**Prompting:**\n",
    "- Always include `{model.audio_locator_tag}` placeholder\n",
    "- Customize prompt: `\"Transcribe this interview:\"`, `\"Convert speech to text:\"`\n",
    "- Model handles punctuation/capitalization automatically\n",
    "\n",
    "**Performance:**\n",
    "- RTFx: 418× (processes audio 418× faster than real-time)\n",
    "- Batch size: 1 for development, increase for production\n",
    "- GPU memory: ~10 GB baseline + ~0.1 GB per concurrent inference\n",
    "\n",
    "**Troubleshooting:**\n",
    "- \"Missing audio_locator_tag\": Make sure prompt includes `{model.audio_locator_tag}`\n",
    "- \"CUDA out of memory\": Reduce batch size or process sequentially\n",
    "- \"Expected N replacements\": Audio placeholder missing from prompt\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- Model card: https://huggingface.co/nvidia/canary-qwen-2.5b\n",
    "- Production script: `scripts/infer_canary.py`\n",
    "- Config examples: `configs/runs/vhp-canary-*.yaml`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
