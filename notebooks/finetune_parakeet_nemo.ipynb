{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Fine-tuning Parakeet-TDT with NeMo\n",
    "\n",
    "**Target Hardware**: RunPod A6000 (48GB VRAM)\n",
    "\n",
    "This notebook fine-tunes NVIDIA Parakeet on VHP oral history audio using NeMo's official `speech_to_text_finetune.py` script.\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "Based on [official NVIDIA tutorials](https://github.com/nvidia-riva/tutorials/blob/main/asr-finetune-parakeet-nemo.ipynb) and [community discussions](https://github.com/NVIDIA-NeMo/NeMo/issues/13825):\n",
    "\n",
    "1. **Data Requirements**: Fine-tuning typically needs 1000+ hours for good WER. With ~31 demo samples, this is just a **smoke test**.\n",
    "2. **Memory**: The 0.6B model ideally needs 80GB+ VRAM. A6000 (48GB) may require small batch sizes.\n",
    "3. **Training Method**: Uses `speech_to_text_finetune.py` CLI script (not Python API directly).\n",
    "4. **Model**: Uses FastConformer Hybrid Transducer-CTC architecture.\n",
    "\n",
    "## Data Requirements\n",
    "- **Segmented parquet files** with `segmented_audio_url` (Azure blob) and `segmented_audio_transcript` columns\n",
    "- Audio segments are downloaded from Azure blob storage to local cache\n",
    "- NeMo manifest format: `{\"audio_filepath\": \"...\", \"text\": \"...\", \"duration\": ...}`\n",
    "\n",
    "## References\n",
    "- [NVIDIA Riva Parakeet Fine-tuning Tutorial](https://github.com/nvidia-riva/tutorials/blob/main/asr-finetune-parakeet-nemo.ipynb)\n",
    "- [NeMo ASR Models Documentation](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html)\n",
    "- [Parakeet-TDT-0.6B-v3 on HuggingFace](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup Dependencies\n",
    "\n",
    "Fine-tuning with NeMo requires additional packages. Add them via uv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fine-tuning dependencies to pyproject.toml (run once)\n",
    "# uv add nemo_toolkit[asr] pytorch-lightning>=2.0\n",
    "#\n",
    "# For A6000 with CUDA 11.8:\n",
    "# uv add torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for package imports\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Load Azure credentials from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=PROJECT_ROOT / 'credentials/creds.env')\n",
    "\n",
    "import torch\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import pytorch_lightning as pl\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.utils import exp_manager\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Import project modules\n",
    "from scripts.eval.evaluate import clean_raw_transcript_str\n",
    "from scripts.cloud import azure_utils\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Azure credentials loaded: {os.getenv('AZURE_STORAGE_CONNECTION_STRING') is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths - Demo segmented parquets (small 31-sample set for testing)\n",
    "    \"train_parquet\": str(PROJECT_ROOT / \"data/raw/loc/veterans_history_project_resources_pre2010_train_nfa_segmented_demo.parquet\"),\n",
    "    \"val_parquet\": str(PROJECT_ROOT / \"data/raw/loc/veterans_history_project_resources_pre2010_val_nfa_segmented.parquet\"),\n",
    "    \n",
    "    # Azure blob settings (for downloading segmented audio)\n",
    "    \"blob_prefix\": \"loc_vhp\",\n",
    "    \n",
    "    # Sampling - None to use all data in demo parquets\n",
    "    \"train_sample_size\": None,\n",
    "    \"val_sample_size\": 10,  # Small val set for demo\n",
    "    \"random_seed\": 42,\n",
    "    \n",
    "    # Local audio cache directory (segments downloaded from Azure)\n",
    "    \"audio_cache_dir\": \"/workspace/audio_cache/parakeet_ft\",\n",
    "    \n",
    "    # NeMo manifest output (will be generated)\n",
    "    \"train_manifest\": \"/workspace/data/parakeet_train_manifest.json\",\n",
    "    \"val_manifest\": \"/workspace/data/parakeet_val_manifest.json\",\n",
    "    \n",
    "    # Output directory - follows convention: {dataset}-{model}-{task}-{infra}\n",
    "    \"output_dir\": \"/workspace/outputs/vhp-pre2010-parakeet-tdt-0.6b-ft-a6000\",\n",
    "    \"exp_name\": \"parakeet_tdt_vhp\",\n",
    "    \n",
    "    # Model\n",
    "    \"model_name\": \"nvidia/parakeet-tdt-0.6b-v3\",\n",
    "    \n",
    "    # Training hyperparameters (reduced for demo)\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_accumulation\": 2,\n",
    "    \"warmup_steps\": 50,\n",
    "    \"max_steps\": 500,        # Small for demo\n",
    "    \"val_check_interval\": 100,\n",
    "    \n",
    "    # Audio settings\n",
    "    \"sample_rate\": 16000,\n",
    "    \"max_duration\": 30,   # Max audio duration per sample (seconds)\n",
    "    \"min_duration\": 0.1,\n",
    "    \n",
    "    # Precision\n",
    "    \"precision\": \"16-mixed\",\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"audio_cache_dir\"], exist_ok=True)\n",
    "os.makedirs(os.path.dirname(CONFIG[\"train_manifest\"]), exist_ok=True)\n",
    "print(f\"Output directory: {CONFIG['output_dir']}\")\n",
    "print(f\"Audio cache: {CONFIG['audio_cache_dir']}\")\n",
    "print(f\"Train sample size: {CONFIG['train_sample_size']} (None = use all)\")\n",
    "print(f\"Val sample size: {CONFIG['val_sample_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Load and Sample Data\n",
    "\n",
    "Load segmented parquet files and sample train/val sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segmented_parquet(parquet_path: str, sample_size: int = None, random_seed: int = 42):\n",
    "    \"\"\"\n",
    "    Load segmented parquet and optionally sample.\n",
    "    \n",
    "    Filters to rows that have both segmented_audio_url and segmented_audio_transcript.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    print(f\"Loaded {len(df)} rows from {parquet_path}\")\n",
    "    \n",
    "    # Filter to rows with required columns\n",
    "    df = df[df['segmented_audio_url'].notna() & (df['segmented_audio_url'] != '')]\n",
    "    print(f\"Filtered to {len(df)} rows with segmented_audio_url\")\n",
    "    \n",
    "    df = df[df['segmented_audio_transcript'].notna() & (df['segmented_audio_transcript'] != '')]\n",
    "    print(f\"Filtered to {len(df)} rows with segmented_audio_transcript\")\n",
    "    \n",
    "    # Sample if requested\n",
    "    if sample_size and sample_size < len(df):\n",
    "        df = df.sample(n=sample_size, random_state=random_seed)\n",
    "        print(f\"Sampled {len(df)} rows\")\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading training data...\")\n",
    "df_train = load_segmented_parquet(\n",
    "    CONFIG[\"train_parquet\"],\n",
    "    sample_size=CONFIG[\"train_sample_size\"],\n",
    "    random_seed=CONFIG[\"random_seed\"]\n",
    ")\n",
    "\n",
    "print(\"\\nLoading validation data...\")\n",
    "df_val = load_segmented_parquet(\n",
    "    CONFIG[\"val_parquet\"],\n",
    "    sample_size=CONFIG[\"val_sample_size\"],\n",
    "    random_seed=CONFIG[\"random_seed\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(df_train)} samples\")\n",
    "print(f\"Val: {len(df_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_audio_to_local(blob_url: str, cache_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Download audio from Azure blob to local cache.\n",
    "    \n",
    "    Returns local path, or None if failed.\n",
    "    Skips download if file already exists in cache.\n",
    "    \"\"\"\n",
    "    # Create deterministic local filename from blob URL\n",
    "    # e.g., \"loc_vhp_segments/123/segment_0.wav\" -> \"123_segment_0.wav\"\n",
    "    blob_name = blob_url.split('/')[-2] + '_' + blob_url.split('/')[-1]\n",
    "    local_path = os.path.join(cache_dir, blob_name)\n",
    "    \n",
    "    # Skip if already cached\n",
    "    if os.path.exists(local_path):\n",
    "        return local_path\n",
    "    \n",
    "    try:\n",
    "        # Download from Azure\n",
    "        audio_bytes = azure_utils.download_blob_to_memory(blob_url)\n",
    "        \n",
    "        # Write to local file\n",
    "        with open(local_path, 'wb') as f:\n",
    "            f.write(audio_bytes)\n",
    "        \n",
    "        return local_path\n",
    "    except Exception as e:\n",
    "        print(f\"  Error downloading {blob_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_nemo_manifest_from_segmented(\n",
    "    df: pd.DataFrame,\n",
    "    cache_dir: str,\n",
    "    output_path: str,\n",
    "    max_duration_sec: float = 30.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Create NeMo manifest from segmented parquet.\n",
    "    \n",
    "    Downloads audio from Azure blob URLs to local cache,\n",
    "    then creates NeMo manifest with local paths.\n",
    "    \n",
    "    NeMo manifest format (JSON lines):\n",
    "    {\"audio_filepath\": \"/path/to/audio.wav\", \"text\": \"transcription\", \"duration\": 15.4}\n",
    "    \"\"\"\n",
    "    entries = []\n",
    "    skipped_download = 0\n",
    "    skipped_duration = 0\n",
    "    skipped_empty = 0\n",
    "    \n",
    "    print(f\"Processing {len(df)} samples...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Get transcript (already cleaned in segmentation)\n",
    "        transcript = row.get('segmented_audio_transcript', '').strip()\n",
    "        if not transcript:\n",
    "            skipped_empty += 1\n",
    "            continue\n",
    "        \n",
    "        # Download audio to local cache\n",
    "        blob_url = row.get('segmented_audio_url', '')\n",
    "        local_path = download_audio_to_local(blob_url, cache_dir)\n",
    "        \n",
    "        if local_path is None:\n",
    "            skipped_download += 1\n",
    "            continue\n",
    "        \n",
    "        # Get duration from metadata or load file\n",
    "        duration = row.get('segmented_audio_duration', None)\n",
    "        if duration is None:\n",
    "            try:\n",
    "                y, sr = librosa.load(local_path, sr=None)\n",
    "                duration = librosa.get_duration(y=y, sr=sr)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {local_path}: {e}\")\n",
    "                skipped_download += 1\n",
    "                continue\n",
    "        \n",
    "        # Skip if too long\n",
    "        if duration > max_duration_sec:\n",
    "            skipped_duration += 1\n",
    "            continue\n",
    "        \n",
    "        # NeMo expects lowercase text\n",
    "        entries.append({\n",
    "            \"audio_filepath\": local_path,\n",
    "            \"text\": transcript.lower(),\n",
    "            \"duration\": round(float(duration), 2)\n",
    "        })\n",
    "        \n",
    "        if len(entries) % 200 == 0:\n",
    "            print(f\"  Processed {len(entries)} entries...\")\n",
    "    \n",
    "    # Write manifest\n",
    "    with open(output_path, 'w') as f:\n",
    "        for entry in entries:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "    \n",
    "    total_hours = sum(e['duration'] for e in entries) / 3600\n",
    "    print(f\"\\nCreated manifest: {output_path}\")\n",
    "    print(f\"  Valid entries: {len(entries)}\")\n",
    "    print(f\"  Skipped (download failed): {skipped_download}\")\n",
    "    print(f\"  Skipped (empty transcript): {skipped_empty}\")\n",
    "    print(f\"  Skipped (too long): {skipped_duration}\")\n",
    "    print(f\"  Total hours: {total_hours:.2f}\")\n",
    "    \n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training manifest (downloads audio to local cache)\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING TRAINING MANIFEST\")\n",
    "print(\"=\"*60)\n",
    "train_entries = create_nemo_manifest_from_segmented(\n",
    "    df_train,\n",
    "    CONFIG[\"audio_cache_dir\"],\n",
    "    CONFIG[\"train_manifest\"],\n",
    "    max_duration_sec=CONFIG[\"max_duration\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING VALIDATION MANIFEST\")\n",
    "print(\"=\"*60)\n",
    "val_entries = create_nemo_manifest_from_segmented(\n",
    "    df_val,\n",
    "    CONFIG[\"audio_cache_dir\"],\n",
    "    CONFIG[\"val_manifest\"],\n",
    "    max_duration_sec=CONFIG[\"max_duration\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a sample from manifest\n",
    "import random\n",
    "random.seed(CONFIG[\"random_seed\"])\n",
    "\n",
    "with open(CONFIG[\"train_manifest\"], 'r') as f:\n",
    "    manifest_lines = f.readlines()\n",
    "\n",
    "if manifest_lines:\n",
    "    sample_entry = json.loads(random.choice(manifest_lines))\n",
    "    print(\"Sample manifest entry:\")\n",
    "    print(f\"  Audio: {sample_entry['audio_filepath']}\")\n",
    "    print(f\"  Duration: {sample_entry['duration']:.1f}s\")\n",
    "    print(f\"  Text (first 200 chars): {sample_entry['text'][:200]}...\")\n",
    "else:\n",
    "    print(\"WARNING: No entries in manifest!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 4. Find NeMo Installation Path\n",
    "\n",
    "We need to locate the NeMo examples directory for the fine-tuning script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find NeMo installation path\n",
    "# Note: NeMo examples are NOT included in pip install, must clone the repo\n",
    "import nemo\n",
    "\n",
    "NEMO_GIT_DIR = \"/workspace/NeMo\"\n",
    "\n",
    "# Always use the git repo for examples (pip install doesn't include them)\n",
    "if not os.path.exists(NEMO_GIT_DIR):\n",
    "    print(\"Cloning NeMo repo for training scripts...\")\n",
    "    !git clone --depth 1 https://github.com/NVIDIA/NeMo.git {NEMO_GIT_DIR}\n",
    "else:\n",
    "    print(f\"NeMo repo already exists at {NEMO_GIT_DIR}\")\n",
    "\n",
    "NEMO_EXAMPLES = os.path.join(NEMO_GIT_DIR, \"examples\")\n",
    "\n",
    "print(f\"NeMo package: {os.path.dirname(nemo.__file__)}\")\n",
    "print(f\"NeMo examples: {NEMO_EXAMPLES}\")\n",
    "print(f\"Finetune script exists: {os.path.exists(os.path.join(NEMO_EXAMPLES, 'asr/speech_to_text_finetune.py'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 5. Configure Training\n",
    "\n",
    "The official method uses NeMo's `speech_to_text_finetune.py` script with command-line config overrides.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `init_from_pretrained_model`: Load pretrained FastConformer model\n",
    "- `trainer.max_epochs`: Number of epochs (set low for demo)\n",
    "- `trainer.precision`: bf16 for memory efficiency\n",
    "- `model.optim.lr`: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAIN_CONFIG = {\n",
    "    # Pretrained model to fine-tune\n",
    "    # Options: stt_en_fastconformer_hybrid_large_pc (recommended for Parakeet-like)\n",
    "    #          nvidia/parakeet-tdt-0.6b-v3 (direct, but may need different script)\n",
    "    \"pretrained_model\": \"stt_en_fastconformer_hybrid_large_pc\",\n",
    "    \n",
    "    # Training parameters (reduced for demo)\n",
    "    \"max_epochs\": 5,           # Set to 50-200 for real training\n",
    "    \"precision\": \"bf16\",       # bf16 or 16-mixed\n",
    "    \"lr\": 0.0001,              # Learning rate (0.1 in tutorial, but that's aggressive)\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"warmup_steps\": 50,\n",
    "    \n",
    "    # Batch size (small for A6000 memory)\n",
    "    \"batch_size\": 4,\n",
    "    \n",
    "    # Checkpointing\n",
    "    \"exp_dir\": CONFIG[\"output_dir\"],\n",
    "    \"exp_name\": \"parakeet_ft_demo\",\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in TRAIN_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training command\n",
    "# Based on: https://github.com/nvidia-riva/tutorials/blob/main/asr-finetune-parakeet-nemo.ipynb\n",
    "\n",
    "finetune_script = os.path.join(NEMO_EXAMPLES, \"asr/speech_to_text_finetune.py\")\n",
    "config_path = \"../asr/conf/fastconformer/hybrid_transducer_ctc/\"\n",
    "config_name = \"fastconformer_hybrid_transducer_ctc_bpe\"\n",
    "\n",
    "train_cmd = f\"\"\"python {finetune_script} \\\\\n",
    "  --config-path=\"{config_path}\" \\\\\n",
    "  --config-name={config_name} \\\\\n",
    "  +init_from_pretrained_model={TRAIN_CONFIG['pretrained_model']} \\\\\n",
    "  ++model.train_ds.manifest_filepath=\"{CONFIG['train_manifest']}\" \\\\\n",
    "  ++model.validation_ds.manifest_filepath=\"{CONFIG['val_manifest']}\" \\\\\n",
    "  ++model.train_ds.batch_size={TRAIN_CONFIG['batch_size']} \\\\\n",
    "  ++model.validation_ds.batch_size={TRAIN_CONFIG['batch_size']} \\\\\n",
    "  ++model.optim.sched.d_model=1024 \\\\\n",
    "  ++trainer.devices=1 \\\\\n",
    "  ++trainer.max_epochs={TRAIN_CONFIG['max_epochs']} \\\\\n",
    "  ++trainer.precision={TRAIN_CONFIG['precision']} \\\\\n",
    "  ++model.optim.name=\"adamw\" \\\\\n",
    "  ++model.optim.lr={TRAIN_CONFIG['lr']} \\\\\n",
    "  ++model.optim.weight_decay={TRAIN_CONFIG['weight_decay']} \\\\\n",
    "  ++model.optim.sched.warmup_steps={TRAIN_CONFIG['warmup_steps']} \\\\\n",
    "  ++exp_manager.exp_dir={TRAIN_CONFIG['exp_dir']} \\\\\n",
    "  ++exp_manager.name={TRAIN_CONFIG['exp_name']} \\\\\n",
    "  ++exp_manager.use_datetime_version=False \\\\\n",
    "  ++exp_manager.version=v1\n",
    "\"\"\"\n",
    "\n",
    "print(\"Training command:\")\n",
    "print(train_cmd)\n",
    "print(\"\\nScript exists:\", os.path.exists(finetune_script))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 6. Run Training\n",
    "\n",
    "⚠️ **Warning**: This will take a while even for the demo dataset. For the full dataset, consider running in a screen session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "# Note: This uses subprocess to capture output properly in Jupyter\n",
    "import subprocess\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Pretrained model: {TRAIN_CONFIG['pretrained_model']}\")\n",
    "print(f\"Train manifest: {CONFIG['train_manifest']}\")\n",
    "print(f\"Val manifest: {CONFIG['val_manifest']}\")\n",
    "print(f\"Max epochs: {TRAIN_CONFIG['max_epochs']}\")\n",
    "print(f\"Output dir: {TRAIN_CONFIG['exp_dir']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Execute training\n",
    "!{train_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 7. Find Saved Model\n",
    "\n",
    "After training, NeMo saves checkpoints to the experiment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find saved model checkpoint\n",
    "import glob\n",
    "\n",
    "checkpoint_dir = os.path.join(TRAIN_CONFIG['exp_dir'], TRAIN_CONFIG['exp_name'], \"v1\", \"checkpoints\")\n",
    "print(f\"Looking for checkpoints in: {checkpoint_dir}\")\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    nemo_files = glob.glob(os.path.join(checkpoint_dir, \"*.nemo\"))\n",
    "    ckpt_files = glob.glob(os.path.join(checkpoint_dir, \"*.ckpt\"))\n",
    "    \n",
    "    print(f\"\\n.nemo files: {nemo_files}\")\n",
    "    print(f\".ckpt files: {ckpt_files}\")\n",
    "    \n",
    "    if nemo_files:\n",
    "        MODEL_PATH = nemo_files[0]\n",
    "        print(f\"\\nUsing model: {MODEL_PATH}\")\n",
    "    elif ckpt_files:\n",
    "        MODEL_PATH = ckpt_files[-1]  # Use latest\n",
    "        print(f\"\\nUsing checkpoint: {MODEL_PATH}\")\n",
    "    else:\n",
    "        MODEL_PATH = None\n",
    "        print(\"\\nNo checkpoints found!\")\n",
    "else:\n",
    "    print(f\"Checkpoint directory not found: {checkpoint_dir}\")\n",
    "    MODEL_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel saved to: {MODEL_PATH}\")\n",
    "print(f\"\\nTo load later:\")\n",
    "print(f\"  model = nemo_asr.models.ASRModel.restore_from('{MODEL_PATH}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 8. Test Inference\n",
    "\n",
    "Load the fine-tuned model and test on validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model and run test inference\n",
    "test_results = []\n",
    "\n",
    "if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "    finetuned_model = nemo_asr.models.ASRModel.restore_from(MODEL_PATH)\n",
    "    finetuned_model.eval()\n",
    "    finetuned_model = finetuned_model.cuda()\n",
    "    print(\"Model loaded successfully!\\n\")\n",
    "    \n",
    "    # Test on validation manifest\n",
    "    with open(CONFIG[\"val_manifest\"], 'r') as f:\n",
    "        val_manifest_lines = f.readlines()\n",
    "    \n",
    "    print(f\"Testing on {min(5, len(val_manifest_lines))} samples from validation manifest...\")\n",
    "    \n",
    "    for i, line in enumerate(val_manifest_lines[:5]):\n",
    "        entry = json.loads(line)\n",
    "        audio_path = entry['audio_filepath']\n",
    "        ground_truth = entry['text']\n",
    "        \n",
    "        print(f\"  [{i}] Processing: {os.path.basename(audio_path)}\")\n",
    "        \n",
    "        try:\n",
    "            result = finetuned_model.transcribe([audio_path])\n",
    "            hypothesis = result[0] if result else \"\"\n",
    "            \n",
    "            test_results.append({\n",
    "                \"file_id\": i,\n",
    "                \"hypothesis\": hypothesis.lower(),\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"audio_path\": audio_path\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nCompleted: {len(test_results)} files\")\n",
    "else:\n",
    "    print(\"No model checkpoint found. Skipping inference test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View test results\n",
    "if 'test_results' in dir() and test_results:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TEST INFERENCE RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for r in test_results[:3]:  # Show first 3\n",
    "        print(f\"\\nFile ID: {r['file_id']}\")\n",
    "        print(f\"Audio: {os.path.basename(r['audio_path'])}\")\n",
    "        print(f\"\\nHypothesis (first 200 chars):\")\n",
    "        print(r['hypothesis'][:200] + \"...\" if len(r['hypothesis']) > 200 else r['hypothesis'])\n",
    "        print(f\"\\nGround truth (first 200 chars):\")\n",
    "        print(r['ground_truth'][:200] + \"...\" if len(r['ground_truth']) > 200 else r['ground_truth'])\n",
    "        print(\"-\" * 70)\n",
    "else:\n",
    "    print(\"No test results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "The fine-tuned model is saved to:\n",
    "```\n",
    "/workspace/outputs/vhp-pre2010-parakeet-tdt-0.6b-ft-a6000/parakeet_tdt_finetuned.nemo\n",
    "```\n",
    "\n",
    "To run full evaluation:\n",
    "\n",
    "1. **Create inference script** for the fine-tuned NeMo model (similar to `infer_parakeet.py`)\n",
    "\n",
    "2. **Run evaluation** with `scripts/eval/evaluate.py` to compute WER metrics\n",
    "\n",
    "3. **Compare** with baseline Parakeet (pre-fine-tuning) results\n",
    "\n",
    "To load the fine-tuned model later:\n",
    "```python\n",
    "import nemo.collections.asr as nemo_asr\n",
    "model = nemo_asr.models.ASRModel.restore_from(\"/workspace/outputs/vhp-pre2010-parakeet-tdt-0.6b-ft-a6000/parakeet_tdt_finetuned.nemo\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
