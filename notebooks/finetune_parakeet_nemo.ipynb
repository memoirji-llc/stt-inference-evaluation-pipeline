{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Fine-tuning Parakeet-TDT-0.6B-v3 with NeMo\n",
    "\n",
    "**Target Hardware**: RunPod A6000 (48GB VRAM)\n",
    "\n",
    "This notebook fine-tunes NVIDIA Parakeet-TDT-0.6B-v3 on VHP oral history audio using NeMo.\n",
    "\n",
    "## Key Configuration\n",
    "- **Model**: nvidia/parakeet-tdt-0.6b-v3\n",
    "- **Learning Rate**: 5e-5\n",
    "- **Precision**: 16-mixed\n",
    "- **Batch Size**: 8 with gradient accumulation 4 (effective: 32)\n",
    "\n",
    "## Data Requirements\n",
    "- Parquet files: `veterans_history_project_resources_pre2010_train.parquet` and `_val.parquet`\n",
    "- Audio files must be pre-downloaded to local directory (NeMo requires local paths)\n",
    "\n",
    "## Pre-requisite: Download Audio Files\n",
    "Before running this notebook, audio files must be downloaded from Azure blob to local storage.\n",
    "Use the download script or notebook to fetch files to `/workspace/audio/loc_vhp/`.\n",
    "\n",
    "See [learnings/parakeet-nemo-finetuning.md](../learnings/parakeet-nemo-finetuning.md) for gotchas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup Dependencies\n",
    "\n",
    "Fine-tuning with NeMo requires additional packages. Add them via uv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fine-tuning dependencies to pyproject.toml (run once)\n",
    "# uv add nemo_toolkit[asr] pytorch-lightning>=2.0\n",
    "#\n",
    "# For A6000 with CUDA 11.8:\n",
    "# uv add torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for package imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import pytorch_lightning as pl\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.utils import exp_manager\n",
    "\n",
    "# Import project modules for transcript cleaning\n",
    "from scripts.eval.evaluate import clean_raw_transcript_str\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths - VHP parquet files (upload these to RunPod)\n",
    "    \"train_parquet\": \"/workspace/data/veterans_history_project_resources_pre2010_train.parquet\",\n",
    "    \"val_parquet\": \"/workspace/data/veterans_history_project_resources_pre2010_val.parquet\",\n",
    "    \"test_parquet\": \"/workspace/data/veterans_history_project_resources_pre2010_test.parquet\",\n",
    "    \n",
    "    # Audio directory - where audio files are pre-downloaded from Azure blob\n",
    "    # Expected structure: {audio_dir}/{azure_blob_index}/video.mp4 or audio.mp3\n",
    "    # Must download BEFORE running this notebook (NeMo requires local paths)\n",
    "    \"audio_dir\": \"/workspace/audio/loc_vhp\",\n",
    "    \n",
    "    # Random seed for reproducibility\n",
    "    \"random_seed\": 42,\n",
    "    \n",
    "    # NeMo manifest output (will be generated)\n",
    "    \"train_manifest\": \"/workspace/data/train_manifest.json\",\n",
    "    \"val_manifest\": \"/workspace/data/val_manifest.json\",\n",
    "    \n",
    "    # Output directory - follows convention: {dataset}-{model}-{task}-{infra}\n",
    "    \"output_dir\": \"/workspace/outputs/vhp-pre2010-parakeet-tdt-0.6b-ft-a6000\",\n",
    "    \"exp_name\": \"parakeet_tdt_vhp\",\n",
    "    \n",
    "    # Model\n",
    "    \"model_name\": \"nvidia/parakeet-tdt-0.6b-v3\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"batch_size\": 8,\n",
    "    \"gradient_accumulation\": 4,\n",
    "    \"warmup_steps\": 10000,\n",
    "    \"max_steps\": 100000,\n",
    "    \"val_check_interval\": 1000,\n",
    "    \n",
    "    # Audio settings\n",
    "    \"sample_rate\": 16000,\n",
    "    \"max_duration\": 30,   # Max audio duration per sample (seconds)\n",
    "    \"min_duration\": 0.1,\n",
    "    \n",
    "    # Precision\n",
    "    \"precision\": \"16-mixed\",\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "os.makedirs(os.path.dirname(CONFIG[\"train_manifest\"]), exist_ok=True)\n",
    "print(f\"Output directory: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Create NeMo Manifests from Parquet\n",
    "\n",
    "NeMo requires JSON lines manifest format:\n",
    "```json\n",
    "{\"audio_filepath\": \"/path/to/audio.wav\", \"text\": \"transcription\", \"duration\": 15.4}\n",
    "```\n",
    "\n",
    "We convert VHP parquet files to this format, using:\n",
    "- `fulltext_file_str` column with `clean_raw_transcript_str()` for ground truth (same as Whisper notebook)\n",
    "- `azure_blob_index` column to map to pre-downloaded audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_audio_file(audio_dir: str, blob_idx: int) -> str:\n",
    "    \"\"\"\n",
    "    Find audio file for a given azure_blob_index.\n",
    "    \n",
    "    Matches the structure from Azure blob download:\n",
    "    {audio_dir}/{blob_idx}/video.mp4 or audio.mp3\n",
    "    \"\"\"\n",
    "    base_path = Path(audio_dir) / str(blob_idx)\n",
    "    \n",
    "    # Priority order (matches upload script preference)\n",
    "    candidates = [\n",
    "        base_path / \"video.mp4\",\n",
    "        base_path / \"audio.mp3\",\n",
    "        base_path / \"audio.wav\",\n",
    "        base_path / \"video.mp3\",\n",
    "    ]\n",
    "    \n",
    "    for path in candidates:\n",
    "        if path.exists():\n",
    "            return str(path)\n",
    "    \n",
    "    # Fallback: any audio/video file in directory\n",
    "    if base_path.exists():\n",
    "        for ext in ['*.mp4', '*.mp3', '*.wav', '*.m4a']:\n",
    "            files = list(base_path.glob(ext))\n",
    "            if files:\n",
    "                return str(files[0])\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def create_nemo_manifest(parquet_path: str, audio_dir: str, output_path: str, max_duration_sec: int = 1440):\n",
    "    \"\"\"\n",
    "    Convert VHP parquet to NeMo manifest format.\n",
    "    \n",
    "    Uses:\n",
    "    - fulltext_file_str column with clean_raw_transcript_str() for ground truth\n",
    "    - azure_blob_index column to find pre-downloaded audio files\n",
    "    \n",
    "    Only includes samples where audio duration <= max_duration_sec (for proper alignment).\n",
    "    Files longer than this are SKIPPED because we don't have timestamped transcripts.\n",
    "    \n",
    "    Args:\n",
    "        parquet_path: Path to parquet file\n",
    "        audio_dir: Directory with pre-downloaded audio files\n",
    "        output_path: Path to write NeMo manifest\n",
    "        max_duration_sec: Maximum audio duration in seconds (default 1440 = 24 min for Parakeet).\n",
    "                          Parakeet can handle up to 24 min on A100 80GB with full attention.\n",
    "    \n",
    "    NeMo manifest is JSON lines with: audio_filepath, text, duration\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    print(f\"Loaded {len(df)} rows from {parquet_path}\")\n",
    "    \n",
    "    entries = []\n",
    "    missing_audio = 0\n",
    "    empty_transcript = 0\n",
    "    skipped_too_long = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Get blob index for audio file lookup\n",
    "        blob_idx = row.get('azure_blob_index', row.get('original_parquet_index', idx))\n",
    "        audio_path = find_audio_file(audio_dir, blob_idx)\n",
    "        \n",
    "        if audio_path is None:\n",
    "            missing_audio += 1\n",
    "            continue\n",
    "        \n",
    "        # Clean transcript using existing evaluate.py function (same as Whisper notebook)\n",
    "        raw_transcript = row.get('fulltext_file_str', '')\n",
    "        cleaned_transcript = clean_raw_transcript_str(raw_transcript)\n",
    "        \n",
    "        if not cleaned_transcript.strip():\n",
    "            empty_transcript += 1\n",
    "            continue\n",
    "        \n",
    "        # Get audio duration\n",
    "        try:\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            duration = librosa.get_duration(y=y, sr=sr)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {audio_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Skip if audio is too long (no proper alignment possible without timestamps)\n",
    "        if duration > max_duration_sec:\n",
    "            print(f\"  Skipping {idx}: audio too long ({duration:.1f}s > {max_duration_sec}s)\")\n",
    "            skipped_too_long += 1\n",
    "            continue\n",
    "        \n",
    "        # NeMo expects lowercase text\n",
    "        entries.append({\n",
    "            \"audio_filepath\": audio_path,\n",
    "            \"text\": cleaned_transcript.lower(),\n",
    "            \"duration\": round(duration, 2)\n",
    "        })\n",
    "        \n",
    "        if len(entries) % 100 == 0:\n",
    "            print(f\"  Processed {len(entries)} entries...\")\n",
    "    \n",
    "    # Write manifest\n",
    "    with open(output_path, 'w') as f:\n",
    "        for entry in entries:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "    \n",
    "    total_hours = sum(e['duration'] for e in entries) / 3600\n",
    "    print(f\"\\nCreated manifest: {output_path}\")\n",
    "    print(f\"  Valid entries: {len(entries)}\")\n",
    "    print(f\"  Missing audio: {missing_audio}\")\n",
    "    print(f\"  Empty transcripts: {empty_transcript}\")\n",
    "    print(f\"  Skipped (too long): {skipped_too_long}\")\n",
    "    print(f\"  Total hours: {total_hours:.1f}\")\n",
    "    \n",
    "    if len(entries) == 0:\n",
    "        raise ValueError(f\"No samples found with duration <= {max_duration_sec}s. \"\n",
    "                         \"VHP files are typically 30-60+ minute interviews. \"\n",
    "                         \"Consider using forced alignment to create shorter segments.\")\n",
    "    \n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training manifest\n",
    "print(\"Creating training manifest...\")\n",
    "train_entries = create_nemo_manifest(\n",
    "    CONFIG[\"train_parquet\"],\n",
    "    CONFIG[\"audio_dir\"],\n",
    "    CONFIG[\"train_manifest\"]\n",
    ")\n",
    "\n",
    "print(\"\\nCreating validation manifest...\")\n",
    "val_entries = create_nemo_manifest(\n",
    "    CONFIG[\"val_parquet\"],\n",
    "    CONFIG[\"audio_dir\"],\n",
    "    CONFIG[\"val_manifest\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_manifest(manifest_path):\n",
    "    \"\"\"Validate manifest and print statistics.\"\"\"\n",
    "    entries = []\n",
    "    with open(manifest_path, 'r') as f:\n",
    "        for line in f:\n",
    "            entries.append(json.loads(line))\n",
    "    \n",
    "    total_duration = sum(e['duration'] for e in entries)\n",
    "    avg_duration = total_duration / len(entries) if entries else 0\n",
    "    \n",
    "    print(f\"{manifest_path}:\")\n",
    "    print(f\"  Entries: {len(entries)}\")\n",
    "    print(f\"  Total: {total_duration/3600:.1f} hours\")\n",
    "    print(f\"  Avg duration: {avg_duration:.1f}s\")\n",
    "    return len(entries), total_duration\n",
    "\n",
    "print(\"Validating manifests...\")\n",
    "validate_manifest(CONFIG[\"train_manifest\"])\n",
    "validate_manifest(CONFIG[\"val_manifest\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a sample from manifest\n",
    "import random\n",
    "random.seed(CONFIG[\"random_seed\"])\n",
    "\n",
    "with open(CONFIG[\"train_manifest\"], 'r') as f:\n",
    "    manifest_lines = f.readlines()\n",
    "\n",
    "sample_entry = json.loads(random.choice(manifest_lines))\n",
    "print(\"Sample manifest entry:\")\n",
    "print(f\"  Audio: {sample_entry['audio_filepath']}\")\n",
    "print(f\"  Duration: {sample_entry['duration']:.1f}s\")\n",
    "print(f\"  Text (first 200 chars): {sample_entry['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 4. Load Model\n",
    "\n",
    "### Why Parakeet-TDT?\n",
    "\n",
    "**Parakeet-TDT (Token-and-Duration Transducer)** is NVIDIA's state-of-the-art ASR model with several advantages:\n",
    "\n",
    "1. **Streaming-capable**: TDT architecture supports both streaming and offline transcription\n",
    "2. **Fast inference**: Optimized for NVIDIA GPUs with TensorRT support\n",
    "3. **Strong baseline**: Competitive with Whisper large-v3 on benchmarks\n",
    "4. **NeMo integration**: Full fine-tuning support with NeMo toolkit\n",
    "\n",
    "**References:**\n",
    "- [Parakeet-TDT-0.6B-v3 on HuggingFace](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3)\n",
    "- [NeMo ASR Documentation](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html)\n",
    "- [Turbocharge ASR with Parakeet-TDT](https://developer.nvidia.com/blog/turbocharge-asr-accuracy-and-speed-with-nvidia-nemo-parakeet-tdt/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Loading model: {CONFIG['model_name']}\")\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(CONFIG[\"model_name\"])\n",
    "\n",
    "num_params = sum(p.numel() for p in asr_model.parameters())\n",
    "print(f\"Model loaded: {num_params / 1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 5. Configure Training\n",
    "\n",
    "### Full Fine-tuning vs LoRA\n",
    "\n",
    "Unlike Whisper (where we used LoRA), we do **full fine-tuning** for Parakeet because:\n",
    "\n",
    "1. **Smaller model**: Parakeet-TDT-0.6B has 600M params (vs Whisper's 1.5B), fits in A6000 memory\n",
    "2. **NeMo optimization**: NeMo's training pipeline is highly optimized for full fine-tuning\n",
    "3. **TDT architecture**: The transducer head benefits from end-to-end fine-tuning\n",
    "\n",
    "**Note**: If you encounter OOM issues, reduce `batch_size` or `max_duration`, or use gradient checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data loaders\n",
    "train_ds_cfg = DictConfig({\n",
    "    \"manifest_filepath\": CONFIG[\"train_manifest\"],\n",
    "    \"sample_rate\": CONFIG[\"sample_rate\"],\n",
    "    \"batch_size\": CONFIG[\"batch_size\"],\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 4,\n",
    "    \"pin_memory\": True,\n",
    "    \"max_duration\": CONFIG[\"max_duration\"],\n",
    "    \"min_duration\": CONFIG[\"min_duration\"],\n",
    "})\n",
    "\n",
    "val_ds_cfg = DictConfig({\n",
    "    \"manifest_filepath\": CONFIG[\"val_manifest\"],\n",
    "    \"sample_rate\": CONFIG[\"sample_rate\"],\n",
    "    \"batch_size\": 16,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 4,\n",
    "    \"pin_memory\": True,\n",
    "    \"max_duration\": CONFIG[\"max_duration\"],\n",
    "    \"min_duration\": CONFIG[\"min_duration\"],\n",
    "})\n",
    "\n",
    "asr_model.setup_training_data(train_ds_cfg)\n",
    "asr_model.setup_validation_data(val_ds_cfg)\n",
    "print(\"Data loaders ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure optimizer\n",
    "optim_cfg = DictConfig({\n",
    "    \"name\": \"adamw\",\n",
    "    \"lr\": CONFIG[\"learning_rate\"],\n",
    "    \"betas\": [0.9, 0.999],\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"sched\": {\n",
    "        \"name\": \"CosineAnnealing\",\n",
    "        \"warmup_steps\": CONFIG[\"warmup_steps\"],\n",
    "        \"max_steps\": CONFIG[\"max_steps\"],\n",
    "    }\n",
    "})\n",
    "\n",
    "asr_model.setup_optimization(optim_cfg)\n",
    "print(f\"Optimizer: lr={CONFIG['learning_rate']}, warmup={CONFIG['warmup_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 6. Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    max_steps=CONFIG[\"max_steps\"],\n",
    "    val_check_interval=CONFIG[\"val_check_interval\"],\n",
    "    log_every_n_steps=50,\n",
    "    enable_checkpointing=True,\n",
    "    precision=CONFIG[\"precision\"],\n",
    "    gradient_clip_val=1.0,\n",
    "    accumulate_grad_batches=CONFIG[\"gradient_accumulation\"],\n",
    ")\n",
    "\n",
    "exp_manager_cfg = DictConfig({\n",
    "    \"exp_dir\": CONFIG[\"output_dir\"],\n",
    "    \"name\": CONFIG[\"exp_name\"],\n",
    "    \"create_tensorboard_logger\": True,\n",
    "    \"resume_if_exists\": True,\n",
    "    \"checkpoint_callback_params\": {\n",
    "        \"monitor\": \"val_wer\",\n",
    "        \"mode\": \"min\",\n",
    "        \"save_top_k\": 3,\n",
    "        \"save_last\": True,\n",
    "    }\n",
    "})\n",
    "\n",
    "exp_manager(trainer, exp_manager_cfg)\n",
    "print(f\"Trainer ready. Effective batch: {CONFIG['batch_size'] * CONFIG['gradient_accumulation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 7. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Batch: {CONFIG['batch_size']} x {CONFIG['gradient_accumulation']} = {CONFIG['batch_size'] * CONFIG['gradient_accumulation']}\")\n",
    "print(f\"Max steps: {CONFIG['max_steps']}\")\n",
    "print(f\"Precision: {CONFIG['precision']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(asr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(CONFIG[\"output_dir\"], \"parakeet_tdt_finetuned.nemo\")\n",
    "asr_model.save_to(model_path)\n",
    "print(f\"Model saved: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"=\"*60)\n",
    "print(\"FINE-TUNING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {CONFIG['model_name']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Train parquet: {CONFIG['train_parquet']}\")\n",
    "print(f\"  Val parquet: {CONFIG['val_parquet']}\")\n",
    "print(f\"\\nSaved to: {model_path}\")\n",
    "print(f\"\\nTo load: nemo_asr.models.ASRModel.restore_from('{model_path}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 9. Test Inference\n",
    "\n",
    "Quick test of the fine-tuned model on samples from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model\n",
    "finetuned_model = nemo_asr.models.ASRModel.restore_from(model_path)\n",
    "finetuned_model.eval()\n",
    "print(f\"Loaded fine-tuned model from: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on samples from test parquet\n",
    "test_results = []\n",
    "\n",
    "if os.path.exists(CONFIG[\"test_parquet\"]):\n",
    "    test_df = pd.read_parquet(CONFIG[\"test_parquet\"])\n",
    "    print(f\"Test parquet: {len(test_df)} rows\")\n",
    "    \n",
    "    # Test on first 10 samples\n",
    "    for idx, row in test_df.head(10).iterrows():\n",
    "        blob_idx = row.get('azure_blob_index', row.get('original_parquet_index', idx))\n",
    "        audio_path = find_audio_file(CONFIG[\"audio_dir\"], blob_idx)\n",
    "        \n",
    "        if audio_path is None:\n",
    "            print(f\"  [{idx}] Audio not found for blob_idx={blob_idx}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  [{idx}] Processing: {audio_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Run inference\n",
    "            result = finetuned_model.transcribe([audio_path])\n",
    "            hypothesis = result[0] if result else \"\"\n",
    "            \n",
    "            # Get ground truth\n",
    "            gt = clean_raw_transcript_str(row.get('fulltext_file_str', ''))\n",
    "            \n",
    "            test_results.append({\n",
    "                \"file_id\": idx,\n",
    "                \"blob_idx\": blob_idx,\n",
    "                \"hypothesis\": hypothesis,\n",
    "                \"ground_truth\": gt,\n",
    "                \"audio_path\": audio_path\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nCompleted: {len(test_results)} files\")\n",
    "else:\n",
    "    print(f\"Test parquet not found: {CONFIG['test_parquet']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View test results\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST INFERENCE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for r in test_results[:3]:  # Show first 3\n",
    "    print(f\"\\nFile ID: {r['file_id']} (blob_idx: {r['blob_idx']})\")\n",
    "    print(f\"Audio: {r['audio_path']}\")\n",
    "    print(f\"\\nHypothesis (first 300 chars):\")\n",
    "    print(r['hypothesis'][:300] + \"...\" if len(r['hypothesis']) > 300 else r['hypothesis'])\n",
    "    print(f\"\\nGround truth (first 300 chars):\")\n",
    "    print(r['ground_truth'][:300] + \"...\" if len(r['ground_truth']) > 300 else r['ground_truth'])\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Save test results\n",
    "if test_results:\n",
    "    test_output_dir = Path(CONFIG[\"output_dir\"]) / \"test-inference\"\n",
    "    test_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    df_test_results = pd.DataFrame(test_results)\n",
    "    df_test_results.to_parquet(test_output_dir / \"test_results.parquet\", index=False)\n",
    "    print(f\"\\nTest results saved to: {test_output_dir / 'test_results.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "To run full evaluation on the test set:\n",
    "\n",
    "1. **Create inference config** for the fine-tuned model:\n",
    "   ```yaml\n",
    "   experiment_id: vhp-pre2010-parakeet-tdt-finetuned-test\n",
    "   model:\n",
    "     name: \"parakeet-tdt-finetuned\"\n",
    "     nemo_path: \"/workspace/outputs/vhp-pre2010-parakeet-tdt-0.6b-ft-a6000/parakeet_tdt_finetuned.nemo\"\n",
    "     device: \"cuda\"\n",
    "   input:\n",
    "     parquet_path: \"data/raw/loc/veterans_history_project_resources_pre2010_test.parquet\"\n",
    "     audio_dir: \"/workspace/audio/loc_vhp\"\n",
    "   output:\n",
    "     dir: \"outputs/vhp-pre2010-parakeet-tdt-finetuned-test\"\n",
    "   ```\n",
    "\n",
    "2. **Run evaluation** with `scripts/evaluate.py` to compute WER metrics\n",
    "\n",
    "3. **Compare** with baseline Parakeet (pre-fine-tuning) results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
