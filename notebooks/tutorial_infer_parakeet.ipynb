{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: NVIDIA Parakeet-TDT-0.6B-v3 Inference Pipeline\n",
    "\n",
    "This notebook demonstrates how to run the NVIDIA Parakeet-TDT-0.6B-v3 inference pipeline on a single sample file.\n",
    "\n",
    "**Model**: [nvidia/parakeet-tdt-0.6b-v3](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3) (600M params, 25 languages, 6.32% WER, **fastest multilingual ASR**)\n",
    "\n",
    "**What this notebook does:**\n",
    "- Uses production orchestration (Azure blob storage)\n",
    "- Runs Parakeet-TDT-0.6B-v3 model (fastest multilingual ASR on HF leaderboard)\n",
    "- Processes just 1 sample file for testing\n",
    "- Shows outputs (inference results, hypothesis text)\n",
    "\n",
    "**Why Parakeet?**\n",
    "- **4.4x faster than Canary-1B-v2** (RTFx: 3332.74 vs 749)\n",
    "- **Better English WER**: 6.32% vs 8.40%\n",
    "- **Smaller model**: 600M params vs 978M\n",
    "- **Auto language detection**: No lang params needed\n",
    "- **Rich timestamps**: Word/segment/char-level (optional)\n",
    "\n",
    "**Requirements:**\n",
    "- GPU: ~4 GB VRAM (less than Canary-1B)\n",
    "- RAM: ~2-3 GB system memory\n",
    "- Azure credentials configured (for blob storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Import and Configure Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../credentials/creds.env')\n",
    "\n",
    "# Disable wandb for notebook runs (tutorials don't need tracking)\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "# Set HuggingFace cache to use local models directory (avoid re-downloading)\n",
    "os.environ['HF_HOME'] = str(Path.cwd().parent / \"models/parakeet\")\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(Path.cwd().parent / \"models/parakeet\")\n",
    "\n",
    "# Add scripts directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"scripts\"))\n",
    "\n",
    "from infer_parakeet import run\n",
    "\n",
    "# System and GPU Memory Check\n",
    "print(\"=\" * 70)\n",
    "print(\"Memory Status Check\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check system RAM\n",
    "try:\n",
    "    import psutil\n",
    "    mem = psutil.virtual_memory()\n",
    "    ram_used = mem.used / 1024**3\n",
    "    ram_total = mem.total / 1024**3\n",
    "    ram_available = mem.available / 1024**3\n",
    "    \n",
    "    print(f\"System RAM: {ram_used:.1f}/{ram_total:.1f} GB used\")\n",
    "    print(f\"Available: {ram_available:.1f} GB\")\n",
    "    \n",
    "    if ram_available < 3:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Only {ram_available:.1f} GB RAM available\")\n",
    "        print(f\"   Parakeet needs ~2-3 GB free for model loading\")\n",
    "except ImportError:\n",
    "    print(\"psutil not installed, skipping RAM check\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print()\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        free = total - reserved\n",
    "        \n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Total: {total:.2f} GB\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  Free: {free:.2f} GB\")\n",
    "        \n",
    "        if free < 4.0:\n",
    "            print(f\"  ‚ö†Ô∏è  WARNING: Only {free:.2f} GB GPU free\")\n",
    "            print(f\"     Parakeet needs ~4 GB GPU VRAM\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No CUDA GPU available - Parakeet requires GPU!\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüí° Note: Wandb logging is disabled for tutorial runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 GPU Memory Cleanup (Run if needed)\n",
    "\n",
    "If you see \"CUDA out of memory\" errors, run this cell to clear GPU memory from previous runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Force Python garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úì GPU memory cleared\")\n",
    "\n",
    "# Show updated memory status\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        free = total - reserved\n",
    "        print(f\"GPU {i}: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved, {free:.2f} GB free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Configure Inference Parameters\n",
    "\n",
    "Set parameters directly in Python (no yaml file needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build configuration dictionary\n",
    "cfg = {\n",
    "    \"experiment_id\": \"tutorial-parakeet-sample1\",\n",
    "    \n",
    "    # Model settings\n",
    "    \"model\": {\n",
    "        \"name\": \"parakeet-tdt-0.6b-v3\",\n",
    "        \"dir\": \"nvidia/parakeet-tdt-0.6b-v3\",  # HuggingFace model ID\n",
    "        \"device\": \"cuda\",  # Parakeet requires GPU\n",
    "        \"enable_timestamps\": False,  # Set True for word/segment/char timestamps\n",
    "        \"use_local_attention\": False,  # Set True for very long audio (>24 min, up to 3 hrs)\n",
    "        \"att_context_size\": [256, 256]  # Context size for local attention\n",
    "    },\n",
    "    \n",
    "    # Input settings\n",
    "    \"input\": {\n",
    "        \"source\": \"azure_blob\",\n",
    "        \"parquet_path\": \"../data/raw/loc/veterans_history_project_resources_pre2010.parquet\",\n",
    "        \"blob_prefix\": \"loc_vhp\",\n",
    "        \"sample_size\": 1,  # Just 1 file for tutorial\n",
    "        \"duration_sec\": 300,  # First 5 minutes only (faster)\n",
    "        \"sample_rate\": 16000  # Parakeet expects 16kHz\n",
    "    },\n",
    "    \n",
    "    # Output settings\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/tutorial-parakeet-sample1\",\n",
    "        \"save_per_file\": True  # Save individual hypothesis files\n",
    "    },\n",
    "    \n",
    "    # Evaluation (optional)\n",
    "    \"evaluation\": {\n",
    "        \"use_whisper_normalizer\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {cfg['model']['dir']}\")\n",
    "print(f\"  Features: Fastest multilingual ASR (RTFx: 3332.74)\")\n",
    "print(f\"  Languages: 25 European languages (auto-detected)\")\n",
    "print(f\"  WER: 6.32% (English), 9.7% (multilingual avg)\")\n",
    "print(f\"  Memory: ~2-3 GB RAM, ~4 GB GPU (smallest NVIDIA model)\")\n",
    "print(f\"  Device: {cfg['model']['device']} (GPU required)\")\n",
    "print(f\"  Sample size: {cfg['input']['sample_size']} file(s)\")\n",
    "print(f\"  Duration: {cfg['input']['duration_sec']}s (first 5 minutes)\")\n",
    "print(f\"  Output: {cfg['output']['dir']}\")\n",
    "print(f\"\\nüí° Advantages over Canary-1B-v2:\")\n",
    "print(f\"   - 4.4x faster (RTFx: 3332 vs 749)\")\n",
    "print(f\"   - Better English WER (6.32% vs 8.40%)\")\n",
    "print(f\"   - Smaller model (600M vs 978M params)\")\n",
    "print(f\"   - Auto language detection (no lang params needed)\")\n",
    "print(f\"\\nüí° Tip: First run will download model (~1.2GB). Subsequent runs use cached model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. (optional) Advanced: Testing Local Attention for Long Audio\n",
    "\n",
    "**Local Attention** allows Parakeet to process audio longer than 24 minutes (up to 3 hours).\n",
    "\n",
    "### What is Local Attention?\n",
    "\n",
    "- **NOT traditional chunking** - it's an architectural change to the attention mechanism\n",
    "- Each audio frame only attends to nearby frames (sliding window) instead of all frames\n",
    "- **Single forward pass** through the model (not multiple inference calls)\n",
    "- Memory usage: O(n) instead of O(n¬≤)\n",
    "- Completely handled by NeMo internally (blackbox)\n",
    "\n",
    "### When to Use:\n",
    "- ‚úÖ Audio >24 minutes (up to 3 hours)\n",
    "- ‚ùå Audio <24 minutes (use default full attention - better quality)\n",
    "\n",
    "**Important**: This is different from the manual chunking we use for Canary-Qwen. Local attention is a model-level setting that enables processing longer audio in a single `transcribe()` call.\n",
    "\n",
    "**For very long audio (>24 minutes):**\n",
    "```python\n",
    "cfg['model']['use_local_attention'] = True  # Enables processing up to 3 hours\n",
    "cfg['model']['att_context_size'] = [256, 256]  # Attention context size\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['model']['use_local_attention'] = True  # Enables processing up to 3 hours\n",
    "cfg['model']['att_context_size'] = [256, 256]  # Attention context size\n",
    "cfg['input']['duration_sec'] = None # ‚úÖ FULL AUDIO - test with complete file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Inference\n",
    "\n",
    "This will:\n",
    "1. Load 1 sample from the parquet file\n",
    "2. Download audio from Azure blob storage\n",
    "3. Preprocess audio (trim to 300s, convert to 16kHz mono WAV)\n",
    "4. Load Parakeet-TDT-0.6B-v3 model (~2-3GB RAM, ~4GB GPU)\n",
    "5. Run transcription with auto language detection\n",
    "6. Save results\n",
    "\n",
    "**Note**: First run will download the model (~1.2GB). Cached after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference pipeline\n",
    "result = run(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View Results\n",
    "\n",
    "### 4.1 Inference Results (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results parquet\n",
    "results_path = Path(cfg['output']['dir']) / \"inference_results.parquet\"\n",
    "df_results = pd.read_parquet(results_path)\n",
    "\n",
    "print(f\"Results shape: {df_results.shape}\")\n",
    "print(f\"\\nColumns: {list(df_results.columns)}\")\n",
    "print(f\"\\nFirst row:\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hypothesis Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the transcription\n",
    "row = df_results.iloc[0]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"File ID: {row['file_id']}\")\n",
    "print(f\"Collection: {row['collection_number']}\")\n",
    "print(f\"Duration: {row['duration_sec']:.1f}s\")\n",
    "print(f\"Processing time: {row['processing_time_sec']:.1f}s\")\n",
    "print(f\"Status: {row['status']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTranscription (hypothesis):\\n\")\n",
    "print(row['hypothesis'][:500] + \"...\" if len(row['hypothesis']) > 500 else row['hypothesis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Individual Hypothesis File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check individual hypothesis file\n",
    "hyp_file = Path(cfg['output']['dir']) / f\"hyp_{row['file_id']}.txt\"\n",
    "\n",
    "if hyp_file.exists():\n",
    "    print(f\"Hypothesis file: {hyp_file}\")\n",
    "    print(f\"\\nContent:\\n\")\n",
    "    print(hyp_file.read_text()[:500] + \"...\" if len(hyp_file.read_text()) > 500 else hyp_file.read_text())\n",
    "else:\n",
    "    print(f\"Hypothesis file not found: {hyp_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding the Output\n",
    "\n",
    "**Key output files:**\n",
    "- `inference_results.parquet`: All results in structured format (hypothesis, duration, status, etc.)\n",
    "- `hyp_{file_id}.txt`: Individual hypothesis text files (one per audio file)\n",
    "- `hyp_parakeet.txt`: Combined hypothesis text (all transcriptions concatenated)\n",
    "- `parakeet_log_*.txt`: Detailed inference log\n",
    "\n",
    "**Key result columns:**\n",
    "- `file_id`: Unique identifier\n",
    "- `collection_number`: VHP collection number\n",
    "- `hypothesis`: Model transcription output\n",
    "- `ground_truth`: Reference transcript (for evaluation)\n",
    "- `duration_sec`: Audio duration processed\n",
    "- `processing_time_sec`: Time taken for inference\n",
    "- `status`: success/error\n",
    "- `model_name`: Model identifier\n",
    "\n",
    "**Model used:**\n",
    "- **[nvidia/parakeet-tdt-0.6b-v3](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3)**: 600M params, 25 languages\n",
    "- **Fastest multilingual ASR** on HuggingFace leaderboard (RTFx: 3332.74)\n",
    "- **Better English WER** than Canary-1B-v2 (6.32% vs 8.40%)\n",
    "- Trained on 1.7M hours of multilingual audio (Granary dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding Parakeet-TDT-0.6B-v3\n",
    "\n",
    "**Model**: [nvidia/parakeet-tdt-0.6b-v3](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3)\n",
    "\n",
    "Parakeet-TDT-0.6B-v3 is a **Fast-Conformer TDT (Transducer)** model optimized for speed and efficiency.\n",
    "\n",
    "**Key features:**\n",
    "- 600M parameters (smallest NVIDIA multilingual ASR model)\n",
    "- FastConformer-TDT architecture (Transducer decoder)\n",
    "- **Fastest multilingual ASR on HuggingFace leaderboard** (RTFx: 3332.74)\n",
    "- 25 European languages with auto language detection\n",
    "- 6.32% WER on English (HF leaderboard)\n",
    "- 9.7% WER on multilingual average (24 languages)\n",
    "- Trained on ~660,000 hours from Granary ASR subset\n",
    "- Auto punctuation and capitalization\n",
    "- Word/segment/char-level timestamps (optional)\n",
    "- Supports up to 24 min audio (full attention) or 3 hrs (local attention)\n",
    "\n",
    "**Memory requirements (tested on Tesla T4):**\n",
    "- GPU: ~4 GB VRAM (smallest NVIDIA model)\n",
    "- RAM: ~2-3 GB system memory\n",
    "- Fits comfortably on T4 16GB GPU\n",
    "\n",
    "**Performance:**\n",
    "- **4.4x faster than Canary-1B-v2** (RTFx: 3332.74 vs 749)\n",
    "- **Better English WER** than Canary-1B-v2 (6.32% vs 8.40%)\n",
    "- Lower memory footprint (600M vs 978M params)\n",
    "- Slightly lower multilingual accuracy vs Canary-1B-v2 (9.7% vs 8.1% WER)\n",
    "\n",
    "**Comparison with NVIDIA models:**\n",
    "\n",
    "| Aspect | Parakeet-TDT-0.6B-v3 | Canary-1B-v2 | Canary-Qwen-2.5B |\n",
    "|--------|---------------------|--------------|------------------|\n",
    "| **HuggingFace** | [nvidia/parakeet-tdt-0.6b-v3](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3) | [nvidia/canary-1b-v2](https://huggingface.co/nvidia/canary-1b-v2) | [nvidia/canary-qwen-2.5b](https://huggingface.co/nvidia/canary-qwen-2.5b) |\n",
    "| **Architecture** | FastConformer-TDT | FastConformer-CTC | SALM (ASR+LLM) |\n",
    "| **Parameters** | 600M | 978M | 2.5B |\n",
    "| **Speed (RTFx)** | **3332.74** (FASTEST) | 749 | ~60-100 |\n",
    "| **English WER** | **6.32%** (BEST) | 8.40% | **5.63%** (BEST) |\n",
    "| **Multilingual WER** | 9.7% | **8.1%** (BEST) | Better |\n",
    "| **GPU Memory** | **~4 GB** (SMALLEST) | ~6 GB | ~10 GB |\n",
    "| **Timestamps** | ‚úÖ Word/segment/char | ‚úÖ Yes | ‚ùå No |\n",
    "| **Punctuation** | ‚úÖ Auto | Basic | ‚úÖ Full (LLM) |\n",
    "| **Language Detection** | ‚úÖ Auto | Manual params | Auto |\n",
    "| **Method** | `transcribe()` | `transcribe()` | `generate()` |\n",
    "| **Use Case** | **High-throughput, real-time** | Balanced speed/accuracy | Best accuracy |\n",
    "\n",
    "**When to use Parakeet:**\n",
    "- ‚úÖ Need fastest processing (4.4x faster than Canary-1B)\n",
    "- ‚úÖ High-throughput scenarios (transcribing large volumes)\n",
    "- ‚úÖ Real-time transcription requirements\n",
    "- ‚úÖ Limited GPU memory (~4 GB)\n",
    "- ‚úÖ English-primary workloads (best English WER among non-LLM models)\n",
    "- ‚úÖ Need timestamps (word/segment/char level)\n",
    "- ‚ùå Need absolute best multilingual accuracy (use Canary-1B-v2 instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "To run on more files:\n",
    "1. Increase `sample_size` (e.g., 10, 50, 500)\n",
    "2. Set `duration_sec: None` for full audio\n",
    "3. Enable `enable_timestamps: True` for word-level timestamps (if needed)\n",
    "4. Set `use_local_attention: True` for very long audio (>24 min, up to 3 hrs)\n",
    "\n",
    "To evaluate results:\n",
    "```python\n",
    "# This will be available after implementing evaluation\n",
    "from scripts.evaluate import evaluate_results\n",
    "metrics = evaluate_results(df_results, use_whisper_normalizer=True)\n",
    "print(f\"WER: {metrics['wer']:.2%}\")\n",
    "```\n",
    "\n",
    "**Recommended config for 10-sample test:**\n",
    "```python\n",
    "cfg = {\n",
    "    \"experiment_id\": \"vhp-parakeet-sample10\",\n",
    "    \"model\": {\n",
    "        \"name\": \"parakeet-tdt-0.6b-v3\",\n",
    "        \"dir\": \"nvidia/parakeet-tdt-0.6b-v3\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"enable_timestamps\": False,\n",
    "        \"use_local_attention\": False  # Full attention for <24 min audio\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"sample_size\": 10,  # Test with 10 files\n",
    "        \"duration_sec\": None  # Full audio\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/vhp-parakeet-sample10\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Recommended config for 500-sample benchmark:**\n",
    "```python\n",
    "cfg = {\n",
    "    \"experiment_id\": \"vhp-parakeet-500\",\n",
    "    \"model\": {\n",
    "        \"name\": \"parakeet-tdt-0.6b-v3\",\n",
    "        \"dir\": \"nvidia/parakeet-tdt-0.6b-v3\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"enable_timestamps\": False,\n",
    "        \"use_local_attention\": False  # Most VHP files <24 min\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"sample_size\": 500,  # Full benchmark\n",
    "        \"duration_sec\": None  # Full audio\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/vhp-parakeet-500\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**For very long audio (>24 minutes, up to 3 hours):**\n",
    "```python\n",
    "cfg = {\n",
    "    \"experiment_id\": \"vhp-parakeet-long-audio\",\n",
    "    \"model\": {\n",
    "        \"name\": \"parakeet-tdt-0.6b-v3\",\n",
    "        \"dir\": \"nvidia/parakeet-tdt-0.6b-v3\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"enable_timestamps\": False,\n",
    "        \"use_local_attention\": True,  # ‚úÖ Enable for >24 min\n",
    "        \"att_context_size\": [256, 256]  # Default window size\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"sample_size\": 10,\n",
    "        \"duration_sec\": None  # Full audio (will handle up to 3 hours)\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/vhp-parakeet-long-audio\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Configuration Options:**\n",
    "\n",
    "| Config Parameter | Values | When to Use |\n",
    "|-----------------|--------|-------------|\n",
    "| `use_local_attention` | `false` (default) | Audio <24 minutes (better quality) |\n",
    "|  | `true` | Audio 24 min - 3 hours (enables longer processing) |\n",
    "| `att_context_size` | `[256, 256]` (default) | Standard attention window (~13 sec) |\n",
    "|  | `[128, 128]` | Smaller window (faster, less context) |\n",
    "|  | `[512, 512]` | Larger window (slower, more context) |\n",
    "| `enable_timestamps` | `false` (default) | Don't need word-level timestamps |\n",
    "|  | `true` | Need word/segment/char timestamps |\n",
    "\n",
    "**See Also:**\n",
    "- `learnings/nvidia-models-comparison.md` - Full model comparison\n",
    "- `learnings/nvidia-models-chunking-mechanisms.md` - Understanding local attention vs chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Configure for Local Attention\n",
    "\n",
    "Let's create a new config with local attention enabled to process longer audio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Output\n",
    "\n",
    "**Key output files:**\n",
    "- `inference_results.parquet`: All results in structured format (hypothesis, duration, status, etc.)\n",
    "- `hyp_{file_id}.txt`: Individual hypothesis text files (one per audio file)\n",
    "- `hyp_parakeet.txt`: Combined hypothesis text (all transcriptions concatenated)\n",
    "- `parakeet_log_*.txt`: Detailed inference log\n",
    "\n",
    "**Key result columns:**\n",
    "- `file_id`: Unique identifier\n",
    "- `collection_number`: VHP collection number\n",
    "- `hypothesis`: Model transcription output\n",
    "- `ground_truth`: Reference transcript (for evaluation)\n",
    "- `duration_sec`: Audio duration processed\n",
    "- `processing_time_sec`: Time taken for inference\n",
    "- `status`: success/error\n",
    "- `model_name`: Model identifier\n",
    "\n",
    "**Model used:**\n",
    "- **[nvidia/parakeet-tdt-0.6b-v3](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3)**: 600M params, 25 languages\n",
    "- **Fastest multilingual ASR** on HuggingFace leaderboard (RTFx: 3332.74)\n",
    "- **Better English WER** than Canary-1B-v2 (6.32% vs 8.40%)\n",
    "- Trained on 1.7M hours of multilingual audio (Granary dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Parakeet-TDT-0.6B-v3\n",
    "\n",
    "**Model**: [nvidia/parakeet-tdt-0.6b-v3](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3)\n",
    "\n",
    "Parakeet-TDT-0.6B-v3 is a **Fast-Conformer TDT (Transducer)** model optimized for speed and efficiency.\n",
    "\n",
    "**Key features:**\n",
    "- 600M parameters (smallest NVIDIA multilingual ASR model)\n",
    "- FastConformer-TDT architecture (Transducer decoder)\n",
    "- **Fastest multilingual ASR on HuggingFace leaderboard** (RTFx: 3332.74)\n",
    "- 25 European languages with auto language detection\n",
    "- 6.32% WER on English (HF leaderboard)\n",
    "- 9.7% WER on multilingual average (24 languages)\n",
    "- Trained on ~660,000 hours from Granary ASR subset\n",
    "- Auto punctuation and capitalization\n",
    "- Word/segment/char-level timestamps (optional)\n",
    "- Supports up to 24 min audio (full attention) or 3 hrs (local attention)\n",
    "\n",
    "**Memory requirements (tested on Tesla T4):**\n",
    "- GPU: ~4 GB VRAM (smallest NVIDIA model)\n",
    "- RAM: ~2-3 GB system memory\n",
    "- Fits comfortably on T4 16GB GPU\n",
    "\n",
    "**Performance:**\n",
    "- **4.4x faster than Canary-1B-v2** (RTFx: 3332.74 vs 749)\n",
    "- **Better English WER** than Canary-1B-v2 (6.32% vs 8.40%)\n",
    "- Lower memory footprint (600M vs 978M params)\n",
    "- Slightly lower multilingual accuracy vs Canary-1B-v2 (9.7% vs 8.1% WER)\n",
    "\n",
    "**Comparison with NVIDIA models:**\n",
    "\n",
    "| Aspect | Parakeet-TDT-0.6B-v3 | Canary-1B-v2 | Canary-Qwen-2.5B |\n",
    "|--------|---------------------|--------------|------------------|\n",
    "| **HuggingFace** | [nvidia/parakeet-tdt-0.6b-v3](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3) | [nvidia/canary-1b-v2](https://huggingface.co/nvidia/canary-1b-v2) | [nvidia/canary-qwen-2.5b](https://huggingface.co/nvidia/canary-qwen-2.5b) |\n",
    "| **Architecture** | FastConformer-TDT | FastConformer-CTC | SALM (ASR+LLM) |\n",
    "| **Parameters** | 600M | 978M | 2.5B |\n",
    "| **Speed (RTFx)** | **3332.74** (FASTEST) | 749 | ~60-100 |\n",
    "| **English WER** | **6.32%** (BEST) | 8.40% | **5.63%** (BEST) |\n",
    "| **Multilingual WER** | 9.7% | **8.1%** (BEST) | Better |\n",
    "| **GPU Memory** | **~4 GB** (SMALLEST) | ~6 GB | ~10 GB |\n",
    "| **Timestamps** | ‚úÖ Word/segment/char | ‚úÖ Yes | ‚ùå No |\n",
    "| **Punctuation** | ‚úÖ Auto | Basic | ‚úÖ Full (LLM) |\n",
    "| **Language Detection** | ‚úÖ Auto | Manual params | Auto |\n",
    "| **Method** | `transcribe()` | `transcribe()` | `generate()` |\n",
    "| **Use Case** | **High-throughput, real-time** | Balanced speed/accuracy | Best accuracy |\n",
    "\n",
    "**When to use Parakeet:**\n",
    "- ‚úÖ Need fastest processing (4.4x faster than Canary-1B)\n",
    "- ‚úÖ High-throughput scenarios (transcribing large volumes)\n",
    "- ‚úÖ Real-time transcription requirements\n",
    "- ‚úÖ Limited GPU memory (~4 GB)\n",
    "- ‚úÖ English-primary workloads (best English WER among non-LLM models)\n",
    "- ‚úÖ Need timestamps (word/segment/char level)\n",
    "- ‚ùå Need absolute best multilingual accuracy (use Canary-1B-v2 instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "To run on more files:\n",
    "1. Increase `sample_size` (e.g., 10, 50, 500)\n",
    "2. Set `duration_sec: None` for full audio\n",
    "3. Enable `enable_timestamps: True` for word-level timestamps (if needed)\n",
    "4. Set `use_local_attention: True` for very long audio (>24 min, up to 3 hrs)\n",
    "\n",
    "To evaluate results:\n",
    "```python\n",
    "# This will be available after implementing evaluation\n",
    "from scripts.evaluate import evaluate_results\n",
    "metrics = evaluate_results(df_results, use_whisper_normalizer=True)\n",
    "print(f\"WER: {metrics['wer']:.2%}\")\n",
    "```\n",
    "\n",
    "**Recommended config for 10-sample test:**\n",
    "```python\n",
    "cfg = {\n",
    "    \"experiment_id\": \"vhp-parakeet-sample10\",\n",
    "    \"model\": {\n",
    "        \"name\": \"parakeet-tdt-0.6b-v3\",\n",
    "        \"dir\": \"nvidia/parakeet-tdt-0.6b-v3\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"enable_timestamps\": False,\n",
    "        \"use_local_attention\": False\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"sample_size\": 10,  # Test with 10 files\n",
    "        \"duration_sec\": None  # Full audio\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/vhp-parakeet-sample10\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Recommended config for 500-sample benchmark:**\n",
    "```python\n",
    "cfg = {\n",
    "    \"experiment_id\": \"vhp-parakeet-500\",\n",
    "    \"model\": {\n",
    "        \"name\": \"parakeet-tdt-0.6b-v3\",\n",
    "        \"dir\": \"nvidia/parakeet-tdt-0.6b-v3\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"enable_timestamps\": False,\n",
    "        \"use_local_attention\": False\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"sample_size\": 500,  # Full benchmark\n",
    "        \"duration_sec\": None  # Full audio\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/vhp-parakeet-500\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
