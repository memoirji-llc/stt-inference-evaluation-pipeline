{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canary-Qwen-2.5B Resource Testing\n",
    "\n",
    "**Purpose:** Measure RAM and GPU memory requirements for Canary-Qwen-2.5B\n",
    "\n",
    "**Test Results:**\n",
    "- Model loading: **5.1 GB RAM, 9.6 GB GPU**\n",
    "- Inference overhead: **+0.01 GB RAM, +0.01 GB GPU** (negligible)\n",
    "- **Total: ~5 GB RAM, ~10 GB GPU** needed\n",
    "\n",
    "**VM Requirements:**\n",
    "- âœ… Fits on NC4as_T4_v3 (28GB RAM, 16GB T4 GPU)\n",
    "- âœ… No need for larger VM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "import torch\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure environment\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "model_cache = Path.cwd().parent / \"models\" / \"canary\"\n",
    "os.environ['HF_HOME'] = str(model_cache)\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(model_cache)\n",
    "\n",
    "def print_resources(label=\"\"):\n",
    "    \"\"\"Print current RAM and GPU memory usage.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"RESOURCE CHECK: {label}\")\n",
    "    print(f\"Timestamp: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # System RAM\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"\\nðŸ“Š RAM: {mem.used/1024**3:.2f}/{mem.total/1024**3:.2f} GB ({mem.percent:.1f}%)\")\n",
    "    print(f\"   Available: {mem.available/1024**3:.2f} GB\")\n",
    "    \n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            print(f\"\\nðŸŽ® GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"   Allocated: {allocated:.2f}/{total:.2f} GB\")\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"âœ“ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Load Canary-Qwen-2.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean start\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print_resources(\"BEFORE loading model\")\n",
    "\n",
    "# Capture baseline\n",
    "ram_before = psutil.virtual_memory().used / 1024**3\n",
    "gpu_before = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n",
    "\n",
    "# Load model\n",
    "from nemo.collections.speechlm2.models import SALM\n",
    "model = SALM.from_pretrained(\"nvidia/canary-qwen-2.5b\")\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "print(\"\\nâœ“ Model loaded\\n\")\n",
    "print_resources(\"AFTER loading model\")\n",
    "\n",
    "# Calculate delta\n",
    "ram_after = psutil.virtual_memory().used / 1024**3\n",
    "gpu_after = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“Š MEMORY DELTA:\")\n",
    "print(f\"  RAM: +{ram_after - ram_before:.2f} GB\")\n",
    "print(f\"  GPU: +{gpu_after - gpu_before:.2f} GB\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "# Create dummy audio (10s silence)\n",
    "audio = np.zeros(16000 * 10, dtype=np.float32)\n",
    "\n",
    "print_resources(\"BEFORE inference\")\n",
    "ram_before = psutil.virtual_memory().used / 1024**3\n",
    "gpu_before = torch.cuda.memory_allocated(0) / 1024**3\n",
    "\n",
    "# Run inference\n",
    "with NamedTemporaryFile(suffix=\".wav\", delete=True) as tmp:\n",
    "    sf.write(tmp.name, audio, 16000)\n",
    "    \n",
    "    prompts = [[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Transcribe the following: {model.audio_locator_tag}\",\n",
    "            \"audio\": [tmp.name]\n",
    "        }\n",
    "    ]]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        answer_ids = model.generate(prompts=prompts, max_new_tokens=512)\n",
    "    \n",
    "    result = model.tokenizer.ids_to_text(answer_ids[0].cpu()).strip()\n",
    "\n",
    "print(f\"\\nâœ“ Transcription: '{result}'\\n\")\n",
    "print_resources(\"AFTER inference\")\n",
    "\n",
    "ram_after = psutil.virtual_memory().used / 1024**3\n",
    "gpu_after = torch.cuda.memory_allocated(0) / 1024**3\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“Š INFERENCE OVERHEAD:\")\n",
    "print(f\"  RAM: +{ram_after - ram_before:.2f} GB\")\n",
    "print(f\"  GPU: +{gpu_after - gpu_before:.2f} GB\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Canary-Qwen-2.5B Memory Requirements:**\n",
    "- Model loading: ~5 GB RAM, ~10 GB GPU\n",
    "- Inference overhead: Negligible (<0.1 GB)\n",
    "\n",
    "**Conclusion:**\n",
    "- âœ… Runs on NC4as_T4_v3 (28GB RAM, 16GB T4)\n",
    "- âœ… Previous \"20-25GB RAM\" estimate was incorrect\n",
    "- âœ… No need for larger VM or quota increase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
