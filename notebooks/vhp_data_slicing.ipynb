{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load parquet\n",
    "\n",
    "# Specify the path to your Parquet file\n",
    "parquet_file_path = '../data/raw/loc/veterans_history_project.parquet'\n",
    "\n",
    "# Read the Parquet file into a Pandas DataFrame\n",
    "df = pd.read_parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names checker\n",
    "# note: date/ dats are related to dates of service/ war campaigns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year of record creation as a proxy of the age of media\n",
    "df['number_date_created_first_itm'] = df['number_date_created'].apply(lambda x: x[0])\n",
    "df['year_record_created'] = df['number_date_created_first_itm'].str.extract(r'^(\\d{4})').astype(int)\n",
    "df = df.sort_values(by='year_record_created', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "bins = df['year_record_created'].max() - df['year_record_created'].min()\n",
    "df['year_record_created'].hist(bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre2010 = df[df['year_record_created']<=2010]\n",
    "df_pre2010.to_parquet('../data/raw/loc/veterans_history_project_pre2010.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post2010 = df[df['year_record_created']>2010]\n",
    "df_post2010.to_parquet('../data/raw/loc/veterans_history_project_post2010.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre2010.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "range(len(df_pre2010))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre2010 = df_pre2010.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post2010.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "range(len(df_post2010))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post2010 = df_post2010.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve resource DataFrame from the parquet file\n",
    "df_resources = pd.read_parquet('../data/raw/loc/veterans_history_project_resources.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the dataframe where each row contains only one media resource\n",
    "l_collection_numbers = []\n",
    "for n in range(len(df_pre2010)):\n",
    "    collection_number = df_pre2010['item'][n]['collection_number']\n",
    "    # print(collection_number)\n",
    "    l_collection_numbers.append(collection_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the item collection numbers are unique\n",
    "print(len(l_collection_numbers))\n",
    "print(len(set(l_collection_numbers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resources_filtered = df_resources[df_resources['collection_number'].isin(l_collection_numbers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame to a parquet file\n",
    "df_resources_filtered.to_parquet('../data/raw/loc/veterans_history_project_resources_pre2010.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post 2010:\n",
    "# reconstruct the dataframe where each row contains only one media resource\n",
    "l_collection_numbers = []\n",
    "for n in range(len(df_post2010)):\n",
    "    collection_number = df_post2010['item'][n]['collection_number']\n",
    "    # print(collection_number)\n",
    "    l_collection_numbers.append(collection_number)\n",
    "df_resources_filtered = df_resources[df_resources['collection_number'].isin(l_collection_numbers)]\n",
    "# save the DataFrame to a parquet file\n",
    "df_resources_filtered.to_parquet('../data/raw/loc/veterans_history_project_resources_post2010.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Create train/ validation splits as current sampled set (random seed `42`) as evaluatio(test) set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Retrieve pre 2010 dataframe and simulated sample=1000:\n",
    "df_pre2010 = pd.read_parquet('../data/raw/loc/veterans_history_project_resources_pre2010.parquet')\n",
    "\n",
    "# Replicate sample set creation from current production config\n",
    "\n",
    "# 1. Filter for items that have transcripts\n",
    "if 'fulltext_file_str' in df_pre2010.columns:\n",
    "    df_pre2010 = df_pre2010[df_pre2010['fulltext_file_str'].notna()]\n",
    "    print(f\"Filtered to {len(df_pre2010)} items with transcripts\")\n",
    "    has_media = (df_pre2010['audio_url'].notna()) | (df_pre2010['video_url'].notna())\n",
    "    df_pre2010 = df_pre2010[has_media]\n",
    "    print(f\"Filtered to {len(df_pre2010)} items with media\")\n",
    "\n",
    "# 2. Sort by index for deterministic order\n",
    "df_pre2010 = df_pre2010.sort_index()\n",
    "\n",
    "# 3. Random see = 42, sample size = 1000\n",
    "df_pre2010_sample1000 = df_pre2010.sample(n=1000, random_state=42)\n",
    "\n",
    "# Train/ Validation set creation\n",
    "df_pre2010_train_val = df_pre2010.drop(df_pre2010_sample1000.index)\n",
    "print(\"number of rows after filtering: \" + str(len(df_pre2010)))\n",
    "print(\"number of inference samples created (eval set): \" + str(len(df_pre2010_sample1000)))\n",
    "print(\"number of remaining rows used for training and validation: \" + str(len(df_pre2010_train_val)))\n",
    "\n",
    "# helper to check dataframe slice\n",
    "# df_pre2010_sample1000.head()\n",
    "\n",
    "# reserved for future use (e.g. featuring engineering)\n",
    "# Separate features (X) and target (y)\n",
    "# X = df_pre2010_train_val.drop(columns = ['fulltext_file_str', 'fulltext_file_str_cleaned', 'transcript_raw_text_only'], axis=1)\n",
    "# y = df_pre2010_train_val[['fulltext_file_str', 'fulltext_file_str_cleaned', 'transcript_raw_text_only']]\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# df_pre2010_train = pd.concat([X_train, y_train])\n",
    "# df_pre2010_val = pd.concat([X_val, y_val])\n",
    "\n",
    "# Create train/ val splits\n",
    "df_pre2010_train, df_pre2010_val = train_test_split(df_pre2010_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"number of rows for training: \" + str(len(df_pre2010_train)))\n",
    "print(\"number of rows for validation: \" + str(len(df_pre2010_val)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train, validation and test sets as parquet files\n",
    "df_pre2010_sample1000.to_parquet('../data/raw/loc/veterans_history_project_resources_pre2010_test.parquet', index=False)\n",
    "df_pre2010_train.to_parquet('../data/raw/loc/veterans_history_project_resources_pre2010_train.parquet', index=False)\n",
    "df_pre2010_val.to_parquet('../data/raw/loc/veterans_history_project_resources_pre2010_val.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Critical issue found when using train set to finetune: \n",
    "The above datasets contain audio file/ transcript pairs that exceed training/ finetuning requirements for whisper (30 secs). \n",
    "\n",
    "## Discussion:\n",
    "\n",
    "Since there's no timestamp provided in the transcripts, we need to apply a technique called forced alignment to break down the transcripts precisely while truncating our media files.\n",
    "\n",
    "## Proposed resolution:\n",
    "- Develop a set of utility functions under /scripts that orchestrate the process of data loading, audio + transcript truncating with forced alignment in place.\n",
    "\n",
    "- Projected main function of the utilities will orchestrate the series of processes and return a list of az blob paths of wav files (truncated audio) and the list of chopped transcripts (the transcript of the corresponding audio)\n",
    "\n",
    "## Implementation planning\n",
    "1. Data load from az blob/ get stream/ file to format that can be used by forced alignment tool (maybe utilizing what we currently have, such as dataloader and azure utils)\n",
    "\n",
    "2. Utilize tool(s) to chop audio to be chunks less than the “audio length constraint”(30 secs for whisper, with VAD applied ideally), and locate transcript portion within that time range based on forced alignment process\n",
    "\n",
    "4. Data storage in az blob with proper path and naming and e.g. path/1_1.wav means the first <30s chunk of path/1.mp3 or path/1.mp4(current blob path of those long form interviews)\n",
    "\n",
    "5. Create df of dict of datastruct put the the az blob paths of the clipped media, as well as the corresponding clipped transcript (imagine like a 2 col table) that can be converted to timestamped tokens used for ft jobs\n",
    "\n",
    "6. Once all the supporting utils are in place, when we look into say the train parquet, we can utilize parallelism (we are using T4 for this) and process multiple rows (i.e. media files) at the same time.\n",
    "\n",
    "## Unsolved questions:\n",
    "- which forced alignment tools/package should we use? which one is the most convenient one for our current setup? do they have audio duration limits?\n",
    "- how to make sure the output tokens with timestamp can be used properly during finetunng? if not we have to strip out the timestamps...\n",
    "- do we only need to truncate the _train parquet, or we also need to do it on the _val parquet?\n",
    "- more questions from you?\n",
    "\n",
    "## Useful Resources:\n",
    "1. (seems super useful)Nvidia has a tool that serves similar purpose:\n",
    "- doc: https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/tools/ctc_segmentation.html\n",
    "- tutorial notebook: https://github.com/NVIDIA-NeMo/NeMo/blob/main/tutorials/tools/CTC_Segmentation_Tutorial.ipynb (you have to let me know if you have trouble reading it, i can help dl it)\n",
    "supporting scripts: https://github.com/NVIDIA-NeMo/NeMo/tree/main/tools/ctc_segmentation/scripts (you have to let me know if you have trouble reading the scripts from there, i can help dl them)\n",
    "2. Montreal Forced Aligner - it was praised as good community based project: (https://www.reddit.com/r/MLQuestions/comments/mczow7/generating_timestamps_for_transcript_text_to_an/), some blogs pasted some tutorials that may be helpful: (https://eleanorchodroff.com/tutorial/montreal-forced-aligner.html)\n",
    "3. context on timestamped tokens:\n",
    "https://github.com/openai/whisper/discussions/620\n",
    "4. reference on finetuning whisper:\n",
    "https://www.diabolocom.com/research/fine-tuning-asr-focus-on-whisper/#tutorial-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# NeMo Forced Aligner (NFA) Segmentation Demo\n",
    "\n",
    "Using **NeMo Forced Aligner** to create training-ready data from long-form interviews.\n",
    "\n",
    "**Why NFA instead of CTC-Segmentation:**\n",
    "- NFA is the newer, recommended tool from NVIDIA\n",
    "- Provides word-level timestamps (more precise than sentence-level)\n",
    "- More robust alignment algorithm\n",
    "- Better handling of speech variations\n",
    "\n",
    "**Process:**\n",
    "1. Load 5 rows from train parquet\n",
    "2. For each row: download audio, run NFA alignment, cut into <30s segments\n",
    "3. Upload segments to Azure blob\n",
    "4. Create new parquet with segmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NFA segmentation utilities\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"scripts\"))\n",
    "\n",
    "from nfa_segmentation_utils import process_parquet_batch\n",
    "\n",
    "# Load Azure credentials\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../credentials/creds.env')\n",
    "\n",
    "# Configuration\n",
    "NEMO_MODEL = \"stt_en_conformer_ctc_large\"  # NeMo ASR model for NFA alignment\n",
    "INPUT_PARQUET = \"../data/raw/loc/veterans_history_project_resources_pre2010_train.parquet\"\n",
    "OUTPUT_PARQUET = \"../data/raw/loc/veterans_history_project_resources_pre2010_train_nfa_segmented_demo.parquet\"\n",
    "SAMPLE_SIZE = 5  # Process first 5 rows for demo\n",
    "\n",
    "print(f\"Using NeMo Forced Aligner (NFA)\")\n",
    "print(f\"Model: {NEMO_MODEL}\")\n",
    "print(f\"Input: {INPUT_PARQUET}\")\n",
    "print(f\"Output: {OUTPUT_PARQUET}\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE}\")\n",
    "print(\"\\nThis will take ~5-10 minutes on T4 GPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NFA segmentation on 5 rows\n",
    "# \n",
    "# IMPORTANT: Choose which transcript field to use:\n",
    "# - transcript_field=\"fulltext_file_str\" (default): Raw XML transcript\n",
    "#   May have encoding issues (curly quotes, XML artifacts) causing NFA bugs\n",
    "# - transcript_field=\"transcript_raw_text_only\" (recommended): Pre-cleaned text\n",
    "#   No XML, no curly quotes, more compatible with NFA tokenizer\n",
    "\n",
    "df_segmented = process_parquet_batch(\n",
    "    parquet_path=INPUT_PARQUET,\n",
    "    output_parquet_path=OUTPUT_PARQUET,\n",
    "    model_name=NEMO_MODEL,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    max_duration=30.0,  # Max segment duration for Whisper\n",
    "    blob_prefix=\"loc_vhp\",\n",
    "    transcript_field=\"transcript_raw_text_only\"  # Use pre-cleaned text to avoid NFA bugs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results - show first 5 segments\n",
    "print(f\"Total segments generated: {len(df_segmented)}\")\n",
    "print(f\"\\nColumns: {list(df_segmented.columns)}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE SEGMENTS (first 5)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, row in df_segmented.head(5).iterrows():\n",
    "    print(f\"\\nSegment {i}:\")\n",
    "    print(f\"  Source row: {row.get('source_row_idx', 'N/A')}\")\n",
    "    print(f\"  Segment idx: {row.get('segment_idx', 'N/A')}\")\n",
    "    print(f\"  Audio URL: {row.get('audio_url', 'N/A')}\")\n",
    "    print(f\"  Duration: {row.get('segment_duration', 0):.1f}s\")\n",
    "    print(f\"  Confidence: {row.get('confidence', 0):.2f}\")\n",
    "    print(f\"  Text (first 100 chars): {row.get('fulltext_file_str', '')[:100]}...\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify schema compatibility with fine-tuning pipeline\n",
    "print(\"Schema Verification:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "required_cols = ['audio_url', 'fulltext_file_str']\n",
    "for col in required_cols:\n",
    "    if col in df_segmented.columns:\n",
    "        print(f\"✓ {col}: present\")\n",
    "    else:\n",
    "        print(f\"✗ {col}: MISSING\")\n",
    "\n",
    "print(f\"\\nSegmented parquet saved to: {OUTPUT_PARQUET}\")\n",
    "print(f\"Ready for fine-tuning with finetune_whisper_lora.ipynb\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Process full train parquet (2273 rows → ~100k segments)\")\n",
    "print(f\"  2. Process val parquet (569 rows → ~25k segments)\")  \n",
    "print(f\"  3. Use segmented parquets in fine-tuning notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# How CTC-Based Forced Alignment Works\n",
    "\n",
    "This implementation uses **NeMo CTC Segmentation** to align long-form interview transcripts with audio, creating training-ready segments.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Input:** 30-60 minute oral history interviews with full transcripts (no timestamps)  \n",
    "**Output:** <30 second audio segments with aligned transcript text  \n",
    "**Success Rate:** 80% (4 out of 5 demo files processed successfully)\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Process\n",
    "\n",
    "### 1. **Download & Convert Audio**\n",
    "- Download original audio from Azure blob storage (`loc_vhp/{index}/video.mp4` or `audio.mp3`)\n",
    "- Convert to WAV format, 16kHz mono (required by NeMo ASR models)\n",
    "\n",
    "### 2. **Prepare Transcript**\n",
    "Function: `prepare_text_for_segmentation(fulltext_file_str)`\n",
    "\n",
    "- **Extract plain text** from XML using `clean_raw_transcript_str()` \n",
    "  - Removes XML tags, metadata, stage directions\n",
    "  - Example: `<?xml version=\"1.0\"...><TEI.2>...` → plain spoken text\n",
    "- **Split into sentences** using punctuation (`.`, `!`, `?`)\n",
    "  - Example: 457 sentences from 40,296 character transcript\n",
    "- **Normalize** to lowercase (ASR models expect lowercase)\n",
    "\n",
    "### 3. **Generate CTC Alignments**\n",
    "Function: `segment_audio_with_ctc(audio_path, sentences)`\n",
    "\n",
    "Uses **NeMo Conformer-CTC-Large** model to map audio to text:\n",
    "\n",
    "#### 3a. Extract CTC Log Probabilities\n",
    "- Process audio through ASR model: `asr_model.transcribe(audio, return_hypotheses=True)`\n",
    "- For long audio (>30 min): Process in 60-second chunks to avoid GPU OOM, then concatenate\n",
    "- Output: Log probability matrix of shape `(time_steps, vocab_size)`\n",
    "  - Example: `(61877, 129)` = 61,877 time frames × 129 vocabulary tokens\n",
    "  - Each time frame = ~40ms of audio\n",
    "\n",
    "#### 3b. Prepare Vocabulary\n",
    "- Extract model's vocabulary (128 subword tokens for BPE model)\n",
    "- **Critical step:** Add blank token `\"ε\"` at position 0\n",
    "- Move blank column in logits from last position to first (ctc-segmentation requirement)\n",
    "\n",
    "#### 3c. Run CTC Segmentation Algorithm\n",
    "Uses `ctc-segmentation` library ([paper](https://arxiv.org/pdf/2007.09127.pdf)):\n",
    "\n",
    "```python\n",
    "from ctc_segmentation import prepare_text, ctc_segmentation\n",
    "\n",
    "# Create ground truth matrix from sentences\n",
    "ground_truth_mat, utt_begin_indices = prepare_text(config, text)\n",
    "\n",
    "# Align text to audio using CTC dynamic programming\n",
    "timings, char_probs, state_list = ctc_segmentation(\n",
    "    config, logits, ground_truth_mat\n",
    ")\n",
    "```\n",
    "\n",
    "**Output:** Timestamp pairs `(start, end)` for each sentence with confidence scores\n",
    "\n",
    "### 4. **Cut Audio Segments**\n",
    "Function: `cut_audio_segments(wav_path, segments)`\n",
    "\n",
    "- For each aligned sentence:\n",
    "  - Extract audio from `start` to `end` timestamp\n",
    "  - Filter segments >30 seconds (Whisper training limit)\n",
    "  - Save as individual WAV file: `{blob_index}_{segment_idx}.wav`\n",
    "  - Example: `2840_000.wav`, `2840_001.wav`, ..., `2840_376.wav`\n",
    "\n",
    "### 5. **Upload to Azure**\n",
    "Function: `upload_segments_to_blob(segment_paths, blob_prefix, blob_index)`\n",
    "\n",
    "- Upload each segment to Azure blob storage\n",
    "- Naming pattern: `loc_vhp/{blob_index}/{blob_index}_{segment_idx}.wav`\n",
    "- Example: `loc_vhp/2840/2840_150.wav`\n",
    "\n",
    "### 6. **Create Output DataFrame**\n",
    "Each segment becomes a row in the output parquet:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'source_row_idx': 3,           # Original interview row\n",
    "    'segment_idx': 150,            # Segment number within interview\n",
    "    'audio_url': 'loc_vhp/2840/2840_150.wav',\n",
    "    'fulltext_file_str': 'i was a pfc by the end of the war',\n",
    "    'start_time': 600.5,           # Seconds from start of original audio\n",
    "    'end_time': 615.2,             # Seconds from start of original audio\n",
    "    'confidence': -0.85,           # CTC alignment confidence (log space)\n",
    "    'segment_duration': 14.7       # Duration in seconds\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Row 3 (blob_index=2840)\n",
    "\n",
    "**Input:**\n",
    "- Audio: 18 minutes (1080 seconds)\n",
    "- Transcript: 457 sentences, 7,735 words\n",
    "\n",
    "**Processing:**\n",
    "- Chunked into 18 × 60-second pieces for GPU processing\n",
    "- Generated logits: `(27077, 129)` matrix\n",
    "- Index duration: 0.0399s per CTC frame\n",
    "- CTC segmentation aligned 457 sentences to audio\n",
    "\n",
    "**Output:**\n",
    "- **377 segments** created (some sentences merged/split for optimal alignment)\n",
    "- Average segment duration: ~2.9 seconds\n",
    "- All segments <30 seconds (Whisper requirement)\n",
    "- Uploaded to: `loc_vhp/2840/2840_000.wav` through `loc_vhp/2840/2840_376.wav`\n",
    "\n",
    "**Sample segment (segment_idx=150):**\n",
    "```\n",
    "Text: \"i was a corporal with a military intelligence outfit\"\n",
    "Duration: 3.2 seconds\n",
    "Confidence: -0.91\n",
    "Audio URL: loc_vhp/2840/2840_150.wav\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Technical Details\n",
    "\n",
    "### Why CTC Segmentation?\n",
    "1. **No timestamps required** - Works with plain text transcripts\n",
    "2. **Handles long audio** - Processes 30-60 minute files\n",
    "3. **Natural sentence boundaries** - Aligns at sentence level, not arbitrary chunks\n",
    "4. **Confidence scores** - Filters low-quality alignments (threshold: -2.0)\n",
    "\n",
    "### GPU Memory Management\n",
    "- **Try-catch pattern**: Attempt whole file first, fallback to chunking on OOM\n",
    "- **Adaptive chunking**: 60s chunks for >30min audio, 120s for shorter\n",
    "- **Cache clearing**: Clear CUDA cache after each file to prevent memory leaks\n",
    "\n",
    "### Error Handling\n",
    "**\"Audio is shorter than text\" error:**\n",
    "- Occurs when transcript is longer than audio duration\n",
    "- Example: Row 4 had 51.6 min transcript for 41.2 min audio\n",
    "- Likely cause: Transcript contains non-spoken content (interviewer notes, metadata)\n",
    "- Solution: Skip these files (normal for real-world data)\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. [CTC-Segmentation Paper](https://arxiv.org/pdf/2007.09127.pdf)\n",
    "2. [NeMo CTC Segmentation Tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/tools/CTC_Segmentation_Tutorial.ipynb)\n",
    "3. [NeMo Documentation](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/tools/ctc_segmentation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segmented.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segmented['fulltext_file_str']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
