{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load parquet\n",
    "\n",
    "# Specify the path to your Parquet file\n",
    "parquet_file_path = '../data/raw/loc/veterans_history_project.parquet'\n",
    "\n",
    "# Read the Parquet file into a Pandas DataFrame\n",
    "df = pd.read_parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names checker\n",
    "# note: date/ dats are related to dates of service/ war campaigns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year of record creation as a proxy of the age of media\n",
    "df['number_date_created_first_itm'] = df['number_date_created'].apply(lambda x: x[0])\n",
    "df['year_record_created'] = df['number_date_created_first_itm'].str.extract(r'^(\\d{4})').astype(int)\n",
    "df = df.sort_values(by='year_record_created', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "bins = df['year_record_created'].max() - df['year_record_created'].min()\n",
    "df['year_record_created'].hist(bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre2010 = df[df['year_record_created']<=2010]\n",
    "df_pre2010.to_parquet('../data/raw/loc/veterans_history_project_pre2010.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post2010 = df[df['year_record_created']>2010]\n",
    "df_post2010.to_parquet('../data/raw/loc/veterans_history_project_post2010.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre2010.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "range(len(df_pre2010))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre2010 = df_pre2010.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post2010.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "range(len(df_post2010))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post2010 = df_post2010.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve resource DataFrame from the parquet file\n",
    "df_resources = pd.read_parquet('../data/raw/loc/veterans_history_project_resources.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the dataframe where each row contains only one media resource\n",
    "l_collection_numbers = []\n",
    "for n in range(len(df_pre2010)):\n",
    "    collection_number = df_pre2010['item'][n]['collection_number']\n",
    "    # print(collection_number)\n",
    "    l_collection_numbers.append(collection_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the item collection numbers are unique\n",
    "print(len(l_collection_numbers))\n",
    "print(len(set(l_collection_numbers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resources_filtered = df_resources[df_resources['collection_number'].isin(l_collection_numbers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame to a parquet file\n",
    "df_resources_filtered.to_parquet('../data/raw/loc/veterans_history_project_resources_pre2010.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post 2010:\n",
    "# reconstruct the dataframe where each row contains only one media resource\n",
    "l_collection_numbers = []\n",
    "for n in range(len(df_post2010)):\n",
    "    collection_number = df_post2010['item'][n]['collection_number']\n",
    "    # print(collection_number)\n",
    "    l_collection_numbers.append(collection_number)\n",
    "df_resources_filtered = df_resources[df_resources['collection_number'].isin(l_collection_numbers)]\n",
    "# save the DataFrame to a parquet file\n",
    "df_resources_filtered.to_parquet('../data/raw/loc/veterans_history_project_resources_post2010.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Create train/ validation splits as current sampled set (random seed `42`) as evaluatio(test) set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Retrieve pre 2010 dataframe and simulated sample=1000:\n",
    "df_pre2010 = pd.read_parquet('../data/raw/loc/veterans_history_project_resources_pre2010.parquet')\n",
    "\n",
    "# Replicate sample set creation from current production config\n",
    "\n",
    "# 1. Filter for items that have transcripts\n",
    "if 'fulltext_file_str' in df_pre2010.columns:\n",
    "    df_pre2010 = df_pre2010[df_pre2010['fulltext_file_str'].notna()]\n",
    "    print(f\"Filtered to {len(df_pre2010)} items with transcripts\")\n",
    "    has_media = (df_pre2010['audio_url'].notna()) | (df_pre2010['video_url'].notna())\n",
    "    df_pre2010 = df_pre2010[has_media]\n",
    "    print(f\"Filtered to {len(df_pre2010)} items with media\")\n",
    "\n",
    "# 2. Sort by index for deterministic order\n",
    "df_pre2010 = df_pre2010.sort_index()\n",
    "\n",
    "# 3. Random see = 42, sample size = 1000\n",
    "df_pre2010_sample1000 = df_pre2010.sample(n=1000, random_state=42)\n",
    "\n",
    "# Train/ Validation set creation\n",
    "df_pre2010_train_val = df_pre2010.drop(df_pre2010_sample1000.index)\n",
    "print(\"number of rows after filtering: \" + str(len(df_pre2010)))\n",
    "print(\"number of inference samples created (eval set): \" + str(len(df_pre2010_sample1000)))\n",
    "print(\"number of remaining rows used for training and validation: \" + str(len(df_pre2010_train_val)))\n",
    "\n",
    "# helper to check dataframe slice\n",
    "# df_pre2010_sample1000.head()\n",
    "\n",
    "# reserved for future use (e.g. featuring engineering)\n",
    "# Separate features (X) and target (y)\n",
    "# X = df_pre2010_train_val.drop(columns = ['fulltext_file_str', 'fulltext_file_str_cleaned', 'transcript_raw_text_only'], axis=1)\n",
    "# y = df_pre2010_train_val[['fulltext_file_str', 'fulltext_file_str_cleaned', 'transcript_raw_text_only']]\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# df_pre2010_train = pd.concat([X_train, y_train])\n",
    "# df_pre2010_val = pd.concat([X_val, y_val])\n",
    "\n",
    "# Create train/ val splits\n",
    "df_pre2010_train, df_pre2010_val = train_test_split(df_pre2010_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"number of rows for training: \" + str(len(df_pre2010_train)))\n",
    "print(\"number of rows for validation: \" + str(len(df_pre2010_val)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train, validation and test sets as parquet files\n",
    "df_pre2010_sample1000.to_parquet('../data/raw/loc/veterans_history_project_resources_pre2010_test.parquet', index=False)\n",
    "df_pre2010_train.to_parquet('../data/raw/loc/veterans_history_project_resources_pre2010_train.parquet', index=False)\n",
    "df_pre2010_val.to_parquet('../data/raw/loc/veterans_history_project_resources_pre2010_val.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Critical issue found when using train set to finetune: \n",
    "The above datasets contain audio file/ transcript pairs that exceed training/ finetuning requirements for whisper (30 secs). \n",
    "\n",
    "## Discussion:\n",
    "\n",
    "Since there's no timestamp provided in the transcripts, we need to apply a technique called forced alignment to break down the transcripts precisely while truncating our media files.\n",
    "\n",
    "## Proposed resolution:\n",
    "- Develop a set of utility functions under /scripts that orchestrate the process of data loading, audio + transcript truncating with forced alignment in place.\n",
    "\n",
    "- Projected main function of the utilities will orchestrate the series of processes and return a list of az blob paths of wav files (truncated audio) and the list of chopped transcripts (the transcript of the corresponding audio)\n",
    "\n",
    "## Implementation planning\n",
    "1. Data load from az blob/ get stream/ file to format that can be used by forced alignment tool (maybe utilizing what we currently have, such as dataloader and azure utils)\n",
    "\n",
    "2. Utilize tool(s) to chop audio to be chunks less than the “audio length constraint”(30 secs for whisper, with VAD applied ideally), and locate transcript portion within that time range based on forced alignment process\n",
    "\n",
    "4. Data storage in az blob with proper path and naming and e.g. path/1_1.wav means the first <30s chunk of path/1.mp3 or path/1.mp4(current blob path of those long form interviews)\n",
    "\n",
    "5. Create df of dict of datastruct put the the az blob paths of the clipped media, as well as the corresponding clipped transcript (imagine like a 2 col table) that can be converted to timestamped tokens used for ft jobs\n",
    "\n",
    "6. Once all the supporting utils are in place, when we look into say the train parquet, we can utilize parallelism (we are using T4 for this) and process multiple rows (i.e. media files) at the same time.\n",
    "\n",
    "## Unsolved questions:\n",
    "- which forced alignment tools/package should we use? which one is the most convenient one for our current setup? do they have audio duration limits?\n",
    "- how to make sure the output tokens with timestamp can be used properly during finetunng? if not we have to strip out the timestamps...\n",
    "- do we only need to truncate the _train parquet, or we also need to do it on the _val parquet?\n",
    "- more questions from you?\n",
    "\n",
    "## Useful Resources:\n",
    "1. (seems super useful)Nvidia has a tool that serves similar purpose:\n",
    "- doc: https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/tools/ctc_segmentation.html\n",
    "- tutorial notebook: https://github.com/NVIDIA-NeMo/NeMo/blob/main/tutorials/tools/CTC_Segmentation_Tutorial.ipynb (you have to let me know if you have trouble reading it, i can help dl it)\n",
    "supporting scripts: https://github.com/NVIDIA-NeMo/NeMo/tree/main/tools/ctc_segmentation/scripts (you have to let me know if you have trouble reading the scripts from there, i can help dl them)\n",
    "2. Montreal Forced Aligner - it was praised as good community based project: (https://www.reddit.com/r/MLQuestions/comments/mczow7/generating_timestamps_for_transcript_text_to_an/), some blogs pasted some tutorials that may be helpful: (https://eleanorchodroff.com/tutorial/montreal-forced-aligner.html)\n",
    "3. context on timestamped tokens:\n",
    "https://github.com/openai/whisper/discussions/620\n",
    "4. reference on finetuning whisper:\n",
    "https://www.diabolocom.com/research/fine-tuning-asr-focus-on-whisper/#tutorial-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# CTC Segmentation Demo\n",
    "\n",
    "Using NeMo CTC Segmentation to create training-ready data from long-form interviews.\n",
    "\n",
    "**Process:**\n",
    "1. Load 5 rows from train parquet\n",
    "2. For each row: download audio, run CTC alignment, cut into <30s segments\n",
    "3. Upload segments to Azure blob\n",
    "4. Create new parquet with segmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import segmentation utilities\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"scripts\"))\n",
    "\n",
    "from ctc_segmentation_utils import process_parquet_batch\n",
    "\n",
    "# Load Azure credentials\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../credentials/creds.env')\n",
    "\n",
    "# Configuration\n",
    "NEMO_MODEL = \"stt_en_conformer_ctc_large\"  # Will auto-download on first run\n",
    "INPUT_PARQUET = \"../data/raw/loc/veterans_history_project_resources_pre2010_train.parquet\"\n",
    "OUTPUT_PARQUET = \"../data/raw/loc/veterans_history_project_resources_pre2010_train_segmented_demo.parquet\"\n",
    "SAMPLE_SIZE = 5  # Process first 5 rows for demo\n",
    "\n",
    "print(f\"Model: {NEMO_MODEL}\")\n",
    "print(f\"Input: {INPUT_PARQUET}\")\n",
    "print(f\"Output: {OUTPUT_PARQUET}\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE}\")\n",
    "print(\"\\nThis will take ~5-10 minutes on T4 GPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CTC segmentation on 5 rows\n",
    "df_segmented = process_parquet_batch(\n",
    "    parquet_path=INPUT_PARQUET,\n",
    "    output_parquet_path=OUTPUT_PARQUET,\n",
    "    model_name=NEMO_MODEL,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    max_duration=30.0,  # Max segment duration for Whisper\n",
    "    min_confidence=-2.0,  # CTC confidence threshold\n",
    "    blob_prefix=\"loc_vhp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results - show first 5 segments\n",
    "print(f\"Total segments generated: {len(df_segmented)}\")\n",
    "print(f\"\\nColumns: {list(df_segmented.columns)}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE SEGMENTS (first 5)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, row in df_segmented.head(5).iterrows():\n",
    "    print(f\"\\nSegment {i}:\")\n",
    "    print(f\"  Source row: {row.get('source_row_idx', 'N/A')}\")\n",
    "    print(f\"  Segment idx: {row.get('segment_idx', 'N/A')}\")\n",
    "    print(f\"  Audio URL: {row.get('audio_url', 'N/A')}\")\n",
    "    print(f\"  Duration: {row.get('segment_duration', 0):.1f}s\")\n",
    "    print(f\"  Confidence: {row.get('confidence', 0):.2f}\")\n",
    "    print(f\"  Text (first 100 chars): {row.get('fulltext_file_str', '')[:100]}...\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify schema compatibility with fine-tuning pipeline\n",
    "print(\"Schema Verification:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "required_cols = ['audio_url', 'fulltext_file_str']\n",
    "for col in required_cols:\n",
    "    if col in df_segmented.columns:\n",
    "        print(f\"✓ {col}: present\")\n",
    "    else:\n",
    "        print(f\"✗ {col}: MISSING\")\n",
    "\n",
    "print(f\"\\nSegmented parquet saved to: {OUTPUT_PARQUET}\")\n",
    "print(f\"Ready for fine-tuning with finetune_whisper_lora.ipynb\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Process full train parquet (2273 rows → ~100k segments)\")\n",
    "print(f\"  2. Process val parquet (569 rows → ~25k segments)\")  \n",
    "print(f\"  3. Use segmented parquets in fine-tuning notebooks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
