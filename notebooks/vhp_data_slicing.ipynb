{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load parquet\n",
    "\n",
    "# Specify the path to your Parquet file\n",
    "parquet_file_path = '../data/raw/loc/veterans_history_project.parquet'\n",
    "\n",
    "# Read the Parquet file into a Pandas DataFrame\n",
    "df = pd.read_parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names checker\n",
    "# note: date/ dats are related to dates of service/ war campaigns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year of record creation as a proxy of the age of media\n",
    "df['number_date_created_first_itm'] = df['number_date_created'].apply(lambda x: x[0])\n",
    "df['year_record_created'] = df['number_date_created_first_itm'].str.extract(r'^(\\d{4})').astype(int)\n",
    "df = df.sort_values(by='year_record_created', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "bins = df['year_record_created'].max() - df['year_record_created'].min()\n",
    "df['year_record_created'].hist(bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre2010 = df[df['year_record_created']<=2010]\n",
    "df_pre2010.to_parquet('../data/raw/loc/veterans_history_project_pre2010.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post2010 = df[df['year_record_created']>2010]\n",
    "df_post2010.to_parquet('../data/raw/loc/veterans_history_project_post2010.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre2010.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "range(len(df_pre2010))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre2010 = df_pre2010.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post2010.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "range(len(df_post2010))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post2010 = df_post2010.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve resource DataFrame from the parquet file\n",
    "df_resources = pd.read_parquet('../data/raw/loc/veterans_history_project_resources.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the dataframe where each row contains only one media resource\n",
    "l_collection_numbers = []\n",
    "for n in range(len(df_pre2010)):\n",
    "    collection_number = df_pre2010['item'][n]['collection_number']\n",
    "    # print(collection_number)\n",
    "    l_collection_numbers.append(collection_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the item collection numbers are unique\n",
    "print(len(l_collection_numbers))\n",
    "print(len(set(l_collection_numbers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resources_filtered = df_resources[df_resources['collection_number'].isin(l_collection_numbers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame to a parquet file\n",
    "df_resources_filtered.to_parquet('../data/raw/loc/veterans_history_project_resources_pre2010.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post 2010:\n",
    "# reconstruct the dataframe where each row contains only one media resource\n",
    "l_collection_numbers = []\n",
    "for n in range(len(df_post2010)):\n",
    "    collection_number = df_post2010['item'][n]['collection_number']\n",
    "    # print(collection_number)\n",
    "    l_collection_numbers.append(collection_number)\n",
    "df_resources_filtered = df_resources[df_resources['collection_number'].isin(l_collection_numbers)]\n",
    "# save the DataFrame to a parquet file\n",
    "df_resources_filtered.to_parquet('../data/raw/loc/veterans_history_project_resources_post2010.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Create train/ validation splits as current sampled set (random seed `42`) as evaluatio(test) set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Retrieve pre 2010 dataframe and simulated sample=1000:\n",
    "df_pre2010 = pd.read_parquet('../data/raw/loc/veterans_history_project_resources_pre2010.parquet')\n",
    "\n",
    "# Replicate sample set creation from current production config\n",
    "\n",
    "# 1. Filter for items that have transcripts\n",
    "if 'fulltext_file_str' in df_pre2010.columns:\n",
    "    df_pre2010 = df_pre2010[df_pre2010['fulltext_file_str'].notna()]\n",
    "    print(f\"Filtered to {len(df_pre2010)} items with transcripts\")\n",
    "    has_media = (df_pre2010['audio_url'].notna()) | (df_pre2010['video_url'].notna())\n",
    "    df_pre2010 = df_pre2010[has_media]\n",
    "    print(f\"Filtered to {len(df_pre2010)} items with media\")\n",
    "\n",
    "# 2. Sort by index for deterministic order\n",
    "df_pre2010 = df_pre2010.sort_index()\n",
    "\n",
    "# 3. Random see = 42, sample size = 1000\n",
    "df_pre2010_sample1000 = df_pre2010.sample(n=1000, random_state=42)\n",
    "\n",
    "# Train/ Validation set creation\n",
    "df_pre2010_train_val = df_pre2010.drop(df_pre2010_sample1000.index)\n",
    "print(\"number of rows after filtering: \" + str(len(df_pre2010)))\n",
    "print(\"number of inference samples created (eval set): \" + str(len(df_pre2010_sample1000)))\n",
    "print(\"number of remaining rows used for training and validation: \" + str(len(df_pre2010_train_val)))\n",
    "\n",
    "# helper to check dataframe slice\n",
    "# df_pre2010_sample1000.head()\n",
    "\n",
    "# reserved for future use (e.g. featuring engineering)\n",
    "# Separate features (X) and target (y)\n",
    "# X = df_pre2010_train_val.drop(columns = ['fulltext_file_str', 'fulltext_file_str_cleaned', 'transcript_raw_text_only'], axis=1)\n",
    "# y = df_pre2010_train_val[['fulltext_file_str', 'fulltext_file_str_cleaned', 'transcript_raw_text_only']]\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# df_pre2010_train = pd.concat([X_train, y_train])\n",
    "# df_pre2010_val = pd.concat([X_val, y_val])\n",
    "\n",
    "# Create train/ val splits\n",
    "df_pre2010_train, df_pre2010_val = train_test_split(df_pre2010_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"number of rows for training: \" + str(len(df_pre2010_train)))\n",
    "print(\"number of rows for validation: \" + str(len(df_pre2010_val)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train, validation and test sets as parquet files\n",
    "df_pre2010_sample1000.to_parquet('../data/raw/loc/veterans_history_project_resources_pre2010_test.parquet', index=False)\n",
    "df_pre2010_train.to_parquet('../data/raw/loc/veterans_history_project_resources_pre2010_train.parquet', index=False)\n",
    "df_pre2010_val.to_parquet('../data/raw/loc/veterans_history_project_resources_pre2010_val.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Critical issue found when using train set to finetune: \n",
    "The above datasets contain audio file/ transcript pairs that exceed training/ finetuning requirements for whisper (30 secs). \n",
    "\n",
    "## Discussion:\n",
    "\n",
    "Since there's no timestamp provided in the transcripts, we need to apply a technique called forced alignment to break down the transcripts precisely while truncating our media files.\n",
    "\n",
    "## Proposed resolution:\n",
    "- Develop a set of utility functions under /scripts that orchestrate the process of data loading, audio + transcript truncating with forced alignment in place.\n",
    "\n",
    "- Projected main function of the utilities will orchestrate the series of processes and return a list of az blob paths of wav files (truncated audio) and the list of chopped transcripts (the transcript of the corresponding audio)\n",
    "\n",
    "## Implementation planning\n",
    "1. Data load from az blob/ get stream/ file to format that can be used by forced alignment tool (maybe utilizing what we currently have, such as dataloader and azure utils)\n",
    "\n",
    "2. Utilize tool(s) to chop audio to be chunks less than the ‚Äúaudio length constraint‚Äù(30 secs for whisper, with VAD applied ideally), and locate transcript portion within that time range based on forced alignment process\n",
    "\n",
    "4. Data storage in az blob with proper path and naming and e.g. path/1_1.wav means the first <30s chunk of path/1.mp3 or path/1.mp4(current blob path of those long form interviews)\n",
    "\n",
    "5. Create df of dict of datastruct put the the az blob paths of the clipped media, as well as the corresponding clipped transcript (imagine like a 2 col table) that can be converted to timestamped tokens used for ft jobs\n",
    "\n",
    "6. Once all the supporting utils are in place, when we look into say the train parquet, we can utilize parallelism (we are using T4 for this) and process multiple rows (i.e. media files) at the same time.\n",
    "\n",
    "## Unsolved questions:\n",
    "- which forced alignment tools/package should we use? which one is the most convenient one for our current setup? do they have audio duration limits?\n",
    "- how to make sure the output tokens with timestamp can be used properly during finetunng? if not we have to strip out the timestamps...\n",
    "- do we only need to truncate the _train parquet, or we also need to do it on the _val parquet?\n",
    "- more questions from you?\n",
    "\n",
    "## Useful Resources:\n",
    "1. (seems super useful)Nvidia has a tool that serves similar purpose:\n",
    "- doc: https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/tools/ctc_segmentation.html\n",
    "- tutorial notebook: https://github.com/NVIDIA-NeMo/NeMo/blob/main/tutorials/tools/CTC_Segmentation_Tutorial.ipynb (you have to let me know if you have trouble reading it, i can help dl it)\n",
    "supporting scripts: https://github.com/NVIDIA-NeMo/NeMo/tree/main/tools/ctc_segmentation/scripts (you have to let me know if you have trouble reading the scripts from there, i can help dl them)\n",
    "2. Montreal Forced Aligner - it was praised as good community based project: (https://www.reddit.com/r/MLQuestions/comments/mczow7/generating_timestamps_for_transcript_text_to_an/), some blogs pasted some tutorials that may be helpful: (https://eleanorchodroff.com/tutorial/montreal-forced-aligner.html)\n",
    "3. context on timestamped tokens:\n",
    "https://github.com/openai/whisper/discussions/620\n",
    "4. reference on finetuning whisper:\n",
    "https://www.diabolocom.com/research/fine-tuning-asr-focus-on-whisper/#tutorial-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# NeMo Forced Aligner (NFA) Segmentation Demo\n",
    "\n",
    "Using **NeMo Forced Aligner** to create training-ready data from long-form interviews.\n",
    "\n",
    "**Why NFA instead of CTC-Segmentation:**\n",
    "- NFA is the newer, recommended tool from NVIDIA\n",
    "- Provides word-level timestamps (more precise than sentence-level)\n",
    "- More robust alignment algorithm\n",
    "- Better handling of speech variations\n",
    "\n",
    "**Process:**\n",
    "1. Load 5 rows from train parquet\n",
    "2. For each row: download audio, run NFA alignment, cut into <30s segments\n",
    "3. Upload segments to Azure blob\n",
    "4. Create new parquet with segmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Recent Fixes (Latest Update)\n",
    "\n",
    "### Issue 1: \"NA lex NA\" Patterns in Transcripts\n",
    "\n",
    "**Problem:** Previous runs showed special tokens in transcripts:\n",
    "```\n",
    "\"This NA lex NA is NA lex NA the NA lex NA Oral NA lex NA History...\"\n",
    "```\n",
    "\n",
    "**Root Cause:** NFA outputs special tokens (`NA`, `lex`, `<unk>`) for non-lexical sounds and alignment markers. These were not being filtered.\n",
    "\n",
    "**Fix:** Added filtering in `parse_ctm_file()` to skip these special tokens ([nfa_segmentation_utils.py:155-171](../scripts/nfa_segmentation_utils.py#L155))\n",
    "\n",
    "---\n",
    "\n",
    "### Issue 2: Naive Word-Level Truncation (No Natural Boundaries)\n",
    "\n",
    "**Problem:** Audio was being cut at arbitrary 30-second boundaries by grouping individual words, resulting in:\n",
    "- Mid-sentence cuts\n",
    "- No respect for natural pauses\n",
    "- Awkward segment boundaries\n",
    "\n",
    "**Root Cause:** Code was using **word-level CTM** and just packing words until hitting 30 seconds.\n",
    "\n",
    "**Fix:** Switched to **segment-level CTM** which uses NFA's built-in sentence segmentation ([nfa_segmentation_utils.py:489](../scripts/nfa_segmentation_utils.py#L489))\n",
    "\n",
    "**How NFA Segment-Level Works:**\n",
    "1. NFA is configured with `additional_segment_grouping_separator=[\".\",\"?\",\"!\",\"...\"]`\n",
    "2. This tells NFA to create segments at natural sentence boundaries\n",
    "3. We then group these sentences until hitting the 30-second limit\n",
    "4. Result: Clean cuts at sentence endings, not mid-sentence\n",
    "\n",
    "**Before (word-level):**\n",
    "```\n",
    "Segment 1: \"I was born in Pennsylvania and grew up in a small town and I always wanted to serve my coun‚Äî\" [cut at 30.0s]\n",
    "Segment 2: \"‚Äîtry so when World War II broke out I immediately enlisted...\"\n",
    "```\n",
    "\n",
    "**After (segment-level):**\n",
    "```\n",
    "Segment 1: \"I was born in Pennsylvania and grew up in a small town. I always wanted to serve my country.\" [29.5s]\n",
    "Segment 2: \"So when World War II broke out I immediately enlisted in the Navy...\" [28.2s]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Issue 3: CUDA Out of Memory on Long Files\n",
    "\n",
    "**Problem:** Files with very long transcripts (40k+ chars, ~50 min speech) caused CUDA OOM errors:\n",
    "```\n",
    "torch.OutOfMemoryError: Tried to allocate 18.84 GiB. GPU 0 has a total capacity of 15.56 GiB\n",
    "```\n",
    "\n",
    "**Fixes Applied:**\n",
    "\n",
    "1. **Switch to Medium Model** (lower GPU memory usage)\n",
    "   - Changed from `stt_en_conformer_ctc_large` ‚Üí `stt_en_conformer_ctc_medium`\n",
    "   - Reduces memory footprint while maintaining good alignment quality\n",
    "\n",
    "2. **Skip Very Long Files** (prevent OOM)\n",
    "   - Added `max_audio_duration` parameter (default: 1800s = 30 min)\n",
    "   - Files longer than this are automatically skipped with clear warning\n",
    "   - Prevents crashes and focuses on processable data\n",
    "\n",
    "**Expected Impact:**\n",
    "- Success rate improves from 40% ‚Üí 75-85%\n",
    "- Expected segments on full train set: ~75k-92k (more than enough for Whisper fine-tuning)\n",
    "\n",
    "---\n",
    "\n",
    "### All Fixes Applied:\n",
    "\n",
    "1. **Filter Special Tokens** ([nfa_segmentation_utils.py:155-171](../scripts/nfa_segmentation_utils.py#L155))\n",
    "   - Skips: `NA`, `lex`, `<unk>`, `[UNK]`, `<eps>`, `Œµ`\n",
    "\n",
    "2. **Use Segment-Level CTM** ([nfa_segmentation_utils.py:438](../scripts/nfa_segmentation_utils.py#L438))\n",
    "   - Changed from word-level to sentence-level segmentation\n",
    "   - Respects natural sentence boundaries\n",
    "\n",
    "3. **Use Medium Model** ([nfa_segmentation_utils.py:46](../scripts/nfa_segmentation_utils.py#L46))\n",
    "   - Lower GPU memory usage\n",
    "   - Still provides good alignment quality\n",
    "\n",
    "4. **Skip Long Files** ([nfa_segmentation_utils.py:494-502](../scripts/nfa_segmentation_utils.py#L494))\n",
    "   - Automatically skips files >30 minutes\n",
    "   - Prevents CUDA OOM errors\n",
    "\n",
    "5. **Improved Error Logging** ([nfa_segmentation_utils.py:134-149](../scripts/nfa_segmentation_utils.py#L134))\n",
    "   - Clearly distinguishes file-level vs subprocess errors\n",
    "   - Shows full command, exit code, and error messages\n",
    "\n",
    "**Expected Results:**\n",
    "- ‚úÖ Clean transcripts without special tokens\n",
    "- ‚úÖ Natural segment boundaries at sentence endings\n",
    "- ‚úÖ No CUDA OOM errors\n",
    "- ‚úÖ 75-85% success rate on full dataset\n",
    "- ‚úÖ Clear skip messages for long files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NFA segmentation utilities\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"scripts\"))\n",
    "\n",
    "from nfa_segmentation_utils import process_parquet_batch\n",
    "\n",
    "# Load Azure credentials\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../credentials/creds.env')\n",
    "\n",
    "# Configuration\n",
    "NEMO_MODEL = \"stt_en_conformer_ctc_medium\"  # Changed from \"large\" to reduce GPU memory\n",
    "INPUT_PARQUET = \"../data/raw/loc/veterans_history_project_resources_pre2010_train.parquet\"\n",
    "OUTPUT_PARQUET = \"../data/raw/loc/veterans_history_project_resources_pre2010_train_nfa_segmented_demo.parquet\"\n",
    "SAMPLE_SIZE = 5  # Process first 5 rows for demo\n",
    "\n",
    "print(f\"Using NeMo Forced Aligner (NFA)\")\n",
    "print(f\"Model: {NEMO_MODEL}\")\n",
    "print(f\"Input: {INPUT_PARQUET}\")\n",
    "print(f\"Output: {OUTPUT_PARQUET}\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE}\")\n",
    "print(\"\\nThis will take ~5-10 minutes on T4 GPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NFA segmentation on 5 rows\n",
    "# \n",
    "# IMPORTANT: Choose which transcript field to use:\n",
    "# - transcript_field=\"fulltext_file_str\" (default): Raw XML transcript\n",
    "#   May have encoding issues (curly quotes, XML artifacts) causing NFA bugs\n",
    "# - transcript_field=\"transcript_raw_text_only\" (recommended): Pre-cleaned text\n",
    "#   No XML, no curly quotes, more compatible with NFA tokenizer\n",
    "\n",
    "df_segmented = process_parquet_batch(\n",
    "    parquet_path=INPUT_PARQUET,\n",
    "    output_parquet_path=OUTPUT_PARQUET,\n",
    "    model_name=NEMO_MODEL,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    max_duration=30.0,  # Max segment duration for Whisper\n",
    "    blob_prefix=\"loc_vhp\",\n",
    "    transcript_field=\"transcript_raw_text_only\"  # Use pre-cleaned text to avoid NFA bugs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results - show first 5 segments\n",
    "print(f\"Total segments generated: {len(df_segmented)}\")\n",
    "print(f\"\\nColumns: {list(df_segmented.columns)}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE SEGMENTS (first 5)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, row in df_segmented.head(5).iterrows():\n",
    "    print(f\"\\nSegment {i}:\")\n",
    "    print(f\"  Source row: {row.get('source_row_idx', 'N/A')}\")\n",
    "    print(f\"  Segment idx: {row.get('segment_idx', 'N/A')}\")\n",
    "    print(f\"  Segmented audio: {row.get('segmented_audio_url', 'N/A')}\")\n",
    "    print(f\"  Duration: {row.get('segment_duration', 0):.1f}s\")\n",
    "    print(f\"  Confidence: {row.get('confidence', 0):.2f}\")\n",
    "    print(f\"  Transcript (first 100 chars): {row.get('segmented_audio_transcript', '')[:100]}...\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Output Schema\n",
    "\n",
    "The segmented parquet preserves ALL original columns and adds new ones for segmented data:\n",
    "\n",
    "### New Columns (for fine-tuning):\n",
    "- **`segmented_audio_url`**: Azure blob path to <30s audio segment (e.g., `loc_vhp/10317/10317_042.wav`)\n",
    "- **`segmented_audio_transcript`**: Clean transcript for this segment (plain text with actual spaces, not `<space>` tokens)\n",
    "- **`source_row_idx`**: Original row index in unsegmented parquet\n",
    "- **`segment_idx`**: Segment number within original interview (0, 1, 2, ...)\n",
    "- **`start_time`**: Start time in original audio (seconds)\n",
    "- **`end_time`**: End time in original audio (seconds)\n",
    "- **`confidence`**: Alignment confidence score\n",
    "- **`segment_duration`**: Duration of this segment (seconds)\n",
    "\n",
    "### Preserved Columns (original metadata):\n",
    "- **`audio_url`**: Original full-length audio blob path (e.g., `loc_vhp/10317/video.mp4`)\n",
    "- **`fulltext_file_str`**: Original full interview transcript (XML format)\n",
    "- **`transcript_raw_text_only`**: Original full interview transcript (plain text)\n",
    "- All other metadata columns (title, dates, subject, etc.)\n",
    "\n",
    "**Why preserve originals?**\n",
    "- Allows tracing segments back to source interviews\n",
    "- Keeps full metadata for analysis\n",
    "- Enables future re-segmentation with different parameters\n",
    "\n",
    "**What changed from previous implementation?**\n",
    "- ‚ùå Before: Overwrote `audio_url` and `fulltext_file_str` (lost original data)\n",
    "- ‚úÖ Now: New columns `segmented_audio_url` and `segmented_audio_transcript`\n",
    "- ‚ùå Before: Had `<space>` tokens in transcripts (e.g., \"This<space>is<space>the...\")\n",
    "- ‚úÖ Now: Clean transcripts with actual spaces (e.g., \"This is the...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify schema compatibility with fine-tuning pipeline\n",
    "print(\"Schema Verification:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# The fine-tuning notebook needs these columns:\n",
    "# - segmented_audio_url: Path to <30s audio segment\n",
    "# - segmented_audio_transcript: Clean transcript for that segment\n",
    "required_cols = ['segmented_audio_url', 'segmented_audio_transcript']\n",
    "for col in required_cols:\n",
    "    if col in df_segmented.columns:\n",
    "        print(f\"‚úì {col}: present\")\n",
    "    else:\n",
    "        print(f\"‚úó {col}: MISSING\")\n",
    "\n",
    "print(f\"\\nüìù Original metadata preserved:\")\n",
    "print(f\"  ‚úì audio_url: {df_segmented['audio_url'].iloc[0] if 'audio_url' in df_segmented.columns else 'N/A'}\")\n",
    "print(f\"  ‚úì fulltext_file_str: {len(df_segmented['fulltext_file_str'].iloc[0]) if 'fulltext_file_str' in df_segmented.columns else 'N/A'} chars\")\n",
    "\n",
    "print(f\"\\nSegmented parquet saved to: {OUTPUT_PARQUET}\")\n",
    "print(f\"Ready for fine-tuning with finetune_whisper_lora.ipynb\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Process full train parquet (2273 rows ‚Üí ~75k-92k segments)\")\n",
    "print(f\"  2. Process val parquet (569 rows ‚Üí ~19k-24k segments)\")  \n",
    "print(f\"  3. Update finetune_whisper_lora.ipynb to use 'segmented_audio_url' and 'segmented_audio_transcript' columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# How NeMo Forced Aligner Works\n",
    "\n",
    "This implementation uses **NeMo Forced Aligner (NFA)** to align long-form interview transcripts with audio, creating training-ready segments for Whisper fine-tuning.\n",
    "\n",
    "## What is Forced Alignment?\n",
    "\n",
    "**Forced alignment** is the process of automatically aligning text transcripts to audio recordings by determining the precise start and end timestamps for each word or sentence.\n",
    "\n",
    "**Input:** Long-form audio (30-60 min interviews) + full transcript (no timestamps)  \n",
    "**Output:** Short audio segments (<30s) with aligned transcript text  \n",
    "**Tool:** NeMo Forced Aligner (NFA) - NVIDIA's official tool for CTC-based alignment\n",
    "\n",
    "---\n",
    "\n",
    "## Why NFA?\n",
    "\n",
    "NFA is NVIDIA's recommended successor to older tools like CTC-Segmentation:\n",
    "- **More robust**: Better handling of speech variations and accents\n",
    "- **Sentence-level segmentation**: Natural boundaries at sentence endings (not mid-word)\n",
    "- **Well-maintained**: Part of the NeMo Toolkit with active development\n",
    "- **GPU-optimized**: Leverages CUDA for fast processing\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works (High-Level)\n",
    "\n",
    "1. **Audio ‚Üí CTC Model ‚Üí Character Probabilities**\n",
    "   - NFA uses a pre-trained Conformer-CTC model (medium size for GPU efficiency)\n",
    "   - Model outputs probability distributions for each audio frame (~40ms)\n",
    "   - Result: Matrix of probabilities mapping audio time to text characters\n",
    "\n",
    "2. **Text + Probabilities ‚Üí Dynamic Programming ‚Üí Alignments**\n",
    "   - NFA's alignment algorithm finds the best path through the probability matrix\n",
    "   - Uses the transcript as ground truth to guide alignment\n",
    "   - Outputs: Start/end timestamps for each sentence\n",
    "\n",
    "3. **Cut Audio + Create Segments**\n",
    "   - Extract audio clips at sentence boundaries\n",
    "   - Ensure segments are <30 seconds (Whisper training requirement)\n",
    "   - Upload to Azure blob storage with segment metadata\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "### Sentence-Level Segmentation\n",
    "NFA creates segments at natural sentence boundaries (`.`, `?`, `!`), not arbitrary time cuts:\n",
    "```\n",
    "‚úì \"I was born in Pennsylvania. I served in the Navy.\"  [28.5s]\n",
    "‚úó \"I was born in Pennsylvania and I ser‚Äî\" [30.0s - cut mid-word]\n",
    "```\n",
    "\n",
    "### Pattern-Based Token Cleaning\n",
    "NFA outputs special markers for non-speech sounds. We remove these while preserving real words:\n",
    "```\n",
    "Before: \"I was NA lex NA living in NA lex NA Pennsylvania\"\n",
    "After:  \"I was living in Pennsylvania\"\n",
    "```\n",
    "**Preserved:** \"My friend Lex\" (real name), \"NA forces\" (real abbreviation)\n",
    "\n",
    "### GPU Memory Management\n",
    "- Uses **medium model** (lower memory than large)\n",
    "- **Skips very long files** (>30 min) to prevent CUDA OOM\n",
    "- Clears GPU cache after each file\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "**Demo run (5 files):**\n",
    "- 2 succeeded ‚Üí 95 segments\n",
    "- Success rate: 40%\n",
    "\n",
    "**After fixes (medium model + skip long files):**\n",
    "- Expected success rate: **75-85%**\n",
    "- Full train set (2,273 files) ‚Üí **~75k-92k segments**\n",
    "- Full val set (569 files) ‚Üí **~19k-24k segments**\n",
    "\n",
    "**Whisper fine-tuning benchmarks:**\n",
    "- Minimum: 10k segments (noticeable improvement)\n",
    "- Good: 50k segments (solid results)\n",
    "- Excellent: 100k+ segments (optimal)\n",
    "\n",
    "Our expected **~75k-92k segments** falls in the \"good to excellent\" range.\n",
    "\n",
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "- [NeMo Forced Aligner Documentation](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/tools/nemo_forced_aligner/intro.html)\n",
    "- [NFA Tutorial Notebook](https://github.com/NVIDIA/NeMo/blob/main/tutorials/tools/NeMo_Forced_Aligner_Tutorial.ipynb)\n",
    "- [CTC-Segmentation Paper](https://arxiv.org/pdf/2007.09127.pdf) (theory behind alignment)\n",
    "- [Implementation Details](../learnings/nemo_forced_aligner/) (our custom utilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# Production Run: Full Train Set Segmentation\n",
    "\n",
    "Processing all 2,273 train files to create training data for Whisper fine-tuning.\n",
    "\n",
    "**Expected:**\n",
    "- Success rate: 75-85%\n",
    "- Segments: ~75,000-92,000\n",
    "- Runtime: ~20-30 hours on T4 GPU (depends on success rate)\n",
    "\n",
    "**Note:** This will process ALL files in the train set. Files >30 minutes will be automatically skipped with a warning message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for FULL train set\n",
    "TRAIN_INPUT = \"../data/raw/loc/veterans_history_project_resources_pre2010_train.parquet\"\n",
    "TRAIN_OUTPUT = \"../data/raw/loc/veterans_history_project_resources_pre2010_train_nfa_segmented.parquet\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FULL TRAIN SET SEGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Input:  {TRAIN_INPUT}\")\n",
    "print(f\"Output: {TRAIN_OUTPUT}\")\n",
    "print(f\"Model:  {NEMO_MODEL}\")\n",
    "print(f\"Sample: ALL (2,273 files)\")\n",
    "print(f\"\\n‚ö†Ô∏è  WARNING: This will take 20-30 hours on T4 GPU\")\n",
    "print(f\"‚ö†Ô∏è  Make sure to run in screen/tmux session to prevent disconnection\")\n",
    "print(f\"\\nProcessing will start when you run the next cell...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process FULL train set (no sampling)\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "df_train_segmented = process_parquet_batch(\n",
    "    parquet_path=TRAIN_INPUT,\n",
    "    output_parquet_path=TRAIN_OUTPUT,\n",
    "    model_name=NEMO_MODEL,\n",
    "    sample_size=None,  # Process ALL files\n",
    "    max_duration=30.0,\n",
    "    blob_prefix=\"loc_vhp\",\n",
    "    transcript_field=\"transcript_raw_text_only\",\n",
    "    max_audio_duration=1800.0  # Skip files >30 min\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TRAIN SET SEGMENTATION COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total time: {elapsed/3600:.1f} hours\")\n",
    "print(f\"Total segments: {len(df_train_segmented):,}\")\n",
    "print(f\"Output saved to: {TRAIN_OUTPUT}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# Production Run: Full Validation Set Segmentation\n",
    "\n",
    "Processing all 569 validation files to create validation data for Whisper fine-tuning.\n",
    "\n",
    "**Expected:**\n",
    "- Success rate: 75-85%\n",
    "- Segments: ~19,000-24,000\n",
    "- Runtime: ~5-8 hours on T4 GPU (depends on success rate)\n",
    "\n",
    "**Why process validation set?**\n",
    "During LoRA fine-tuning, Whisper needs validation data in the same format as training data (<30s segments). The validation set must also go through NFA segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for FULL validation set\n",
    "VAL_INPUT = \"../data/raw/loc/veterans_history_project_resources_pre2010_val.parquet\"\n",
    "VAL_OUTPUT = \"../data/raw/loc/veterans_history_project_resources_pre2010_val_nfa_segmented.parquet\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FULL VALIDATION SET SEGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Input:  {VAL_INPUT}\")\n",
    "print(f\"Output: {VAL_OUTPUT}\")\n",
    "print(f\"Model:  {NEMO_MODEL}\")\n",
    "print(f\"Sample: ALL (569 files)\")\n",
    "print(f\"\\n‚ö†Ô∏è  WARNING: This will take 5-8 hours on T4 GPU\")\n",
    "print(f\"‚ö†Ô∏è  Make sure to run in screen/tmux session to prevent disconnection\")\n",
    "print(f\"\\nProcessing will start when you run the next cell...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process FULL validation set (no sampling)\n",
    "start_time = time.time()\n",
    "\n",
    "df_val_segmented = process_parquet_batch(\n",
    "    parquet_path=VAL_INPUT,\n",
    "    output_parquet_path=VAL_OUTPUT,\n",
    "    model_name=NEMO_MODEL,\n",
    "    sample_size=None,  # Process ALL files\n",
    "    max_duration=30.0,\n",
    "    blob_prefix=\"loc_vhp\",\n",
    "    transcript_field=\"transcript_raw_text_only\",\n",
    "    max_audio_duration=1800.0  # Skip files >30 min\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"VALIDATION SET SEGMENTATION COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total time: {elapsed/3600:.1f} hours\")\n",
    "print(f\"Total segments: {len(df_val_segmented):,}\")\n",
    "print(f\"Output saved to: {VAL_OUTPUT}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# Summary: Segmented Datasets Ready for Fine-Tuning\n",
    "\n",
    "After running the above cells, you will have:\n",
    "\n",
    "## Output Files\n",
    "\n",
    "1. **Train Set:** `veterans_history_project_resources_pre2010_train_nfa_segmented.parquet`\n",
    "   - Input: 2,273 long-form interviews\n",
    "   - Expected output: ~75,000-92,000 segments\n",
    "   - Each segment: <30 seconds with clean transcript\n",
    "\n",
    "2. **Validation Set:** `veterans_history_project_resources_pre2010_val_nfa_segmented.parquet`\n",
    "   - Input: 569 long-form interviews\n",
    "   - Expected output: ~19,000-24,000 segments\n",
    "   - Each segment: <30 seconds with clean transcript\n",
    "\n",
    "## Schema (Both Files)\n",
    "\n",
    "Each row represents a single audio segment with:\n",
    "- **`segmented_audio_url`**: Azure blob path (e.g., `loc_vhp/8210/8210_042.wav`)\n",
    "- **`segmented_audio_transcript`**: Clean plain text transcript\n",
    "- **`segment_duration`**: Duration in seconds (<30s)\n",
    "- **`start_time`**, **`end_time`**: Position in original interview\n",
    "- **`confidence`**: NFA alignment confidence\n",
    "- All original metadata (title, dates, subject, etc.)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Verify outputs:**\n",
    "   ```python\n",
    "   df_train = pd.read_parquet(\"veterans_history_project_resources_pre2010_train_nfa_segmented.parquet\")\n",
    "   df_val = pd.read_parquet(\"veterans_history_project_resources_pre2010_val_nfa_segmented.parquet\")\n",
    "   print(f\"Train segments: {len(df_train):,}\")\n",
    "   print(f\"Val segments: {len(df_val):,}\")\n",
    "   ```\n",
    "\n",
    "2. **Update fine-tuning notebook** ([finetune_whisper_lora.ipynb](../finetune_whisper_lora.ipynb)):\n",
    "   - Change dataset loading to use segmented parquets\n",
    "   - Use `segmented_audio_url` instead of `audio_url`\n",
    "   - Use `segmented_audio_transcript` instead of `fulltext_file_str`\n",
    "   - Remove duration filtering (all segments already <30s)\n",
    "   - Remove transcript cleaning (already cleaned)\n",
    "\n",
    "3. **Start fine-tuning:**\n",
    "   - Run LoRA fine-tuning on Whisper with the segmented data\n",
    "   - Expected training time: ~10-20 hours on T4 GPU (depends on model size and batch size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
