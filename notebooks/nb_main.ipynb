{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "naId = \"\"\n",
    "parentNaId = \"\"\n",
    "# define env vars\n",
    "API_KEY_NA = \"hzj5ASoq1aj5bHbl8XNI3roMecW9oAm7TOBMjkre\"\n",
    "# Define API endpoints\n",
    "API_ENDPOINT_RECORD_NA = 'https://catalog.archives.gov/api/v2/records/search'\n",
    "API_ENDPOINT_TRANSCRIPT_NA = 'https://catalog.archives.gov/api/v2/transcriptions/naId/'\n",
    "API_ENDPOINT_COLLECTION_NA = 'https://catalog.archives.gov/api/v2/records/parentNaId/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import polars as pl\n",
    "import datetime as dt\n",
    "\n",
    "def fetch_transcription_and_contributors(naId, api_key=API_KEY_NA, api_endpoint=API_ENDPOINT_TRANSCRIPT_NA):\n",
    "    response = requests.get(api_endpoint + naId, headers={'Content-Type': 'application/json', 'x-api-key': api_key})\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if hit := data['body']['hits']['hits']:\n",
    "            transcription_objects = []\n",
    "            for x in hit:\n",
    "                object_id = x['_source']['record']['target']['objectId']\n",
    "                transcriptions_raw = x['_source']['record']['contribution']\n",
    "                contributors_info = x['_source']['record']['contributors']\n",
    "                transcription_object = {object_id: {\"transcription\":transcriptions_raw, \"contributors_info\": contributors_info}}\n",
    "                transcription_objects.append(transcription_object)\n",
    "            # its not clear how the transcription hits are ordered\n",
    "            # we have to assume latest comes first\n",
    "            return transcription_objects\n",
    "        else:\n",
    "            print(\"No hits found in response.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parentNaId = \"653144\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pl.DataFrame(\n",
    "#     {\n",
    "#         \"name\": [\"Alice Archer\", \"Ben Brown\", \"Chloe Cooper\", \"Daniel Donovan\"],\n",
    "#         \"birthdate\": [\n",
    "#             dt.date(1997, 1, 10),\n",
    "#             dt.date(1985, 2, 15),\n",
    "#             dt.date(1983, 3, 22),\n",
    "#             dt.date(1981, 4, 30),\n",
    "#         ],\n",
    "#         \"weight\": [57.9, 72.5, 53.6, 83.1],  # (kg)\n",
    "#         \"height\": [1.56, 1.77, 1.65, 1.75],  # (m)\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# print(df)\n",
    "# Fetch all pages of results and concatenate them into a single DataFrame\n",
    "all_hits = []\n",
    "page = 1\n",
    "limit = 100  # Use a higher limit if the API allows\n",
    "\n",
    "while True:\n",
    "    params = {\n",
    "        'page': page,\n",
    "        'limit': limit,\n",
    "    }\n",
    "    response = requests.get(\n",
    "        API_ENDPOINT_COLLECTION_NA + parentNaId,\n",
    "        headers={'Content-Type': 'application/json', 'x-api-key': API_KEY_NA},\n",
    "        params=params\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "    hits = response.json().get('body', {}).get('hits', {}).get('hits', [])\n",
    "    if not hits:\n",
    "        break\n",
    "    all_hits.extend(hits)\n",
    "    if len(hits) < limit:\n",
    "        break  # Last page reached\n",
    "    page += 1\n",
    "\n",
    "if all_hits:\n",
    "    try:\n",
    "        df = pl.DataFrame(all_hits, strict=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DataFrame: {e}\")\n",
    "        df = pl.DataFrame()\n",
    "else:\n",
    "    print(\"No hits found in response.\")\n",
    "    df = pl.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    pl.col('_source').map_elements(\n",
    "        lambda x: x['record']['title'] if 'record' in x and 'title' in x['record'] else None\n",
    "    ).alias('hit_title')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply this logic: hit_record_urls = [x['objectUrl'] for x in i['record']['digitalObjects']] to create a new column\n",
    "df = df.with_columns(\n",
    "    pl.col('_source').map_elements(\n",
    "        lambda x: [obj['objectUrl'] for obj in (x['record'].get('digitalObjects') or [])] if 'record' in x and 'digitalObjects' in x['record'] else []\n",
    "    ).alias('hit_record_urls')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    pl.col('_source').map_elements(\n",
    "        lambda x: x['record'].get('digitalObjects') if 'record' in x else None\n",
    "    ).alias('hit_digitalObjects_metadata')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    pl.col('_id').map_elements(lambda naId: fetch_transcription_and_contributors(naId)).alias('transcription')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe as a parquet file\n",
    "df.write_parquet(f'{parentNaId}_transcriptions.parquet', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the parquet file and read it back\n",
    "df = pl.read_parquet(f'{parentNaId}_transcriptions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode(\"transcription\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out where transcription is None\n",
    "df = df.filter(pl.col(\"transcription\").is_not_null())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out where transcription is any empty dict\n",
    "df_transcriptions = df.filter(pl.col(\"transcription\").map_elements(lambda x: bool(x) and isinstance(x, dict) and any(value is not None for value in x.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each item in the transcription column, extract the object_id and contributors_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def download_mp3(url):\n",
    "    os.makedirs('./audio', exist_ok=True)\n",
    "    filename = url.split('/')[-1]\n",
    "    filepath = os.path.join('./audio', filename)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return filepath\n",
    "    else:\n",
    "        print(f\"Failed to download {url}: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "    \n",
    "# url_test = \"https://s3.amazonaws.com/NARAprodstorage/lz/mopix/208a/GENERALa/208-192.mp3\"\n",
    "\n",
    "# # Download a test MP3 file\n",
    "# downloaded_file = download_mp3(url_test)\n",
    "\n",
    "# for each item in df_transcriptions['hit_record_urls'] (list of URLs), download the audio files and mark the filepaths in a new column 'audio_filepaths'\n",
    "df_transcriptions = df_transcriptions.with_columns(\n",
    "    pl.col('hit_record_urls').map_elements(\n",
    "        lambda urls: [download_mp3(url) for url in urls if url]  # Filter out None URLs\n",
    "    ).alias('audio_filepaths')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export df_transcriptions to a parquet file\n",
    "df_transcriptions.write_parquet(f'{parentNaId}_transcriptions_with_audio.parquet', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "params = {\n",
    "    'q': 'world war',\n",
    "    # 'naID': [\"2131159\"],\n",
    "    'rows': 10,\n",
    "    'api_key': API_KEY_NA\n",
    "}\n",
    "# Make the API request\n",
    "response = requests.get(API_ENDPOINT_RECORD_NA, params=params, headers={'Content-Type': 'application/json', 'x-api-key': API_KEY_NA})\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    print(\"Records fetched successfully:\")\n",
    "    # print(response.json())\n",
    "else:\n",
    "    # Print the error message\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")\n",
    "# This script fetches records from the National Archives and Records Administration (NARA) API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan: for each collection, get all the records' naIds, fetch audio + metadata, and check if there are transcriptions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
