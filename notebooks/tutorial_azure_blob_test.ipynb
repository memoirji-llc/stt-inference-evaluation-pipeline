{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Azure Blob Storage Authentication & Audio Playback\n",
    "\n",
    "**Purpose:** Test Azure blob storage connection and verify audio files are accessible\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Load environment variables (Azure credentials)\n",
    "2. Inspect the VHP parquet dataset\n",
    "3. Pick one row and identify its blob path\n",
    "4. Authenticate with Azure blob storage\n",
    "5. Download and play the audio file\n",
    "\n",
    "**Use this to debug:**\n",
    "- Azure authentication issues\n",
    "- Blob path mapping problems\n",
    "- Audio file accessibility\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Load Environment Variables and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Load Azure credentials from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../credentials/creds.env')\n",
    "\n",
    "# Add scripts directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"scripts\"))\n",
    "\n",
    "# Import Azure utilities\n",
    "import azure_utils\n",
    "import data_loader\n",
    "\n",
    "print(\"âœ“ Imports loaded\")\n",
    "print(\"\\nEnvironment variables check:\")\n",
    "print(f\"  AZURE_STORAGE_CONTAINER: {os.getenv('AZURE_STORAGE_CONTAINER', 'NOT SET')}\")\n",
    "print(f\"  AZURE_STORAGE_ACCOUNT: {os.getenv('AZURE_STORAGE_ACCOUNT', 'NOT SET')}\")\n",
    "print(f\"  AZURE_AUTH: {os.getenv('AZURE_AUTH', 'NOT SET (defaults to managed_identity)')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect Parquet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VHP pre-2010 parquet file\n",
    "parquet_path = \"../data/raw/loc/veterans_history_project_resources_pre2010.parquet\"\n",
    "df = pd.read_parquet(parquet_path)\n",
    "\n",
    "print(f\"Parquet shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nKey columns for blob mapping:\")\n",
    "print(f\"  - azure_blob_index: {'âœ“ Present' if 'azure_blob_index' in df.columns else 'âœ— Missing'}\")\n",
    "print(f\"  - video_url: {df['video_url'].notna().sum()} items\")\n",
    "print(f\"  - audio_url: {df['audio_url'].notna().sum()} items\")\n",
    "print(f\"  - fulltext_file_str (transcripts): {df['fulltext_file_str'].notna().sum()} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pick One Row with Media and Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows with both transcript and media\n",
    "has_transcript = df['fulltext_file_str'].notna()\n",
    "has_media = (df['video_url'].notna()) | (df['audio_url'].notna())\n",
    "df_filtered = df[has_transcript & has_media]\n",
    "\n",
    "print(f\"Rows with both transcript and media: {len(df_filtered)}\")\n",
    "\n",
    "# Pick the first row from the sample we've been testing\n",
    "# (parquet index 5047 -> azure_blob_index 6357)\n",
    "test_parquet_idx = 5047\n",
    "test_row = df.loc[test_parquet_idx]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Test Row Selected:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Parquet index: {test_parquet_idx}\")\n",
    "print(f\"Azure blob index: {int(test_row['azure_blob_index'])}\")\n",
    "print(f\"Collection number: {test_row['collection_number']}\")\n",
    "print(f\"Title: {test_row['title'][:80]}...\" if len(str(test_row['title'])) > 80 else f\"Title: {test_row['title']}\")\n",
    "print(f\"Has video URL: {pd.notna(test_row['video_url'])}\")\n",
    "print(f\"Has audio URL: {pd.notna(test_row['audio_url'])}\")\n",
    "print(f\"Has transcript: {pd.notna(test_row['fulltext_file_str'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify Blob Path for This Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data_loader to construct blob path (same logic as inference scripts)\n",
    "blob_prefix = \"loc_vhp\"\n",
    "blob_path_candidates = data_loader.get_blob_path_for_row(\n",
    "    test_row, \n",
    "    idx=test_parquet_idx, \n",
    "    blob_prefix=blob_prefix\n",
    ")\n",
    "\n",
    "print(f\"\\nBlob path candidates for this row:\")\n",
    "for i, path in enumerate(blob_path_candidates, 1):\n",
    "    print(f\"  {i}. {path}\")\n",
    "\n",
    "# These paths should match what commercial models successfully used\n",
    "print(f\"\\nðŸ’¡ Note: AWS Transcribe and Chirp2 successfully used these paths before\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Azure Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create blob service client\n",
    "print(\"Creating Azure Blob Service client...\\n\")\n",
    "\n",
    "try:\n",
    "    blob_service = azure_utils.make_blob_service()\n",
    "    print(\"âœ“ Blob service client created\")\n",
    "    \n",
    "    # Test connection by listing containers\n",
    "    print(\"\\nTesting connection by listing containers...\")\n",
    "    containers = list(blob_service.list_containers())\n",
    "    print(f\"âœ“ Successfully authenticated! Found {len(containers)} container(s):\")\n",
    "    for container in containers:\n",
    "        print(f\"  - {container.name}\")\n",
    "    \n",
    "    # Verify the expected container exists\n",
    "    container_name = os.getenv('AZURE_STORAGE_CONTAINER')\n",
    "    container_names = [c.name for c in containers]\n",
    "    if container_name in container_names:\n",
    "        print(f\"\\nâœ“ Target container '{container_name}' found\")\n",
    "    else:\n",
    "        print(f\"\\nâœ— WARNING: Target container '{container_name}' not found!\")\n",
    "        print(f\"  Available containers: {container_names}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Authentication failed: {e}\")\n",
    "    print(f\"\\nError type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    print(f\"\\nFull traceback:\\n{traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check if Blob Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each candidate blob path\n",
    "print(f\"Checking blob existence for {len(blob_path_candidates)} candidate(s):\\n\")\n",
    "\n",
    "existing_path = None\n",
    "for path in blob_path_candidates:\n",
    "    print(f\"Checking: {path}\")\n",
    "    try:\n",
    "        exists = azure_utils.blob_exists(path)\n",
    "        if exists:\n",
    "            print(f\"  âœ“ Found!\")\n",
    "            existing_path = path\n",
    "            break\n",
    "        else:\n",
    "            print(f\"  âœ— Not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error checking: {type(e).__name__}: {e}\")\n",
    "\n",
    "if existing_path:\n",
    "    print(f\"\\nâœ“ SUCCESS: Found blob at {existing_path}\")\n",
    "else:\n",
    "    print(f\"\\nâœ— FAILED: None of the candidate paths exist\")\n",
    "    print(f\"\\nThis suggests either:\")\n",
    "    print(f\"  1. Blobs were not uploaded to this prefix\")\n",
    "    print(f\"  2. Azure authentication doesn't have read permissions\")\n",
    "    print(f\"  3. Wrong container or storage account configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download and Inspect Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if existing_path:\n",
    "    print(f\"Downloading blob: {existing_path}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Download to memory\n",
    "        audio_bytes = azure_utils.download_blob_to_memory(existing_path)\n",
    "        \n",
    "        file_size_mb = len(audio_bytes) / (1024 * 1024)\n",
    "        print(f\"âœ“ Downloaded successfully!\")\n",
    "        print(f\"  File size: {file_size_mb:.2f} MB ({len(audio_bytes):,} bytes)\")\n",
    "        \n",
    "        # Check if it looks like valid audio data\n",
    "        if len(audio_bytes) < 100:\n",
    "            print(f\"\\nâš ï¸  WARNING: File is very small ({len(audio_bytes)} bytes) - may be corrupted\")\n",
    "        else:\n",
    "            # Check file signature\n",
    "            if existing_path.endswith('.mp3'):\n",
    "                # MP3 files typically start with ID3 or FF FB/FF F3 (MPEG sync)\n",
    "                header = audio_bytes[:4]\n",
    "                if header[:3] == b'ID3' or header[:2] in [b'\\xff\\xfb', b'\\xff\\xf3']:\n",
    "                    print(f\"  âœ“ File signature looks like valid MP3\")\n",
    "                else:\n",
    "                    print(f\"  âš ï¸  File signature doesn't match MP3 (first 4 bytes: {header.hex()})\")\n",
    "            elif existing_path.endswith('.mp4'):\n",
    "                # MP4 files have 'ftyp' at bytes 4-8\n",
    "                header = audio_bytes[:12]\n",
    "                if b'ftyp' in header:\n",
    "                    print(f\"  âœ“ File signature looks like valid MP4\")\n",
    "                else:\n",
    "                    print(f\"  âš ï¸  File signature doesn't match MP4 (first 12 bytes: {header.hex()})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Download failed: {e}\")\n",
    "        print(f\"\\nError type: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        print(f\"\\nFull traceback:\\n{traceback.format_exc()}\")\n",
    "else:\n",
    "    print(\"âŠ˜ Skipping download - no blob found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load and Play Audio (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if existing_path and 'audio_bytes' in locals():\n",
    "    print(\"Loading audio with pydub...\\n\")\n",
    "    \n",
    "    try:\n",
    "        import io\n",
    "        from pydub import AudioSegment\n",
    "        \n",
    "        # Load audio from bytes\n",
    "        audio_segment = AudioSegment.from_file(io.BytesIO(audio_bytes))\n",
    "        \n",
    "        print(f\"âœ“ Audio loaded successfully!\")\n",
    "        print(f\"  Duration: {len(audio_segment) / 1000:.1f} seconds ({len(audio_segment) / 60000:.1f} minutes)\")\n",
    "        print(f\"  Sample rate: {audio_segment.frame_rate} Hz\")\n",
    "        print(f\"  Channels: {audio_segment.channels} ({'mono' if audio_segment.channels == 1 else 'stereo'})\")\n",
    "        print(f\"  Sample width: {audio_segment.sample_width} bytes\")\n",
    "        \n",
    "        # Play first 10 seconds (if running locally with audio output)\n",
    "        print(f\"\\nðŸ’¡ To play audio, run: audio_segment[:10000].export('test.wav', format='wav')\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Audio loading failed: {e}\")\n",
    "        print(f\"\\nError type: {type(e).__name__}\")\n",
    "else:\n",
    "    print(\"âŠ˜ Skipping audio playback - no audio downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Diagnostics\n",
    "\n",
    "This notebook tests the full Azure blob authentication and access pipeline:\n",
    "\n",
    "**Expected results if everything works:**\n",
    "- âœ“ Environment variables loaded\n",
    "- âœ“ Blob service client created\n",
    "- âœ“ Can list containers\n",
    "- âœ“ Target container exists\n",
    "- âœ“ Blob exists at expected path\n",
    "- âœ“ Can download blob\n",
    "- âœ“ File has valid audio signature\n",
    "- âœ“ Can load and inspect audio\n",
    "\n",
    "**Common issues:**\n",
    "1. **Env vars not loaded**: Check `credentials/creds.env` exists and has correct values\n",
    "2. **Auth failed**: Check Azure credentials are valid and not expired\n",
    "3. **Container not found**: Check `AZURE_STORAGE_CONTAINER` matches actual container name\n",
    "4. **Blob not found**: Check `azure_blob_index` in parquet matches uploaded blobs\n",
    "5. **Permission denied**: Check Azure credentials have Storage Blob Data Reader role\n",
    "\n",
    "**Next steps if this succeeds:**\n",
    "- Run `tutorial_infer_canary.ipynb` to test full inference pipeline\n",
    "- This proves Azure access works, so any failures in inference are model-related"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
