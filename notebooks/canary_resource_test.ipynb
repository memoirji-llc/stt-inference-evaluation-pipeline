{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canary Models Resource Testing\n",
    "\n",
    "**Purpose:** Test actual RAM and GPU memory requirements for Canary models on your VM.\n",
    "\n",
    "**This notebook will:**\n",
    "1. Check your current system resources (RAM, GPU)\n",
    "2. Test loading Canary-1B (smallest model)\n",
    "3. Test loading Canary-Qwen-2.5B (larger model)\n",
    "4. Monitor memory usage at each step\n",
    "5. Run inference on 1 sample to measure peak memory\n",
    "\n",
    "**Your VM specs:**\n",
    "- 28GB RAM\n",
    "- 16GB T4 GPU\n",
    "- 4 vCPUs\n",
    "\n",
    "**Expected results (based on research):**\n",
    "- Canary-1B: 6GB RAM minimum (should work)\n",
    "- Canary-Qwen-2.5B: ~8-12GB RAM estimated (should work)\n",
    "- GPU VRAM: 6-10GB per model\n",
    "\n",
    "---\n",
    "\n",
    "## How to use this notebook:\n",
    "\n",
    "1. **Run cells sequentially** - don't skip ahead\n",
    "2. **Monitor in terminal** - keep `nvidia-smi` and `htop` running\n",
    "3. **Check resource cells** - they show memory before/after each step\n",
    "4. **Stop if OOM** - if you hit memory errors, document which step failed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminal Commands for Monitoring (Run in Separate SSH Session)\n",
    "\n",
    "**Option 1: Watch GPU memory every 2 seconds:**\n",
    "```bash\n",
    "watch -n 2 nvidia-smi\n",
    "```\n",
    "\n",
    "**Option 2: Watch system RAM every 2 seconds:**\n",
    "```bash\n",
    "watch -n 2 'free -h'\n",
    "```\n",
    "\n",
    "**Option 3: Combined monitoring (one-time check):**\n",
    "```bash\n",
    "nvidia-smi && echo \"---\" && free -h && echo \"---\" && ps aux | grep jupyter | grep -v grep\n",
    "```\n",
    "\n",
    "**Option 4: Log memory usage to file:**\n",
    "```bash\n",
    "# Start logging (run this in separate terminal)\n",
    "while true; do \n",
    "  echo \"[$(date '+%H:%M:%S')]\" >> /tmp/memory_log.txt\n",
    "  free -h | grep Mem >> /tmp/memory_log.txt\n",
    "  nvidia-smi --query-gpu=memory.used,memory.free --format=csv,noheader >> /tmp/memory_log.txt\n",
    "  echo \"---\" >> /tmp/memory_log.txt\n",
    "  sleep 5\n",
    "done\n",
    "\n",
    "# View log (in another terminal)\n",
    "tail -f /tmp/memory_log.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install psutil for RAM monitoring\n",
    "!pip install -q psutil\n",
    "print(\"âœ“ psutil installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Baseline Resource Check (Before Loading Anything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import torch\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "def print_resources(label=\"\"):\n",
    "    \"\"\"Print current RAM and GPU memory usage.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"RESOURCE CHECK: {label}\")\n",
    "    print(f\"Timestamp: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # System RAM\n",
    "    mem = psutil.virtual_memory()\n",
    "    ram_used_gb = mem.used / 1024**3\n",
    "    ram_total_gb = mem.total / 1024**3\n",
    "    ram_avail_gb = mem.available / 1024**3\n",
    "    ram_percent = mem.percent\n",
    "    \n",
    "    print(f\"\\nðŸ“Š SYSTEM RAM:\")\n",
    "    print(f\"  Total:     {ram_total_gb:.2f} GB\")\n",
    "    print(f\"  Used:      {ram_used_gb:.2f} GB ({ram_percent:.1f}%)\")\n",
    "    print(f\"  Available: {ram_avail_gb:.2f} GB\")\n",
    "    \n",
    "    if ram_avail_gb < 10:\n",
    "        print(f\"  âš ï¸  WARNING: Low RAM! Only {ram_avail_gb:.1f} GB available\")\n",
    "    else:\n",
    "        print(f\"  âœ“ RAM looks good\")\n",
    "    \n",
    "    # GPU Memory\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nðŸŽ® GPU MEMORY:\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            total_gb = props.total_memory / 1024**3\n",
    "            allocated_gb = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            reserved_gb = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            free_gb = total_gb - reserved_gb\n",
    "            \n",
    "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"    Total:     {total_gb:.2f} GB\")\n",
    "            print(f\"    Allocated: {allocated_gb:.2f} GB\")\n",
    "            print(f\"    Reserved:  {reserved_gb:.2f} GB\")\n",
    "            print(f\"    Free:      {free_gb:.2f} GB\")\n",
    "            \n",
    "            if free_gb < 6:\n",
    "                print(f\"    âš ï¸  WARNING: Low GPU memory! Only {free_gb:.2f} GB free\")\n",
    "            else:\n",
    "                print(f\"    âœ“ GPU memory looks good\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ NO CUDA GPU AVAILABLE\")\n",
    "        print(f\"   Canary models require GPU acceleration!\")\n",
    "    \n",
    "    # Python process memory\n",
    "    process = psutil.Process()\n",
    "    process_mem_gb = process.memory_info().rss / 1024**3\n",
    "    print(f\"\\nðŸ PYTHON PROCESS:\")\n",
    "    print(f\"  Memory: {process_mem_gb:.2f} GB\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "\n",
    "# Initial baseline\n",
    "print_resources(\"BASELINE (No models loaded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clean GPU Memory (Run if needed)\n",
    "\n",
    "If you see any GPU memory already allocated, run this to clear it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force garbage collection and clear GPU cache\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"âœ“ Memory cleared\\n\")\n",
    "print_resources(\"AFTER CLEANUP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Environment Variables\n",
    "\n",
    "Set up model caching and disable wandb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Disable wandb\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "# Set model cache directory\n",
    "model_cache = Path.cwd().parent / \"models\" / \"canary\"\n",
    "model_cache.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ['HF_HOME'] = str(model_cache)\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(model_cache)\n",
    "os.environ['HF_HUB_CACHE'] = str(model_cache)\n",
    "\n",
    "print(f\"âœ“ Environment configured\")\n",
    "print(f\"  Model cache: {model_cache}\")\n",
    "print(f\"  Wandb: disabled\")\n",
    "\n",
    "# Check if models already cached (HuggingFace cache structure)\n",
    "hub_cache = model_cache / \"hub\"\n",
    "canary_1b_cached = False\n",
    "canary_1b_flash_cached = False\n",
    "canary_qwen_cached = False\n",
    "\n",
    "if hub_cache.exists():\n",
    "    # Check for different Canary-1B variants\n",
    "    if any(hub_cache.glob(\"models--nvidia--canary-1b\")):\n",
    "        canary_1b_cached = True\n",
    "    if any(hub_cache.glob(\"models--nvidia--canary-1b-flash\")):\n",
    "        canary_1b_flash_cached = True\n",
    "    if any(hub_cache.glob(\"models--nvidia--canary-qwen*\")):\n",
    "        canary_qwen_cached = True\n",
    "\n",
    "print(f\"\\nðŸ“¦ Model Cache Status:\")\n",
    "print(f\"  Canary-1B (standard): {'âœ“ Cached' if canary_1b_cached else 'âœ— Not cached (will download ~1GB)'}\")\n",
    "print(f\"  Canary-1B-Flash: {'âœ“ Cached' if canary_1b_flash_cached else 'âœ— Not cached (will download ~1GB)'}\")\n",
    "print(f\"  Canary-Qwen-2.5B: {'âœ“ Cached' if canary_qwen_cached else 'âœ— Not cached (will download ~5GB)'}\")\n",
    "\n",
    "# Show what's actually in cache\n",
    "if hub_cache.exists():\n",
    "    cached_models = [d.name for d in hub_cache.iterdir() if d.is_dir() and d.name.startswith(\"models--nvidia--canary\")]\n",
    "    if cached_models:\n",
    "        print(f\"\\n  Found in cache:\")\n",
    "        for model in cached_models:\n",
    "            print(f\"    - {model.replace('models--nvidia--', '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Loading Canary-1B (Smallest Model)\n",
    "\n",
    "**Expected:** 6GB RAM minimum, ~4-6GB GPU VRAM\n",
    "\n",
    "This step will:\n",
    "1. Show memory before loading\n",
    "2. Load the model\n",
    "3. Show memory after loading\n",
    "4. Calculate memory delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Loading Canary-1B model...\\n\")\n",
    "print(\"This will take 1-3 minutes if downloading for the first time.\\n\")\n",
    "\n",
    "# Memory before loading\n",
    "print_resources(\"BEFORE loading Canary-1B\")\n",
    "\n",
    "# Capture baseline\n",
    "mem_before = psutil.virtual_memory()\n",
    "ram_before_gb = mem_before.used / 1024**3\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_before_gb = torch.cuda.memory_allocated(0) / 1024**3\n",
    "else:\n",
    "    gpu_before_gb = 0\n",
    "\n",
    "print(\"â³ Loading model from nvidia/canary-1b...\")\n",
    "\n",
    "try:\n",
    "    import nemo.collections.asr as nemo_asr\n",
    "    \n",
    "    # Load model\n",
    "    model_1b = nemo_asr.models.EncDecMultiTaskModel.from_pretrained(\n",
    "        \"nvidia/canary-1b\",\n",
    "        map_location=\"cuda:0\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ“ Model loaded successfully!\\n\")\n",
    "    \n",
    "    # Memory after loading\n",
    "    print_resources(\"AFTER loading Canary-1B\")\n",
    "    \n",
    "    # Calculate delta\n",
    "    mem_after = psutil.virtual_memory()\n",
    "    ram_after_gb = mem_after.used / 1024**3\n",
    "    ram_delta_gb = ram_after_gb - ram_before_gb\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_after_gb = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        gpu_delta_gb = gpu_after_gb - gpu_before_gb\n",
    "    else:\n",
    "        gpu_after_gb = 0\n",
    "        gpu_delta_gb = 0\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“Š MEMORY DELTA (Canary-1B):\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  RAM increase:  {ram_delta_gb:+.2f} GB\")\n",
    "    print(f\"  GPU increase:  {gpu_delta_gb:+.2f} GB\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    # Verdict\n",
    "    if ram_delta_gb < 10:\n",
    "        print(f\"âœ… RESULT: Canary-1B uses ~{ram_delta_gb:.1f} GB RAM (within VM limits)\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  RESULT: Canary-1B uses ~{ram_delta_gb:.1f} GB RAM (might be tight on 28GB VM)\")\n",
    "    \n",
    "    if gpu_delta_gb < 10:\n",
    "        print(f\"âœ… RESULT: Canary-1B uses ~{gpu_delta_gb:.1f} GB GPU (within T4 limits)\")\n",
    "    else:\n",
    "        print(f\"âŒ RESULT: Canary-1B uses ~{gpu_delta_gb:.1f} GB GPU (exceeds T4 16GB limit)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ERROR loading Canary-1B:\\n{e}\\n\")\n",
    "    print_resources(\"AFTER ERROR\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Inference with Canary-1B\n",
    "\n",
    "Run inference on a short audio clip to measure peak memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Testing inference with Canary-1B...\\n\")\n",
    "\n",
    "# Create a dummy audio tensor (10 seconds of silence at 16kHz)\n",
    "import numpy as np\n",
    "\n",
    "sample_rate = 16000\n",
    "duration_sec = 10\n",
    "audio_length = sample_rate * duration_sec\n",
    "\n",
    "# Generate silence\n",
    "dummy_audio = np.zeros(audio_length, dtype=np.float32)\n",
    "\n",
    "print(f\"Generated dummy audio: {duration_sec}s @ {sample_rate}Hz\\n\")\n",
    "\n",
    "# Memory before inference\n",
    "print_resources(\"BEFORE inference\")\n",
    "\n",
    "mem_before = psutil.virtual_memory()\n",
    "ram_before_gb = mem_before.used / 1024**3\n",
    "gpu_before_gb = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n",
    "\n",
    "try:\n",
    "    # Run transcription with proper parameters\n",
    "    print(\"â³ Running transcription...\")\n",
    "    \n",
    "    # Canary models need source_lang and target_lang parameters\n",
    "    with torch.no_grad():\n",
    "        transcription = model_1b.transcribe(\n",
    "            audio=[dummy_audio],\n",
    "            batch_size=1,\n",
    "            source_lang=\"en\",  # English source\n",
    "            target_lang=\"en\"   # English target (transcription, not translation)\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nâœ“ Transcription completed\\n\")\n",
    "    print(f\"Result: {transcription}\\n\")\n",
    "    \n",
    "    # Memory after inference\n",
    "    print_resources(\"AFTER inference\")\n",
    "    \n",
    "    mem_after = psutil.virtual_memory()\n",
    "    ram_after_gb = mem_after.used / 1024**3\n",
    "    ram_delta_gb = ram_after_gb - ram_before_gb\n",
    "    \n",
    "    gpu_after_gb = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n",
    "    gpu_delta_gb = gpu_after_gb - gpu_before_gb\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“Š INFERENCE MEMORY DELTA:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  RAM increase:  {ram_delta_gb:+.2f} GB (peak overhead during inference)\")\n",
    "    print(f\"  GPU increase:  {gpu_delta_gb:+.2f} GB (activation memory)\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ERROR during inference:\\n{e}\\n\")\n",
    "    print_resources(\"AFTER ERROR\")\n",
    "    \n",
    "    # If source_lang error, suggest skipping inference test\n",
    "    if \"source_lang\" in str(e):\n",
    "        print(\"ðŸ’¡ This is a parameter configuration issue, not a memory issue.\")\n",
    "        print(\"   Model loaded successfully - that's what we needed to test!\")\n",
    "        print(\"   You can skip to Step 6 (cleanup) and continue.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Clean Up Canary-1B Before Testing Larger Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§¹ Cleaning up Canary-1B to free memory...\\n\")\n",
    "\n",
    "# Delete model\n",
    "del model_1b\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"âœ“ Canary-1B unloaded\\n\")\n",
    "print_resources(\"AFTER cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Loading Canary-Qwen-2.5B (Larger Model)\n",
    "\n",
    "**Expected:** ~8-12GB RAM, ~6-10GB GPU VRAM\n",
    "\n",
    "âš ï¸ **This is the model that crashed before.** We'll carefully monitor memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Loading Canary-Qwen-2.5B model...\\n\")\n",
    "print(\"âš ï¸  This is the larger model (2.5B parameters)\\n\")\n",
    "print(\"This will take 3-5 minutes if downloading for the first time (~5GB).\\n\")\n",
    "\n",
    "# Check available memory first\n",
    "mem = psutil.virtual_memory()\n",
    "ram_avail_gb = mem.available / 1024**3\n",
    "\n",
    "if ram_avail_gb < 12:\n",
    "    print(f\"âš ï¸  WARNING: Only {ram_avail_gb:.1f} GB RAM available\")\n",
    "    print(f\"   Estimated need: 8-12 GB\")\n",
    "    print(f\"   This might fail with OOM error!\\n\")\n",
    "    \n",
    "    response = input(\"Continue anyway? (yes/no): \")\n",
    "    if response.lower() != 'yes':\n",
    "        print(\"Skipping Canary-Qwen-2.5B test\")\n",
    "        raise SystemExit(\"User cancelled due to low memory\")\n",
    "\n",
    "# Memory before loading\n",
    "print_resources(\"BEFORE loading Canary-Qwen-2.5B\")\n",
    "\n",
    "mem_before = psutil.virtual_memory()\n",
    "ram_before_gb = mem_before.used / 1024**3\n",
    "gpu_before_gb = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n",
    "\n",
    "print(\"â³ Loading model from nvidia/canary-qwen-2.5b...\")\n",
    "\n",
    "try:\n",
    "    import nemo.collections.asr as nemo_asr\n",
    "    \n",
    "    # Load model\n",
    "    model_qwen = nemo_asr.models.EncDecMultiTaskModel.from_pretrained(\n",
    "        \"nvidia/canary-qwen-2.5b\",\n",
    "        map_location=\"cuda:0\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ“ Model loaded successfully!\\n\")\n",
    "    \n",
    "    # Memory after loading\n",
    "    print_resources(\"AFTER loading Canary-Qwen-2.5B\")\n",
    "    \n",
    "    # Calculate delta\n",
    "    mem_after = psutil.virtual_memory()\n",
    "    ram_after_gb = mem_after.used / 1024**3\n",
    "    ram_delta_gb = ram_after_gb - ram_before_gb\n",
    "    \n",
    "    gpu_after_gb = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n",
    "    gpu_delta_gb = gpu_after_gb - gpu_before_gb\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“Š MEMORY DELTA (Canary-Qwen-2.5B):\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  RAM increase:  {ram_delta_gb:+.2f} GB\")\n",
    "    print(f\"  GPU increase:  {gpu_delta_gb:+.2f} GB\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    # Verdict\n",
    "    if ram_delta_gb < 15:\n",
    "        print(f\"âœ… RESULT: Canary-Qwen-2.5B uses ~{ram_delta_gb:.1f} GB RAM (within VM limits)\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  RESULT: Canary-Qwen-2.5B uses ~{ram_delta_gb:.1f} GB RAM (tight on 28GB VM)\")\n",
    "    \n",
    "    if gpu_delta_gb < 12:\n",
    "        print(f\"âœ… RESULT: Canary-Qwen-2.5B uses ~{gpu_delta_gb:.1f} GB GPU (within T4 limits)\")\n",
    "    else:\n",
    "        print(f\"âŒ RESULT: Canary-Qwen-2.5B uses ~{gpu_delta_gb:.1f} GB GPU (exceeds T4 16GB limit)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ERROR loading Canary-Qwen-2.5B:\\n{e}\\n\")\n",
    "    print_resources(\"AFTER ERROR\")\n",
    "    \n",
    "    # Check if it's an OOM error\n",
    "    if \"out of memory\" in str(e).lower() or \"oom\" in str(e).lower():\n",
    "        print(\"\\nðŸ’¡ This appears to be an Out-of-Memory error.\")\n",
    "        print(\"   Canary-Qwen-2.5B requires more memory than available on this VM.\\n\")\n",
    "    \n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Inference with Canary-Qwen-2.5B (If Loaded Successfully)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Testing inference with Canary-Qwen-2.5B...\\n\")\n",
    "\n",
    "# Create dummy audio (same as before)\n",
    "dummy_audio = np.zeros(sample_rate * 10, dtype=np.float32)\n",
    "\n",
    "print(f\"Generated dummy audio: 10s @ 16kHz\\n\")\n",
    "\n",
    "# Memory before inference\n",
    "print_resources(\"BEFORE inference\")\n",
    "\n",
    "mem_before = psutil.virtual_memory()\n",
    "ram_before_gb = mem_before.used / 1024**3\n",
    "gpu_before_gb = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n",
    "\n",
    "try:\n",
    "    print(\"â³ Running transcription...\")\n",
    "    \n",
    "    # Canary models need source_lang and target_lang parameters\n",
    "    with torch.no_grad():\n",
    "        transcription = model_qwen.transcribe(\n",
    "            audio=[dummy_audio],\n",
    "            batch_size=1,\n",
    "            source_lang=\"en\",  # English source\n",
    "            target_lang=\"en\"   # English target (transcription, not translation)\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nâœ“ Transcription completed\\n\")\n",
    "    print(f\"Result: {transcription}\\n\")\n",
    "    \n",
    "    # Memory after inference\n",
    "    print_resources(\"AFTER inference\")\n",
    "    \n",
    "    mem_after = psutil.virtual_memory()\n",
    "    ram_after_gb = mem_after.used / 1024**3\n",
    "    ram_delta_gb = ram_after_gb - ram_before_gb\n",
    "    \n",
    "    gpu_after_gb = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n",
    "    gpu_delta_gb = gpu_after_gb - gpu_before_gb\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“Š INFERENCE MEMORY DELTA:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  RAM increase:  {ram_delta_gb:+.2f} GB\")\n",
    "    print(f\"  GPU increase:  {gpu_delta_gb:+.2f} GB\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ERROR during inference:\\n{e}\\n\")\n",
    "    print_resources(\"AFTER ERROR\")\n",
    "    \n",
    "    # If source_lang error, suggest skipping inference test\n",
    "    if \"source_lang\" in str(e):\n",
    "        print(\"ðŸ’¡ This is a parameter configuration issue, not a memory issue.\")\n",
    "        print(\"   Model loaded successfully - that's what we needed to test!\")\n",
    "        print(\"   You can continue to Step 9 (summary).\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“‹ FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"Please copy the output from Steps 4 and 7 (MEMORY DELTA sections)\")\n",
    "print(\"and paste them into your notes.\\n\")\n",
    "print(\"Key questions to answer:\")\n",
    "print(\"  1. Did Canary-1B load successfully?\")\n",
    "print(\"  2. How much RAM did it use?\")\n",
    "print(\"  3. How much GPU memory did it use?\")\n",
    "print(\"  4. Did Canary-Qwen-2.5B load successfully?\")\n",
    "print(\"  5. How much RAM did it use?\")\n",
    "print(\"  6. How much GPU memory did it use?\")\n",
    "print(\"  7. Did inference work for both models?\\n\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Show final memory state\n",
    "print_resources(\"FINAL STATE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### If you get \"CUDA out of memory\" errors:\n",
    "\n",
    "1. **Check other processes using GPU:**\n",
    "   ```bash\n",
    "   nvidia-smi\n",
    "   ```\n",
    "   Look for other Python processes or Jupyter kernels.\n",
    "\n",
    "2. **Kill other Jupyter kernels:**\n",
    "   In Jupyter: Kernel â†’ Shut Down Other Kernels\n",
    "\n",
    "3. **Restart this kernel:**\n",
    "   Kernel â†’ Restart & Clear Output\n",
    "   Then run from Step 0 again.\n",
    "\n",
    "### If you get \"RuntimeError: Out of memory\" (system RAM):\n",
    "\n",
    "1. **Check other processes:**\n",
    "   ```bash\n",
    "   ps aux | sort -k4 -rn | head -10\n",
    "   ```\n",
    "   This shows top 10 memory-consuming processes.\n",
    "\n",
    "2. **Close unnecessary applications:**\n",
    "   Stop any other Jupyter notebooks, scripts, or applications.\n",
    "\n",
    "3. **Try Canary-1B only:**\n",
    "   Skip Canary-Qwen-2.5B test and just run Canary-1B.\n",
    "\n",
    "### If model download fails:\n",
    "\n",
    "1. **Check internet connection:**\n",
    "   ```bash\n",
    "   ping -c 3 huggingface.co\n",
    "   ```\n",
    "\n",
    "2. **Check disk space:**\n",
    "   ```bash\n",
    "   df -h\n",
    "   ```\n",
    "   Models need ~5-10GB free space.\n",
    "\n",
    "3. **Clear cache and retry:**\n",
    "   ```bash\n",
    "   rm -rf ~/projects/amia2025-stt-benchmarking/models/canary/hub\n",
    "   ```\n",
    "   Then re-run the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
