{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: NVIDIA Canary-1B-v2 Inference Pipeline\n",
    "\n",
    "This notebook demonstrates how to run the NVIDIA Canary-1B-v2 inference pipeline on a single sample file.\n",
    "\n",
    "**Model**: [nvidia/canary-1b-v2](https://huggingface.co/nvidia/canary-1b-v2) (978M params, 25 languages, 8.40% WER on Fleurs-25)\n",
    "\n",
    "**What this notebook does:**\n",
    "- Uses production orchestration (Azure blob storage)\n",
    "- Runs Canary-1B-v2 model (pure ASR, faster than Canary-Qwen)\n",
    "- Processes just 1 sample file for testing\n",
    "- Shows outputs (inference results, hypothesis text)\n",
    "\n",
    "**Requirements:**\n",
    "- GPU: ~5 GB VRAM (less than Canary-Qwen)\n",
    "- RAM: ~3 GB system memory\n",
    "- Azure credentials configured (for blob storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Import and Configure Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../credentials/creds.env')\n",
    "\n",
    "# Disable wandb for notebook runs (tutorials don't need tracking)\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "# Set HuggingFace cache to use local models directory (avoid re-downloading)\n",
    "os.environ['HF_HOME'] = str(Path.cwd().parent / \"models/canary\")\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(Path.cwd().parent / \"models/canary\")\n",
    "\n",
    "# Add scripts directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"scripts\"))\n",
    "\n",
    "from infer_canary_1b import run\n",
    "\n",
    "# System and GPU Memory Check\n",
    "print(\"=\" * 70)\n",
    "print(\"Memory Status Check\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check system RAM\n",
    "try:\n",
    "    import psutil\n",
    "    mem = psutil.virtual_memory()\n",
    "    ram_used = mem.used / 1024**3\n",
    "    ram_total = mem.total / 1024**3\n",
    "    ram_available = mem.available / 1024**3\n",
    "    \n",
    "    print(f\"System RAM: {ram_used:.1f}/{ram_total:.1f} GB used\")\n",
    "    print(f\"Available: {ram_available:.1f} GB\")\n",
    "    \n",
    "    if ram_available < 5:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Only {ram_available:.1f} GB RAM available\")\n",
    "        print(f\"   Canary-1B needs ~3 GB free for model loading\")\n",
    "except ImportError:\n",
    "    print(\"psutil not installed, skipping RAM check\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print()\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        free = total - reserved\n",
    "        \n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Total: {total:.2f} GB\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  Free: {free:.2f} GB\")\n",
    "        \n",
    "        if free < 5.0:\n",
    "            print(f\"  ‚ö†Ô∏è  WARNING: Only {free:.2f} GB GPU free\")\n",
    "            print(f\"     Canary-1B needs ~5 GB GPU VRAM\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No CUDA GPU available - Canary requires GPU!\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüí° Note: Wandb logging is disabled for tutorial runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 GPU Memory Cleanup (Run if needed)\n",
    "\n",
    "If you see \"CUDA out of memory\" errors, run this cell to clear GPU memory from previous runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Force Python garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úì GPU memory cleared\")\n",
    "\n",
    "# Show updated memory status\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        free = total - reserved\n",
    "        print(f\"GPU {i}: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved, {free:.2f} GB free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Inference Parameters\n",
    "\n",
    "Set parameters directly in Python (no yaml file needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build configuration dictionary\n",
    "cfg = {\n",
    "    \"experiment_id\": \"tutorial-canary-1b-v2-sample1\",\n",
    "    \n",
    "    # Model settings\n",
    "    \"model\": {\n",
    "        \"name\": \"canary-1b-v2\",\n",
    "        \"dir\": \"nvidia/canary-1b-v2\",  # v2: 978M params, 25 languages, 8.40% WER\n",
    "        \"device\": \"cuda\",  # Canary requires GPU (use \"cuda\")\n",
    "        \"batch_size\": 1  # Process one file at a time\n",
    "        # Note: No max_new_tokens (Canary-1B is pure ASR, not LLM-based)\n",
    "    },\n",
    "    \n",
    "    # Input settings\n",
    "    \"input\": {\n",
    "        \"source\": \"azure_blob\",\n",
    "        \"parquet_path\": \"../data/raw/loc/veterans_history_project_resources_pre2010.parquet\",  # Relative to notebook dir\n",
    "        \"blob_prefix\": \"loc_vhp\",\n",
    "        \"sample_size\": 1,  # Just 1 file for tutorial\n",
    "        \"duration_sec\": 300,  # First 5 minutes only (faster)\n",
    "        \"sample_rate\": 16000  # Canary expects 16kHz\n",
    "    },\n",
    "    \n",
    "    # Output settings\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/tutorial-canary-1b-v2-sample1\",  # Relative to notebook dir\n",
    "        \"save_per_file\": True  # Save individual hypothesis files\n",
    "    },\n",
    "    \n",
    "    # Evaluation (optional)\n",
    "    \"evaluation\": {\n",
    "        \"use_whisper_normalizer\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {cfg['model']['dir']}\")\n",
    "print(f\"  Version: v2 (978M params, 25 languages, trained on 1.7M hours)\")\n",
    "print(f\"  Memory: ~3 GB RAM, ~5 GB GPU (less than Canary-Qwen)\")\n",
    "print(f\"  Device: {cfg['model']['device']} (GPU required)\")\n",
    "print(f\"  Sample size: {cfg['input']['sample_size']} file(s)\")\n",
    "print(f\"  Duration: {cfg['input']['duration_sec']}s (first 5 minutes)\")\n",
    "print(f\"  Output: {cfg['output']['dir']}\")\n",
    "print(f\"\\n‚ö†Ô∏è  Note: Canary requires a CUDA-enabled GPU\")\n",
    "print(f\"\\nüí° Tip: First run will download model (~1.5GB for v2). Subsequent runs use cached model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Inference\n",
    "\n",
    "This will:\n",
    "1. Load 1 sample from the parquet file\n",
    "2. Download audio from Azure blob storage\n",
    "3. Preprocess audio (trim to 300s, convert to 16kHz mono WAV)\n",
    "4. Load Canary-1B-v2 model (~3GB RAM, ~5GB GPU)\n",
    "5. Run transcription with pure ASR model\n",
    "6. Save results\n",
    "\n",
    "**Note**: First run will download the model (~1.5GB for v2). Cached after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference pipeline\n",
    "run(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View Results\n",
    "\n",
    "### 4.1 Inference Results (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results parquet\n",
    "results_path = Path(cfg['output']['dir']) / \"inference_results.parquet\"\n",
    "df_results = pd.read_parquet(results_path)\n",
    "\n",
    "print(f\"Results shape: {df_results.shape}\")\n",
    "print(f\"\\nColumns: {list(df_results.columns)}\")\n",
    "print(f\"\\nFirst row:\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hypothesis Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the transcription\n",
    "row = df_results.iloc[0]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"File ID: {row['file_id']}\")\n",
    "print(f\"Collection: {row['collection_number']}\")\n",
    "print(f\"Duration: {row['duration_sec']:.1f}s\")\n",
    "print(f\"Processing time: {row['processing_time_sec']:.1f}s\")\n",
    "print(f\"Status: {row['status']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTranscription (hypothesis):\\n\")\n",
    "print(row['hypothesis'][:500] + \"...\" if len(row['hypothesis']) > 500 else row['hypothesis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Individual Hypothesis File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check individual hypothesis file\n",
    "hyp_file = Path(cfg['output']['dir']) / f\"hyp_{row['file_id']}.txt\"\n",
    "\n",
    "if hyp_file.exists():\n",
    "    print(f\"Hypothesis file: {hyp_file}\")\n",
    "    print(f\"\\nContent:\\n\")\n",
    "    print(hyp_file.read_text()[:500] + \"...\" if len(hyp_file.read_text()) > 500 else hyp_file.read_text())\n",
    "else:\n",
    "    print(f\"Hypothesis file not found: {hyp_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Output\n",
    "\n",
    "**Key output files:**\n",
    "- `inference_results.parquet`: All results in structured format (hypothesis, duration, status, etc.)\n",
    "- `hyp_{file_id}.txt`: Individual hypothesis text files (one per audio file)\n",
    "- `hyp_canary1b.txt`: Combined hypothesis text (all transcriptions concatenated)\n",
    "- `canary1b_log_*.txt`: Detailed inference log\n",
    "\n",
    "**Key result columns:**\n",
    "- `file_id`: Unique identifier\n",
    "- `collection_number`: VHP collection number\n",
    "- `hypothesis`: Model transcription output\n",
    "- `ground_truth`: Reference transcript (for evaluation)\n",
    "- `duration_sec`: Audio duration processed\n",
    "- `processing_time_sec`: Time taken for inference\n",
    "- `status`: success/error\n",
    "- `model_name`: Model identifier\n",
    "\n",
    "**Model used:**\n",
    "- **[nvidia/canary-1b-v2](https://huggingface.co/nvidia/canary-1b-v2)**: 978M params, 25 languages, 8.40% WER on Fleurs-25\n",
    "- Pure ASR model (not LLM-based), faster than Canary-Qwen\n",
    "- Trained on 1.7M hours of multilingual audio (Granary dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Canary-1B-v2\n",
    "\n",
    "**Model**: [nvidia/canary-1b-v2](https://huggingface.co/nvidia/canary-1b-v2)\n",
    "\n",
    "Canary-1B-v2 is a **pure ASR (Automatic Speech Recognition)** model, unlike Canary-Qwen which is a SALM (Speech Audio Language Model).\n",
    "\n",
    "**Key features:**\n",
    "- 978M parameters (32 encoder + 8 decoder layers)\n",
    "- Fast-Conformer architecture (pure ASR, not LLM-based)\n",
    "- 25 languages supported (vs 4 in v1)\n",
    "- 8.40% WER on Fleurs-25 benchmark\n",
    "- Trained on 1.7M hours of multilingual audio (Granary dataset)\n",
    "- English speech-to-text with basic punctuation\n",
    "- Can generate timestamps (unlike Canary-Qwen)\n",
    "- Uses `transcribe()` method instead of LLM `generate()`\n",
    "\n",
    "**Memory requirements (tested on Tesla T4):**\n",
    "- GPU: ~6 GB VRAM (half of Canary-Qwen)\n",
    "- RAM: ~3 GB system memory (less than Canary-Qwen)\n",
    "- Fits comfortably on T4 16GB GPU\n",
    "\n",
    "**Performance:**\n",
    "- Faster than Canary-Qwen (no LLM decoding overhead)\n",
    "- Lower memory footprint\n",
    "- Good accuracy for clear audio\n",
    "- May be less robust than Canary-Qwen on degraded/archival audio\n",
    "\n",
    "**Differences from Canary-Qwen:**\n",
    "\n",
    "| Aspect | Canary-1B-v2 | Canary-Qwen-2.5B |\n",
    "|--------|--------------|------------------|\n",
    "| **HuggingFace** | [nvidia/canary-1b-v2](https://huggingface.co/nvidia/canary-1b-v2) | [nvidia/canary-qwen-2.5b](https://huggingface.co/nvidia/canary-qwen-2.5b) |\n",
    "| **Architecture** | Pure ASR (Fast-Conformer) | SALM (ASR + LLM) |\n",
    "| **Parameters** | 978M | 2.5B |\n",
    "| **Languages** | 25 | Multilingual |\n",
    "| **WER (Fleurs-25)** | 8.40% | 5.63% (better) |\n",
    "| **GPU Memory** | ~6 GB | ~10 GB |\n",
    "| **Timestamps** | ‚úÖ Yes | ‚ùå No |\n",
    "| **Punctuation** | Basic | ‚úÖ Full (LLM-based) |\n",
    "| **Speed** | Faster (~23x realtime on T4) | Slower |\n",
    "| **Method** | `transcribe()` | `generate()` |\n",
    "| **max_new_tokens** | N/A (not LLM) | Required |\n",
    "| **Prompt** | N/A | Required |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "To run on more files:\n",
    "1. Increase `sample_size` (e.g., 10, 50, 500)\n",
    "2. Set `duration_sec: None` for full audio\n",
    "3. **No prompt needed** (Canary-1B-v2 is pure ASR, not LLM-based)\n",
    "\n",
    "To evaluate results:\n",
    "```python\n",
    "# This will be available after implementing evaluation\n",
    "from scripts.eval.evaluate import evaluate_results\n",
    "metrics = evaluate_results(df_results, use_whisper_normalizer=True)\n",
    "print(f\"WER: {metrics['wer']:.2%}\")\n",
    "```\n",
    "\n",
    "**Recommended config for 10-sample test:**\n",
    "```python\n",
    "cfg = {\n",
    "    \"experiment_id\": \"vhp-canary-1b-v2-sample10\",\n",
    "    \"model\": {\n",
    "        \"name\": \"canary-1b-v2\",\n",
    "        \"dir\": \"nvidia/canary-1b-v2\",  # v2: 978M params, 25 languages\n",
    "        \"device\": \"cuda\",\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"sample_size\": 10,  # Test with 10 files\n",
    "        \"duration_sec\": None  # Full audio\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/vhp-canary-1b-v2-sample10\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Recommended config for 500-sample benchmark:**\n",
    "```python\n",
    "cfg = {\n",
    "    \"experiment_id\": \"vhp-canary-1b-v2-500\",\n",
    "    \"model\": {\n",
    "        \"name\": \"canary-1b-v2\",\n",
    "        \"dir\": \"nvidia/canary-1b-v2\",  # v2: 978M params, 25 languages\n",
    "        \"device\": \"cuda\",\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"sample_size\": 500,  # Full benchmark\n",
    "        \"duration_sec\": None  # Full audio\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"dir\": \"../outputs/vhp-canary-1b-v2-500\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
