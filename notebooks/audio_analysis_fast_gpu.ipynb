{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Quality Analysis - GPU-Accelerated Batch Processing\n",
    "\n",
    "**Optimizations:**\n",
    "- Parallel Azure blob downloads (50-100 files at once)\n",
    "- GPU-accelerated audio processing with torchaudio\n",
    "- Batch spectrogram computation on GPU\n",
    "- Automatic cleanup of downloaded files\n",
    "- Multi-threading for download + GPU processing pipeline\n",
    "\n",
    "**Target:** Process 5000 files in ~1-3 hours (vs 25-30 hours)\n",
    "\n",
    "**Output:** Same as original - `audio_quality_analysis.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyloudnorm as pyln\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "import tempfile\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Azure blob utilities\n",
    "from scripts.cloud.azure_utils import list_blobs, download_blob_to_memory\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../credentials/creds.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing configuration\n",
    "BATCH_SIZE = 64  # Number of files to process on GPU simultaneously\n",
    "DOWNLOAD_WORKERS = 100  # Parallel downloads\n",
    "DOWNLOAD_BATCH_SIZE = 200  # Files to download before processing\n",
    "TARGET_SR = 16000  # Target sample rate\n",
    "\n",
    "# Temp directory for downloaded files (will be cleaned up)\n",
    "TEMP_DIR = Path(tempfile.mkdtemp())\n",
    "print(f\"Temp directory: {TEMP_DIR}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} files\")\n",
    "print(f\"Download workers: {DOWNLOAD_WORKERS}\")\n",
    "print(f\"Download batch size: {DOWNLOAD_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU-Accelerated Audio Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_to_tensor(audio_bytes, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Load audio bytes to torch tensor and resample.\n",
    "    Handles MP3, MP4, WAV, etc. using pydub (more robust).\n",
    "    \"\"\"\n",
    "    from pydub import AudioSegment\n",
    "    \n",
    "    # Use pydub to load audio (handles MP3, MP4, M4A, WAV, etc.)\n",
    "    audio_segment = AudioSegment.from_file(io.BytesIO(audio_bytes))\n",
    "    \n",
    "    # Convert to mono\n",
    "    audio_segment = audio_segment.set_channels(1)\n",
    "    \n",
    "    # Convert to target sample rate\n",
    "    audio_segment = audio_segment.set_frame_rate(target_sr)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    samples = np.array(audio_segment.get_array_of_samples())\n",
    "    \n",
    "    # Normalize to float32 [-1, 1]\n",
    "    if audio_segment.sample_width == 2:  # 16-bit\n",
    "        waveform = samples.astype(np.float32) / 32768.0\n",
    "    elif audio_segment.sample_width == 4:  # 32-bit\n",
    "        waveform = samples.astype(np.float32) / 2147483648.0\n",
    "    else:\n",
    "        waveform = samples.astype(np.float32)\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    waveform_tensor = torch.from_numpy(waveform).float()\n",
    "    \n",
    "    return waveform_tensor, target_sr\n",
    "\n",
    "\n",
    "def batch_compute_spectrogram(waveforms, sr, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Compute spectrograms for batch of waveforms on GPU.\n",
    "    Matches librosa.stft() parameters exactly.\n",
    "    \n",
    "    Args:\n",
    "        waveforms: List of 1D tensors (different lengths OK)\n",
    "        sr: Sample rate\n",
    "    \n",
    "    Returns:\n",
    "        List of (magnitude_spectrogram, frequencies) tuples on GPU\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Frequency bins (matches librosa.fft_frequencies)\n",
    "    freqs = torch.linspace(0, sr/2, n_fft//2 + 1, device=device)\n",
    "    \n",
    "    # Create the spectrogram transform once\n",
    "    spec_transform = torchaudio.transforms.Spectrogram(\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=n_fft,  # librosa default\n",
    "        window_fn=torch.hann_window,\n",
    "        power=None,  # Complex output\n",
    "        center=True,  # librosa default\n",
    "        pad_mode='reflect',  # librosa default\n",
    "        normalized=False\n",
    "    ).to(device)\n",
    "    \n",
    "    for wv in waveforms:\n",
    "        # Move to GPU\n",
    "        wv_gpu = wv.to(device)\n",
    "        \n",
    "        # Check if waveform is long enough for n_fft\n",
    "        # With center=True, we need at least (n_fft // 2 + 1) samples to avoid padding issues\n",
    "        min_length = n_fft\n",
    "        if len(wv_gpu) < min_length:\n",
    "            # Pad short audio to minimum length\n",
    "            pad_needed = min_length - len(wv_gpu)\n",
    "            wv_gpu = torch.nn.functional.pad(wv_gpu, (0, pad_needed), mode='constant', value=0)\n",
    "        \n",
    "        spec = spec_transform(wv_gpu)\n",
    "        mag = torch.abs(spec)  # Magnitude\n",
    "        results.append((mag, freqs))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def snr_cal_batch(waveform, sr):\n",
    "    \"\"\"\n",
    "    Calculate SNR matching librosa.feature.rms() exactly.\n",
    "    \n",
    "    librosa.feature.rms defaults:\n",
    "    - frame_length=2048\n",
    "    - hop_length=512  \n",
    "    - center=True\n",
    "    \"\"\"\n",
    "    frame_length = 2048\n",
    "    hop_length = 512\n",
    "    \n",
    "    # Handle short waveforms\n",
    "    if len(waveform) < frame_length:\n",
    "        # Pad to minimum length\n",
    "        pad_needed = frame_length - len(waveform)\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, pad_needed), mode='constant', value=0)\n",
    "    \n",
    "    # Pad for centering (matches librosa center=True)\n",
    "    pad_length = frame_length // 2\n",
    "    waveform_padded = torch.nn.functional.pad(waveform, (pad_length, pad_length), mode='reflect')\n",
    "    \n",
    "    # Calculate number of frames\n",
    "    num_frames = 1 + (len(waveform_padded) - frame_length) // hop_length\n",
    "    \n",
    "    if num_frames <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute RMS per frame (matches librosa exactly)\n",
    "    rms_values = []\n",
    "    for i in range(num_frames):\n",
    "        start = i * hop_length\n",
    "        end = start + frame_length\n",
    "        if end > len(waveform_padded):\n",
    "            break\n",
    "        frame = waveform_padded[start:end]\n",
    "        rms = torch.sqrt(torch.mean(frame ** 2))\n",
    "        rms_values.append(rms)\n",
    "    \n",
    "    if len(rms_values) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    rms_tensor = torch.stack(rms_values)\n",
    "    \n",
    "    # SNR calculation: assume first 0.5 seconds is noise\n",
    "    noise_frames = int(0.5 * sr / hop_length)\n",
    "    \n",
    "    if noise_frames > 0 and noise_frames < len(rms_tensor):\n",
    "        noise_rms = torch.mean(rms_tensor[:noise_frames])\n",
    "        signal_rms = torch.mean(rms_tensor)\n",
    "        \n",
    "        if noise_rms > 0:\n",
    "            snr_db = 20 * torch.log10(signal_rms / noise_rms)\n",
    "            return snr_db.item()\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def spectral_rolloff_batch(spec_mag, freqs, sr, roll_percent=0.85):\n",
    "    \"\"\"\n",
    "    Calculate spectral rolloff matching librosa.feature.spectral_rolloff() exactly.\n",
    "    \n",
    "    Args:\n",
    "        spec_mag: Magnitude spectrogram (freq_bins x time_frames) on GPU\n",
    "        freqs: Frequency values for each bin\n",
    "        sr: Sample rate\n",
    "        roll_percent: Rolloff percentage (default 0.85)\n",
    "    \"\"\"\n",
    "    # Cumulative sum along frequency axis\n",
    "    cumsum = torch.cumsum(spec_mag, dim=0)\n",
    "    total_energy = cumsum[-1, :]\n",
    "    \n",
    "    # Find frequency where cumsum reaches roll_percent of total\n",
    "    threshold = roll_percent * total_energy\n",
    "    \n",
    "    # For each frame, find the bin where threshold is crossed\n",
    "    rolloff_bins = torch.argmax((cumsum >= threshold.unsqueeze(0)).float(), dim=0)\n",
    "    \n",
    "    # Convert bins to Hz\n",
    "    rolloff_hz = freqs[rolloff_bins]\n",
    "    \n",
    "    # Return median (matches librosa convention)\n",
    "    return torch.median(rolloff_hz).item()\n",
    "\n",
    "\n",
    "def spectral_centroid_batch(spec_mag, freqs, sr):\n",
    "    \"\"\"\n",
    "    Calculate spectral centroid matching librosa.feature.spectral_centroid() exactly.\n",
    "    \n",
    "    Args:\n",
    "        spec_mag: Magnitude spectrogram (freq_bins x time_frames) on GPU\n",
    "        freqs: Frequency values for each bin\n",
    "        sr: Sample rate\n",
    "    \"\"\"\n",
    "    # Weighted average frequency per frame\n",
    "    freqs_2d = freqs.unsqueeze(1)  # (freq_bins, 1)\n",
    "    \n",
    "    # Weighted sum\n",
    "    centroid = torch.sum(freqs_2d * spec_mag, dim=0) / (torch.sum(spec_mag, dim=0) + 1e-8)\n",
    "    \n",
    "    # Return median (matches librosa convention)\n",
    "    return torch.median(centroid).item()\n",
    "\n",
    "\n",
    "def spectral_flatness_batch(spec_mag):\n",
    "    \"\"\"\n",
    "    Calculate spectral flatness matching librosa.feature.spectral_flatness() exactly.\n",
    "    \n",
    "    Spectral flatness = geometric_mean / arithmetic_mean per frame\n",
    "    \n",
    "    Args:\n",
    "        spec_mag: Magnitude spectrogram (freq_bins x time_frames) on GPU\n",
    "    \"\"\"\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    spec_safe = spec_mag + 1e-10\n",
    "    \n",
    "    # Geometric mean per frame: exp(mean(log(x)))\n",
    "    log_spec = torch.log(spec_safe)\n",
    "    geometric_mean = torch.exp(torch.mean(log_spec, dim=0))\n",
    "    \n",
    "    # Arithmetic mean per frame\n",
    "    arithmetic_mean = torch.mean(spec_mag, dim=0)\n",
    "    \n",
    "    # Flatness per frame\n",
    "    flatness = geometric_mean / (arithmetic_mean + 1e-10)\n",
    "    \n",
    "    # Return mean across frames (matches librosa)\n",
    "    return torch.mean(flatness).item()\n",
    "\n",
    "\n",
    "def zcr_batch(waveform):\n",
    "    \"\"\"\n",
    "    Calculate zero crossing rate matching librosa.feature.zero_crossing_rate() exactly.\n",
    "    \n",
    "    librosa defaults:\n",
    "    - frame_length=2048\n",
    "    - hop_length=512\n",
    "    - center=True\n",
    "    \"\"\"\n",
    "    frame_length = 2048\n",
    "    hop_length = 512\n",
    "    \n",
    "    # Handle short waveforms\n",
    "    if len(waveform) < frame_length:\n",
    "        # Pad to minimum length\n",
    "        pad_needed = frame_length - len(waveform)\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, pad_needed), mode='constant', value=0)\n",
    "    \n",
    "    # Pad for centering\n",
    "    pad_length = frame_length // 2\n",
    "    waveform_padded = torch.nn.functional.pad(waveform, (pad_length, pad_length), mode='constant', value=0)\n",
    "    \n",
    "    # Zero crossings: sign changes\n",
    "    signs = torch.sign(waveform_padded)\n",
    "    signs[signs == 0] = 0  # Treat zeros as positive (librosa convention)\n",
    "    \n",
    "    # Indicator where sign changes\n",
    "    sign_changes = torch.abs(torch.diff(signs)) > 0\n",
    "    \n",
    "    # Calculate number of frames\n",
    "    num_frames = 1 + (len(waveform_padded) - frame_length) // hop_length\n",
    "    \n",
    "    if num_frames <= 0:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    # ZCR per frame\n",
    "    zcr_values = []\n",
    "    for i in range(num_frames):\n",
    "        start = i * hop_length\n",
    "        end = start + frame_length - 1  # diff reduces length by 1\n",
    "        if end > len(sign_changes):\n",
    "            break\n",
    "        frame = sign_changes[start:end]\n",
    "        zcr = torch.sum(frame.float()) / (frame_length - 1)\n",
    "        zcr_values.append(zcr)\n",
    "    \n",
    "    if len(zcr_values) == 0:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    zcr_tensor = torch.stack(zcr_values)\n",
    "    \n",
    "    # Return mean and variance\n",
    "    return torch.mean(zcr_tensor).item(), torch.var(zcr_tensor).item()\n",
    "\n",
    "\n",
    "def loudness_cal(waveform, sr):\n",
    "    \"\"\"\n",
    "    Calculate loudness (LUFS) - requires CPU.\n",
    "    Uses pyloudnorm (matches original).\n",
    "    \"\"\"\n",
    "    wv_cpu = waveform.cpu().numpy()\n",
    "    \n",
    "    # Handle very short audio (pyloudnorm needs at least 0.4s)\n",
    "    min_length = int(0.4 * sr)\n",
    "    if len(wv_cpu) < min_length:\n",
    "        wv_cpu = np.pad(wv_cpu, (0, min_length - len(wv_cpu)), mode='constant', constant_values=0)\n",
    "    \n",
    "    meter = pyln.Meter(sr)\n",
    "    loudness = meter.integrated_loudness(wv_cpu)\n",
    "    return float(loudness)\n",
    "\n",
    "\n",
    "def low_frequency_energy_batch(spec_mag, freqs, sr, cutoff_hz=80):\n",
    "    \"\"\"\n",
    "    Calculate low frequency energy ratio matching original librosa implementation exactly.\n",
    "    \n",
    "    Args:\n",
    "        spec_mag: Magnitude spectrogram from librosa.stft() (freq_bins x time_frames) on GPU\n",
    "        freqs: Frequency values for each bin (matches librosa.fft_frequencies)\n",
    "        sr: Sample rate\n",
    "        cutoff_hz: Cutoff frequency for low frequency energy\n",
    "    \"\"\"\n",
    "    # Energy below cutoff\n",
    "    low_freq_mask = freqs < cutoff_hz\n",
    "    low_energy = torch.sum(spec_mag[low_freq_mask, :])\n",
    "    total_energy = torch.sum(spec_mag)\n",
    "    \n",
    "    # Return ratio\n",
    "    if total_energy > 0:\n",
    "        ratio = low_energy / total_energy\n",
    "        return ratio.item()\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_audio_batch_gpu(audio_data_list):\n",
    "    \"\"\"\n",
    "    Analyze a batch of audio files on GPU.\n",
    "    \n",
    "    Args:\n",
    "        audio_data_list: List of (audio_id, audio_bytes) tuples\n",
    "    \n",
    "    Returns:\n",
    "        List of result dictionaries\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    waveforms = []\n",
    "    audio_ids = []\n",
    "    \n",
    "    # Load all audio files\n",
    "    for audio_id, audio_bytes in audio_data_list:\n",
    "        try:\n",
    "            wv, sr = load_audio_to_tensor(audio_bytes, target_sr=TARGET_SR)\n",
    "            waveforms.append(wv)\n",
    "            audio_ids.append(audio_id)\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'audio_id': audio_id,\n",
    "                'status': 'error',\n",
    "                'error_message': f\"Load error: {str(e)}\"\n",
    "            })\n",
    "    \n",
    "    if not waveforms:\n",
    "        return results\n",
    "    \n",
    "    # Compute spectrograms on GPU (batch) - now returns (mag, freqs) tuples\n",
    "    spec_results = batch_compute_spectrogram(waveforms, TARGET_SR)\n",
    "    \n",
    "    # Process each file\n",
    "    for i, (audio_id, wv) in enumerate(zip(audio_ids, waveforms)):\n",
    "        try:\n",
    "            spec_mag, freqs = spec_results[i]\n",
    "            \n",
    "            # Move waveform to GPU for processing\n",
    "            wv_gpu = wv.to(device)\n",
    "            \n",
    "            # Calculate metrics (matching librosa implementations exactly)\n",
    "            duration_sec = len(wv) / TARGET_SR\n",
    "            \n",
    "            # GPU-based metrics (now matching librosa exactly)\n",
    "            snr = snr_cal_batch(wv_gpu, TARGET_SR)\n",
    "            rolloff = spectral_rolloff_batch(spec_mag, freqs, TARGET_SR)\n",
    "            centroid = spectral_centroid_batch(spec_mag, freqs, TARGET_SR)\n",
    "            flatness = spectral_flatness_batch(spec_mag)\n",
    "            zcr_mean, zcr_var = zcr_batch(wv_gpu)\n",
    "            low_freq_ratio = low_frequency_energy_batch(spec_mag, freqs, TARGET_SR)\n",
    "            \n",
    "            # CPU-based metric (loudness)\n",
    "            loudness = loudness_cal(wv, TARGET_SR)\n",
    "            \n",
    "            results.append({\n",
    "                'audio_id': audio_id,\n",
    "                'sample_rate': TARGET_SR,\n",
    "                'duration_sec': float(duration_sec),\n",
    "                'snr_db': float(snr),\n",
    "                'spectral_rolloff_hz': float(rolloff),\n",
    "                'spectral_flatness': float(flatness),\n",
    "                'spectral_centroid_hz': float(centroid),\n",
    "                'zcr_mean': float(zcr_mean),\n",
    "                'zcr_var': float(zcr_var),\n",
    "                'loudness_lufs': float(loudness),\n",
    "                'low_freq_energy_ratio': float(low_freq_ratio),\n",
    "                'status': 'success'\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'audio_id': audio_id,\n",
    "                'status': 'error',\n",
    "                'error_message': f\"Processing error: {str(e)}\"\n",
    "            })\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del waveforms, spec_results\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Download Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_single_blob(blob_path):\n",
    "    \"\"\"\n",
    "    Download a single blob and return (audio_id, audio_bytes).\n",
    "    \"\"\"\n",
    "    audio_id = Path(blob_path).parent.name  # Extract ID from path\n",
    "    try:\n",
    "        audio_bytes = download_blob_to_memory(blob_path)\n",
    "        return (audio_id, audio_bytes, None)\n",
    "    except Exception as e:\n",
    "        return (audio_id, None, str(e))\n",
    "\n",
    "\n",
    "def download_batch_parallel(blob_paths, max_workers=100):\n",
    "    \"\"\"\n",
    "    Download multiple blobs in parallel.\n",
    "    \n",
    "    Args:\n",
    "        blob_paths: List of blob paths to download\n",
    "        max_workers: Number of parallel downloads\n",
    "    \n",
    "    Returns:\n",
    "        List of (audio_id, audio_bytes) tuples (only successful downloads)\n",
    "        List of error dictionaries\n",
    "    \"\"\"\n",
    "    audio_data = []\n",
    "    errors = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(download_single_blob, path): path for path in blob_paths}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading\"):\n",
    "            audio_id, audio_bytes, error = future.result()\n",
    "            \n",
    "            if error:\n",
    "                errors.append({\n",
    "                    'audio_id': audio_id,\n",
    "                    'status': 'error',\n",
    "                    'error_message': f\"Download error: {error}\"\n",
    "                })\n",
    "            else:\n",
    "                audio_data.append((audio_id, audio_bytes))\n",
    "    \n",
    "    return audio_data, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List All Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all audio/video blobs from Azure\n",
    "blob_prefix = \"loc_vhp/\"\n",
    "\n",
    "print(f\"Listing blobs with prefix: {blob_prefix}\")\n",
    "audio_blobs = list_blobs(blob_prefix)\n",
    "\n",
    "# Filter for ORIGINAL VHP source files only (not NFA-segmented WAV files)\n",
    "# VHP project naming convention: audio.mp3 or video.mp4\n",
    "audio_blobs = [b for b in audio_blobs if b.endswith('/audio.mp3') or b.endswith('/video.mp4')]\n",
    "\n",
    "print(f\"Found {len(audio_blobs)} original VHP media files\")\n",
    "print(f\"\\nFirst 5 files:\")\n",
    "for blob in audio_blobs[:5]:\n",
    "    print(f\"  - {blob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING: Uncomment to limit processing for testing\n",
    "# SAMPLE_SIZE = 100\n",
    "# audio_blobs = audio_blobs[:SAMPLE_SIZE]\n",
    "# print(f\"\\nðŸ§ª TEST MODE: Processing only first {SAMPLE_SIZE} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Files (Download + GPU Processing Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "all_results = []\n",
    "total_files = len(audio_blobs)\n",
    "\n",
    "print(f\"\\nProcessing {total_files} files...\")\n",
    "print(f\"Download batch size: {DOWNLOAD_BATCH_SIZE}\")\n",
    "print(f\"GPU batch size: {BATCH_SIZE}\")\n",
    "print(f\"Download workers: {DOWNLOAD_WORKERS}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process in large download batches\n",
    "for i in range(0, total_files, DOWNLOAD_BATCH_SIZE):\n",
    "    batch_blobs = audio_blobs[i:i + DOWNLOAD_BATCH_SIZE]\n",
    "    \n",
    "    print(f\"\\n[Batch {i//DOWNLOAD_BATCH_SIZE + 1}] Downloading {len(batch_blobs)} files...\")\n",
    "    \n",
    "    # Download batch in parallel\n",
    "    audio_data, download_errors = download_batch_parallel(batch_blobs, max_workers=DOWNLOAD_WORKERS)\n",
    "    all_results.extend(download_errors)\n",
    "    \n",
    "    print(f\"Downloaded: {len(audio_data)} files, Errors: {len(download_errors)}\")\n",
    "    \n",
    "    # Process downloaded files in GPU batches\n",
    "    print(f\"Processing on GPU...\")\n",
    "    for j in range(0, len(audio_data), BATCH_SIZE):\n",
    "        gpu_batch = audio_data[j:j + BATCH_SIZE]\n",
    "        batch_results = analyze_audio_batch_gpu(gpu_batch)\n",
    "        all_results.extend(batch_results)\n",
    "    \n",
    "    # Clear memory after batch\n",
    "    del audio_data\n",
    "    gc.collect()\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Progress update\n",
    "    elapsed = time.time() - start_time\n",
    "    processed = min(i + DOWNLOAD_BATCH_SIZE, total_files)\n",
    "    rate = processed / elapsed\n",
    "    remaining = (total_files - processed) / rate if rate > 0 else 0\n",
    "    \n",
    "    print(f\"Progress: {processed}/{total_files} ({processed/total_files*100:.1f}%)\")\n",
    "    print(f\"Elapsed: {elapsed/60:.1f} min, Rate: {rate:.1f} files/sec, ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"âœ“ COMPLETED\")\n",
    "print(f\"Total files: {len(all_results)}\")\n",
    "print(f\"Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
    "print(f\"Average rate: {len(all_results)/total_time:.2f} files/second\")\n",
    "print(f\"Success: {sum(1 for r in all_results if r.get('status') == 'success')}\")\n",
    "print(f\"Errors: {sum(1 for r in all_results if r.get('status') == 'error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue Detection and Preprocessing Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_audio_issues(row):\n",
    "    \"\"\"Detect audio issues based on quality metrics.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    if row['status'] != 'success':\n",
    "        return issues\n",
    "    \n",
    "    # 1. Bandwidth-limited\n",
    "    if row['spectral_rolloff_hz'] < 1000:\n",
    "        issues.append('bandwidth_limited_severe')\n",
    "    elif row['spectral_rolloff_hz'] < 4000:\n",
    "        issues.append('bandwidth_limited_moderate')\n",
    "    \n",
    "    # 2. High noise\n",
    "    if row['zcr_mean'] > 0.05:\n",
    "        issues.append('high_noise_zcr')\n",
    "    if row['snr_db'] < 15:\n",
    "        issues.append('high_noise_snr')\n",
    "    \n",
    "    # 3. Low-frequency rumble\n",
    "    if row['low_freq_energy_ratio'] > 0.15:\n",
    "        issues.append('low_frequency_rumble')\n",
    "    \n",
    "    # 4. Low loudness\n",
    "    if row['loudness_lufs'] < -30:\n",
    "        issues.append('low_loudness')\n",
    "    \n",
    "    # 5. Very flat spectrum\n",
    "    if row['spectral_flatness'] > 0.8:\n",
    "        issues.append('very_flat_spectrum')\n",
    "    \n",
    "    return issues\n",
    "\n",
    "\n",
    "def recommend_preprocessing(issues):\n",
    "    \"\"\"Recommend preprocessing methods based on detected issues.\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Always normalize loudness\n",
    "    recommendations.append('loudness_normalization')\n",
    "    \n",
    "    # Bandwidth-limited â†’ EQ boost\n",
    "    if 'bandwidth_limited_severe' in issues or 'bandwidth_limited_moderate' in issues:\n",
    "        recommendations.append('eq_high_freq_boost')\n",
    "    \n",
    "    # High noise â†’ Noise reduction\n",
    "    if 'high_noise_zcr' in issues or 'high_noise_snr' in issues:\n",
    "        recommendations.append('noise_reduction')\n",
    "    \n",
    "    # Low-frequency rumble â†’ High-pass filter\n",
    "    if 'low_frequency_rumble' in issues:\n",
    "        recommendations.append('highpass_filter')\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Detect issues\n",
    "df['issues'] = df.apply(detect_audio_issues, axis=1)\n",
    "df['recommended_preprocessing'] = df['issues'].apply(recommend_preprocessing)\n",
    "\n",
    "print(f\"\\nFiles with issues: {(df['issues'].str.len() > 0).sum()} / {len(df)}\")\n",
    "\n",
    "from collections import Counter\n",
    "all_issues = [issue for issues in df['issues'] for issue in issues]\n",
    "issue_counts = Counter(all_issues)\n",
    "\n",
    "print(\"\\nIssue breakdown:\")\n",
    "for issue, count in issue_counts.most_common():\n",
    "    print(f\"  {issue}: {count} files ({count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_success = df[df['status'] == 'success']\n",
    "\n",
    "print(\"Overall Statistics:\")\n",
    "print(df_success[['snr_db', 'spectral_rolloff_hz', 'spectral_flatness', \n",
    "                   'zcr_mean', 'loudness_lufs']].describe())\n",
    "\n",
    "print(\"\\nPreprocessing Recommendations:\")\n",
    "all_recs = [rec for recs in df['recommended_preprocessing'] for rec in recs]\n",
    "rec_counts = Counter(all_recs)\n",
    "\n",
    "for rec, count in rec_counts.most_common():\n",
    "    print(f\"{rec:30s}: {count:4d} files ({count/len(df)*100:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"../data/audio_quality_analysis.parquet\")\n",
    "df.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"âœ“ Saved: {output_path}\")\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFile size: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temp directory\n",
    "import shutil\n",
    "\n",
    "if TEMP_DIR.exists():\n",
    "    shutil.rmtree(TEMP_DIR)\n",
    "    print(f\"âœ“ Cleaned up temp directory: {TEMP_DIR}\")\n",
    "\n",
    "# Clear GPU memory\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"âœ“ Cleared GPU memory\")\n",
    "\n",
    "print(\"\\nâœ“ All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amia2025-stt-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
