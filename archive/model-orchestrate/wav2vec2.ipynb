{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MEDIA_DIR = \"/Volumes/KINGSTON/veteran_interviews\"\n",
    "# index = 0\n",
    "# audio_path = f\"{MEDIA_DIR}/{index}/audio.mp3\"\n",
    "# video_path = f\"{MEDIA_DIR}/{index}/video.mp4\"\n",
    "# # check if the audio file exists\n",
    "# if not os.path.exists(audio_path):\n",
    "#     raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "# # check if the video file exists\n",
    "# if not os.path.exists(video_path):\n",
    "#     raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "# develop a function to return media type, and audio/ video path based on index\n",
    "def get_media_type_and_path(index):\n",
    "    audio_path = f\"{MEDIA_DIR}/{index}/audio.mp3\"\n",
    "    video_path = f\"{MEDIA_DIR}/{index}/video.mp4\"\n",
    "    if os.path.exists(video_path):\n",
    "        return \"video\", video_path\n",
    "    elif os.path.exists(audio_path):\n",
    "        return \"audio\", audio_path\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "get_media_type_and_path(0)  # Test the function with index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parquet file\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(\"../datasets/veterans_history_project_resources.parquet\")\n",
    "df['media_type'] = df.index.to_series().apply(lambda x: get_media_type_and_path(x)[0] if get_media_type_and_path(x) else None)\n",
    "df['media_filepath'] = df.index.to_series().apply(lambda x: get_media_type_and_path(x)[1] if get_media_type_and_path(x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample dataset of 100 rows where returned media type is audio\n",
    "df_sample = df[df['media_type'] == 'audio'].sample(n=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "# testing_raw_transcript = df_sample['fulltext_file_str'][730]\n",
    "# strip content from the xml (only get the text between <p> tags)\n",
    "import re\n",
    "def strip_xml_tags(text):\n",
    "    # make sure the input is a string\n",
    "    if not isinstance(text, str):\n",
    "        # force it to be a string\n",
    "        text = str(text)\n",
    "    # Remove all XML tags except <p> and <speaker>\n",
    "    # Find the position of the first <speaker> tag\n",
    "    speaker_match = re.search(r'<speaker>.*?</speaker>', text, re.DOTALL)\n",
    "    if not speaker_match:\n",
    "        return \"\"\n",
    "    start_pos = speaker_match.end()\n",
    "    # Only search for <p>...</p> after the first <speaker>\n",
    "    paragraphs = re.findall(r'<p>(.*?)</p>', text[start_pos:], re.DOTALL)\n",
    "    # Remove everything after \"[Conclusion of Interview]\"\n",
    "    result = []\n",
    "    for para in paragraphs:\n",
    "        if \"[Conclusion of Interview]\" in para:\n",
    "            break\n",
    "        result.append(para)\n",
    "    return '\\n'.join(result)\n",
    "# testing_raw_transcript_stripped = strip_xml_tags(testing_raw_transcript)\n",
    "# apply this function to the fulltext_file_str column\n",
    "df_sample['raw_transcript_stripped'] = df_sample['fulltext_file_str'].apply(strip_xml_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['raw_transcript_stripped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['media_filepath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sample dataset to a new parquet file\n",
    "df_sample.to_parquet(\"../datasets/veterans_history_project_sample.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# load parquet file as a Hugging Face dataset\n",
    "hf_dataset = load_dataset('parquet', data_files=\"../datasets/veterans_history_project_sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "def preprocess_audio(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    return waveform\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = preprocess_audio(batch[\"media_filepath\"])\n",
    "    batch[\"input_values\"] = processor(audio, sampling_rate=16000).input_values[0]\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"raw_transcript_stripped\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = hf_dataset.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset to disk\n",
    "dataset.save_to_disk(\"../datasets/veterans_history_project_sample_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the dataset from disk\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"../datasets/veterans_history_project_sample_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split 90% train, 10% validation\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# Rename 'test' split to 'validation'\n",
    "split_dataset = DatasetDict({\n",
    "    'train': split_dataset['train'],\n",
    "    'validation': split_dataset['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import accelerate\n",
    "reload(accelerate)\n",
    "# reload(TrainingArguments)\n",
    "# reload(Trainer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    per_device_train_batch_size=1,\n",
    "    eval_strategy=\"no\",\n",
    "    num_train_epochs=2,\n",
    "    fp16=False,\n",
    "    save_strategy=\"no\",\n",
    "    dataloader_num_workers=0,\n",
    "    logging_steps=10,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"validation\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())  # Should be True\n",
    "print(torch.backends.mps.is_built())       # Should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "END for HF train here^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "# Load and resample audio\n",
    "waveform, sample_rate = torchaudio.load(wav_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "parentNaId = \"653144\"\n",
    "filepath = f'../datasets/{parentNaId}_transcriptions_with_audio.parquet'\n",
    "filepath = f'../datasets/veterans_history_project_resources.parquet'\n",
    "# Load dataset from CSV\n",
    "dataset = load_dataset('parquet', data_files=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]['audio_filepaths'][0]\n",
    "# create new column to store the filepath after conversion\n",
    "# audio_filepaths are lists, only take the first element for conversion\n",
    "dataset = dataset.map(lambda x: {'audio_filepath_1st': [fp[0].replace(\"./\", \"../datasets/\") for fp in x['audio_filepaths']]}, batched=True)\n",
    "dataset = dataset.map(lambda x: {'transcription_str': [next(iter(t.values()))['transcription'] for t in x['transcription']]}, batched=True)\n",
    "dataset = dataset.map(lambda x: {'audio_filepath_1st': [convert_mp3_to_wav(fp) for fp in x['audio_filepath_1st']]}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]['audio_filepath_1st']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]['transcription_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "def preprocess_text(batch):\n",
    "    batch[\"input_ids\"] = tokenizer(batch[\"transcription_str\"], padding=True, truncation=True).input_ids\n",
    "    return batch\n",
    "\n",
    "    # Convert tokenized output to a numpy array to ensure consistent dtype\n",
    "    tokenized = tokenizer(texts, padding=True, truncation=True, return_tensors='np')\n",
    "    batch[\"input_ids\"] = tokenized[\"input_ids\"].tolist()\n",
    "    batch[\"attention_mask\"] = tokenized[\"attention_mask\"].tolist()\n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset = dataset.map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model configuration for new vocabulary size\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a custom collator\n",
    "def data_collator(batch):\n",
    "    audio_features = [item[\"input_values\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    return {\"input_values\": audio_features, \"labels\": labels}\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=16, shuffle=False, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        # Use \"input_ids\" as both input and label, to avoid KeyError\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = batch[\"input_ids\"]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed with loss {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
