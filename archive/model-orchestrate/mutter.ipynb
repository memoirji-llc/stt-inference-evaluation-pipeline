{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read audio file\n",
    "filepath = \"../datasets/audio/208-192.mp3\"\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# convert mp3 to wav\n",
    "def convert_mp3_to_wav(filepath):\n",
    "    audio = AudioSegment.from_mp3(filepath)\n",
    "    wav_filepath = filepath.replace(\".mp3\", \".wav\")\n",
    "    audio.export(wav_filepath, format='wav')\n",
    "    return wav_filepath\n",
    "\n",
    "wav_filepath = convert_mp3_to_wav(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "# Load and resample audio\n",
    "waveform, sample_rate = torchaudio.load(wav_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "parentNaId = \"653144\"\n",
    "filepath = f'../datasets/{parentNaId}_transcriptions_with_audio.parquet'\n",
    "# Load dataset from CSV\n",
    "dataset = load_dataset('parquet', data_files=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]['audio_filepaths'][0]\n",
    "# create new column to store the filepath after conversion\n",
    "# audio_filepaths are lists, only take the first element for conversion\n",
    "dataset = dataset.map(lambda x: {'audio_filepath_1st': [fp[0].replace(\"./\", \"../datasets/\") for fp in x['audio_filepaths']]}, batched=True)\n",
    "dataset = dataset.map(lambda x: {'transcription_str': [next(iter(t.values()))['transcription'] for t in x['transcription']]}, batched=True)\n",
    "dataset = dataset.map(lambda x: {'audio_filepath_1st': [convert_mp3_to_wav(fp) for fp in x['audio_filepath_1st']]}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]['audio_filepath_1st']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]['transcription_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "def preprocess_text(batch):\n",
    "    batch[\"input_ids\"] = tokenizer(batch[\"transcription_str\"], padding=True, truncation=True).input_ids\n",
    "    return batch\n",
    "\n",
    "    # Convert tokenized output to a numpy array to ensure consistent dtype\n",
    "    tokenized = tokenizer(texts, padding=True, truncation=True, return_tensors='np')\n",
    "    batch[\"input_ids\"] = tokenized[\"input_ids\"].tolist()\n",
    "    batch[\"attention_mask\"] = tokenized[\"attention_mask\"].tolist()\n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset = dataset.map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model configuration for new vocabulary size\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a custom collator\n",
    "def data_collator(batch):\n",
    "    audio_features = [item[\"input_values\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    return {\"input_values\": audio_features, \"labels\": labels}\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=16, shuffle=False, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        # Use \"input_ids\" as both input and label, to avoid KeyError\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = batch[\"input_ids\"]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed with loss {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_train",
   "language": "python",
   "name": "model_train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
