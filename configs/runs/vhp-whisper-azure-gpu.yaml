# Production config for Azure VM with NVIDIA T4 GPU
# Optimized for faster-whisper on CUDA

experiment_id: vhp-whisper-azure-gpu
model:
  name: "whisper"
  dir: "./models/models--Systran--faster-whisper-base"
  batch_size: 16  # T4 has 16GB VRAM, can handle larger batches
  device: "cuda"  # Explicitly use GPU
  compute_type: "float16"  # Use FP16 for faster inference on GPU

input:
  source: "azure_blob"
  parquet_path: "data/veterans_history_project_resources.parquet"
  blob_prefix: "loc_vhp"
  sample_size: 10  # Start small, scale up once working
  duration_sec: 300  # Process first 5 minutes
  sample_rate: 16000

output:
  dir: "outputs/vhp-whisper-azure-gpu"
  save_per_file: true

wandb:
  project: amia-stt
  group: azure-gpu-production
  tags: ["whisper-base", "azure-blob", "gpu", "t4"]
